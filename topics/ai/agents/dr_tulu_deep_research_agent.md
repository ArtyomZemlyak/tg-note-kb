# DR TULU: Открытый агент для глубоких исследований

## Описание

DR TULU (Deep Research Tulu) - это первый открытый языковой агент, предназначенный для выполнения задач глубоких исследований в формате длинных текстов, с прямым обучением через комбинацию Supervised Fine-Tuning (SFT) и Reinforcement Learning with Evolving Rubrics (RLER). DR Tulu начинается с сильной базовой модели и проходит через несколько этапов обучения: SFT на высококачественных запросах, связанных с поиском информации, за которым следует онлайн RL с RLER, адаптированным для задач глубоких исследований.

## Архитектура и функциональность

### Основной рабочий процесс

DR TULU работает в несколько этапов:

1. **Планирование**: Модель начинает с анализа того, какую информацию ей нужно получить и какие источники консультовать.
2. **Поиск и сбор**: Итеративно поиски и сбор доказательств из нескольких мест, синтезирование результатов, идентификация пробелов и уточнение стратегии на основе полученной информации.
3. **Адаптация глубины**: DR Tulu адаптирует глубину поиска в зависимости от сложности вопроса — простые запросы могут требовать один или два поиска, в то время как сложные исследовательские вопросы могут включать множество вызовов инструментов, исследующих несколько углов.

### Действия во время инференса

Во время вывода DR Tulu запускает цикл автопоиска и выбирает между тремя действиями:

- think для внутреннего планирования
- call_tool для вызова инструмента поиска или просмотра
- answer для производства окончательного ответа

Внутри окончательного ответа модель оборачивает утверждения в теги цитирования, которые ссылаются обратно на источники поддержки.

### Поддержка различных источников информации

Исследовательские вопросы требуют различных источников информации. Научные исследования получают пользу от научных баз данных, медицинские запросы нуждаются в авторитетных медицинских источниках, а общие запросы работают лучше всего с широким веб-поиском. Для поддержки этого разнообразия мы построили нашу систему вывода с использованием протокола контекста модели (MCP), рассматривая инструменты как сменные компоненты. В нашей стандартной конфигурации DR Tulu имеет доступ к трем инструментам поиска:

- google_search, который возвращает верхние веб-фрагменты
- web_browse, который извлекает полный текст страницы из URL
- paper_search, который извлекает соответствующие абзацы из научных статей с открытым доступом

## RLER: Обучение с эволюционирующими критериями

RLER (Reinforcement Learning with Evolving Rubrics) - это ключевая новая методология, лежащая в основе DR TULU, которая делает функцию вознаграждения адаптивной, эволюционируя по трем основным осям:

### Специфические для экземпляра, основанные на поиске критерии
Вместо применения общих оценочных подсказок, мы создаем критерий, адаптированный к каждому вопросу. Для каждого обучающего запроса мы сначала запускаем веб- и бумажный поиск, основанный на исходном вопросе, и подаем вопрос плюс retrieved контекст в модель генерации критерия. Это создает постоянный набор основанных на поиске критериев, которые кодируют актуальные, специфические для экземпляра критерии того, что должен содержать хороший ответ.

### Позитивные и негативные эволюционирующие критерии
Во время онлайн RL мы периодически отбираем несколько прогонов из текущей политики и просим генератор критерия предложить новые критерии, которые сравнивают эти ответы. Это дает два типа эволюционирующих критериев: позитивные и негативные критерии. Позитивные критерии поощряют новые, ценные стратегии или доказательства, которые модель обнаружила, но которые еще не зафиксированы в пуле критериев (например, консультация недоиспользуемого источника данных или предоставление особенно полезного промежуточного анализа). Негативные критерии явно наказывают за режимы отказа и взлом вознаграждения, такие как дословное копирование извлеченного текста, заполнение ответов нерелевантным содержимым только для увеличения видимого охвата или чрезмерное использование цитирований без добавления реального синтеза. Сохраняя оба позитивных и негативных критерия в игре, RLER поощряет появляющиеся хорошие поведения и быстро подавляет появляющиеся эксплуатационные поведения.

### Динамический буфер критериев и вспомогательные вознаграждения за цитирование
По мере продвижения обучения мы поддерживаем буфер критериев, который фильтрует и ранжирует критерии на основе того, насколько дискриминирующими они являются. Критерии, которые больше не различают между хорошими и плохими ответами - те, у которых близко к нулю дисперсия вознаграждения - отбрасываются, и мы сохраняем только ограниченное количество самых информативных критериев для каждого вопроса. В дополнение к критериям, основанным на вознаграждении, мы добавляем небольшие форматные и цитатные вознаграждения, которые проверяют, следует ли модель ожидаемому протоколу вывода и прикрепляет верные цитаты, которые действительно поддерживают ее ключевые утверждения.

## dr-agent-lib

DR Tulu включает в себя dr-agent-lib - библиотеку открытых исследований, построенную на MCP с многоинструментальным поиском, асинхронными вызовами инструментов и accompanying suite оценки. Эта библиотека предоставляет программируемую инфраструктуру на основе MCP для экспериментов с шаблонами подсказок, многоступенчатыми рабочими процессами и стратегиями детального вызова инструментов без переобучения базовой модели.

## Производительность и сравнение

DR Tulu-8B значительно превосходит открытые базовые модели на всех четырех долгосрочных бенчмарках - ScholarQA-CSv2, HealthBench, ResearchQA и DeepResearch Bench. На ScholarQA-CSv2 он достигает 86,7 по сравнению с 42,5 для WebExplorer-8B и 32,9 для WebThinker-32B-DPO, несмотря на то, что он в 4 раза меньше. На ResearchQA и DeepResearch Bench DR Tulu-8B (RL) набирает 71,1 и 41,8 по сравнению с 64,8/36,7 для WebExplorer-8B и 48,6/23,3 для WebThinker-32B-DPO.

Сравнивая с проприетарными системами глубоких исследований, DR Tulu-8B (RL) соответствует или превосходит производительность на нескольких долгосрочных бенчмарках. На ScholarQA-CSv2 он превосходит все оцененные проприетарные системы, включая OpenAI Deep Research (79,6) и Perplexity Deep Research (67,3).

## Стоимость и эффективность

DR Tulu-8B (RL) дешево запускать для достигнутой производительности. При нашей конфигурации оценки (и считая только внешние вызовы API, предполагая, что стоимость оборудования и хостинга уже оплачены), типичный запрос глубоких исследований стоит примерно 0,00008 $. Даже если агент выдает максимальное количество разрешенных вызовов поиска, разрешенных во время оценки (10), стоимость API за запрос все равно ограничена примерно 0,0075 $.

Наоборот, OpenAI Deep Research стоит около 1,80 $ за запрос ScholarQA-CSv2, а наш Asta pipeline, использующий Claude Sonnet, стоит около 1,30 $ за запрос.

## Выпущенные компоненты

Вся структура DR Tulu доступна под лицензией с разрешением:

1. **Курированные наборы данных подсказок**
2. **Код обучения и оценки** (включая реализацию RLER)
3. **Контрольная точка DR Tulu-8B**
4. **Фреймворк генерации критериев и обучения RLER**
5. **dr-agent-lib** - библиотека агентов с открытым исходным кодом

## Связи с другими темами

- [[qwen/qwen_deepresearch_2511.md]] - Другая модель глубоких исследований, построенная на основе Qwen
- [[reinforcement_learning/self_proposed_rubrics.md]] - Схожая концепция использования критериев для оценки
- [[reinforcement_learning/laser_reinforcement_learning.md]] - Другие методы обучения с подкреплением для LLM
- [[agents/available_agent_tools.md]] - Инструменты, доступные для агентов
- [[llm/models/qwen/qwen3.md]] - Базовая модель Qwen3, на которой основан DR Tulu
- [[reinforcement_learning/reinforcement_learning_in_llms.md]] - Обучение с подкреплением в контексте LLM

## Источники

1. [DR TULU: AN OPEN, END-TO-END TRAINING RECIPE FOR LONG-FORM DEEP RESEARCH](https://allenai.org/blog/dr-tulu) - официальный блог-пост от Allen Institute о проекте DR Tulu, описывающий архитектуру, методы обучения и результаты