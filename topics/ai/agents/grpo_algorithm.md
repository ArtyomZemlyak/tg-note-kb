# GRPO: Group Relative Policy Optimization

## Краткое описание
Group Relative Policy Optimization (GRPO) - это алгоритм обучения с подкреплением, разработанный для обучения оркестраторов в рамках фреймворка ToolOrchestra. GRPO решает проблему масштабирования наград в традиционном PPO, нормализуя награды внутри групп траекторий для одного и того же входа.

## Основная информация

### Контекст и проблема
Традиционный PPO (Proximal Policy Optimization) плохо подходит для обучения оркестраторов, так как масштаб награды дико скачет в зависимости от выбранного вектора предпочтений P. Это затрудняет обучение модели делать устойчивые улучшения в условиях переменных критериев оптимизации.

### Решение: GRPO
GRPO нормализует награды внутри группы траекторий для одного и того же входа. Преимущество (advantage) A считается так:
A(τ) = (R(τ) - mean(R)) / std(R)

То есть модель учится понимать, что *относительно других вариантов* в данных условиях вызов мат-модели был лучше, чем вызов GPT-5. Это позволяет обучать политику на основе относительного сравнения траекторий, вместо абсолютной шкалы награды.

### Принцип работы
1. Для каждого входного запроса генерируется несколько траекторий (вариантов действий)
2. Вычисляется награда для каждой траектории
3. Награды нормализуются внутри группы траекторий
4. Модель обучается максимизировать нормализованное преимущество
5. Это позволяет модели учиться, какие действия дают лучший результат по сравнению с другими возможными действиями в тех же условиях

### Преимущества GRPO
- Стабилизирует процесс обучения при переменных векторах предпочтений
- Позволяет сравнивать эффективность разных стратегий для одного и того же запроса
- Учитывает относительную эффективность действий в контексте других возможных действий
- Повышает устойчивость обучения в мультиобъектной среде

## Новые концепции и термины

- **Group Relative Policy Optimization (GRPO)**: Алгоритм обучения с подкреплением, нормализующий награды внутри групп траекторий
- **Нормализация внутри группы**: Процесс приведения наград к сопоставимому масштабу для разных траекторий одного запроса
- **Относительное преимущество**: Преимущество действия по сравнению с другими возможными действиями в аналогичной ситуации

## Примеры применения
- Обучение оркестраторов для выбора между различными инструментами
- Оптимизация маршрутизации запросов между моделями с разными характеристиками
- Системы, где важна балансировка между несколькими критериями эффективности

## Связи с другими темами
- [[toolorchestra_framework.md]] - Фреймворк, в котором используется GRPO
- [[../llm/rlhf_approaches.md]] - Другие методы обучения с подкреплением для LLM
- [[../llm/prompt_optimization_methods.md]] - Подходы к оптимизации взаимодействия с моделями

## Источники
1. Su, Hongjin, Diao, Shizhe, Lu, Ximing, et al. "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration" (2025). arXiv:2511.21689
2. "Group Relative Policy Optimization Algorithm" (2024). arXiv:2402.03300