# MLE-bench: Бенчмарк для оценки ИИ-агентов машинного обучения

## Краткое описание

MLE-bench (Autonomous ML Engineering Benchmark) - это бенчмарк, состоящий из 71-75 отобранных соревнований Kaggle для оценки способностей инженерии машинного обучения ИИ-агентов. Он разработан для тестирования того, насколько хорошо ИИ-агенты могут выполнять полные ML-воркфлоу на курируемых соревнованиях Kaggle.

## Назначение и функции

MLE-bench предоставляет строгую и объективную оценку ИИ-агентов, решающих задачи машинного обучения. В отличие от более общих бенчмарков, MLE-bench фокусируется на:

- **Реальных задачах машинного обучения**: Задачи из реальных соревнований Kaggle, требующие комплексного подхода
- **Полный ML-воркфлоу**: От предобработки данных до подбора моделей и оптимизации
- **Сравнение с человеческими решениями**: Агенты соревнуются с существующими решениями мастеров Kaggle
- **Комплексная оценка**: Учитываются как качество модели, так и эффективность процесса разработки

## Структура бенчмарка

- **71-75 задач**: Отобранные соревнования Kaggle, разнообразные по области и сложности
- **Оценка по медалям**: Сравнение с распределением решений Kaggle-мастеров (золотая, серебряная, бронзовая)
- **Разнообразие задач**: Задачи регрессии, классификации, анализ временных рядов и другие

## Значимость для ИИ-агента

MLE-bench стал важным бенчмарком для оценки ИИ-исследовательских агентов, так как:

- Он отражает реальные вызовы, с которыми сталкиваются ML-инженеры
- Позволяет сравнивать агентов с человеческими экспертами
- Используется в исследованиях, таких как "What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity", для анализа факторов успеха агентов

## Сравнение с другими бенчмарками

### MLE-bench vs AlphaResearchComp
- **MLE-bench**: Фокусируется на задачах из реальных соревнований Kaggle, оценивает пайплайны ML
- **AlphaResearchComp**: Более теоретические задачи из геометрии, теории чисел и оптимизации, оценивает автоматическое открытие алгоритмов

### Другие бенчмарки
- **SWE-bench**: Фокусируется на задачах программирования
- **AgentBench**: Комплексный бенчмарк для разных типов агентов
- **BigCodeBench**: Задачи генерации и понимания кода

## Применение в исследованиях

MLE-bench использовался в исследованиях, показавших, что:
- Разнообразие идей (ideation diversity) имеет сильную корреляцию с перформансом
- Агенты с более разнообразными подходами показывают лучшие результаты
- Архитектура скаффолда влияет на разнообразие идей

## Ограничения

- Требует значительных вычислительных ресурсов для оценки
- Некоторые задачи могут быть непредставительны для реальных сценариев
- Зависимость от существующих Kaggle-соревнований может ограничивать новизну задач

## Связи с другими темами

- [[ai/agents/ideation_diversity_in_ai_research_agents.md|Разнообразие идей в ИИ-исследовательских агентах]] - исследование, использующее MLE-bench
- [[ai/agents/ai_agent_benchmarks.md|Бенчмарки для ИИ-агентов]] - другие оценочные фреймворки
- [[ai/graphs/kaggle_competitions.md|Kaggle соревнования]] - базовые соревнования, на которых основан бенчмарк
- [[ai/agents/alpharesearch_benchmark.md|AlphaResearchComp]] - альтернативный бенчмарк для автоматического открытия алгоритмов

## Источники

1. [MLE-bench: Evaluating Machine Learning Agents on Realistic Kaggle Tasks](https://arxiv.org/abs/2410.07095) - основная статья, описывающая бенчмарк MLE-bench
2. [What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity](https://arxiv.org/abs/2511.15593) - исследование, использующее MLE-bench для анализа разнообразия идей
3. [MLE-bench on Hugging Face](https://huggingface.co/papers/2410.07095) - описание бенчмарка на Hugging Face
4. [Emergent Mind: MLE-bench](https://www.emergentmind.com/topics/mle-bench) - обзор бенчмарка