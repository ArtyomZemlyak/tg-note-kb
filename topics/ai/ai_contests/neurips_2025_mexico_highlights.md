# NeurIPS 2025 в Мехико: Основные моменты четвёртого дня

## Обзор

![Perception Encoder: The best visual embeddings are not at the output of the network](../../../media/img_1765199234_aqad9xjrgwzjkul_the_mission_mbeddings_are_not.jpg)

**Изображение:** Основная концепция Perception Encoder, представленная на NeurIPS 2025

Четвёртый день конференции NeurIPS в Мехико оказался насыщенным событиями. В программе были включены выступление Ричарда Саттона о его видении SuperIntelligence, две сессии со статьями и две сессии с постерами.

## Выступление Ричарда Саттона

Ричард Саттон, один из пионеров обучения с подкреплением, выступил с размышлениями о своем видении искусственного суперинтеллекта. Его доклад затронул фундаментальные вопросы о направлении исследований в области ИИ и возможных путях достижения суперинтеллекта.

## Ключевая статья дня: Perception Encoder

Самой интересной статьёй дня, по мнению Владислава Фахретдинова, стала работа "Perception Encoder: The best visual embeddings are not at the output of the network" от Meta AI.

### Основные моменты статьи

Исследователи поставили перед собой цель создать лучший визуальный энкодер для множества downstream-задач. Для этого была использована двухступенчатая схема обучения с contrastive loss на парах "изображение-текст", а затем на парах "видео-текст", используя модель как кадровый энкодер.

### PE_core и PE_spatial

Начав с CLIP-бейзлайна, добавили ряд улучшений и сравнили их по качеству и устойчивости. Уже на этом этапе модель достигла SOTA в zero-shot retrieval и классификации; назвали её PE_core.

При тестировании как энкодера на разных downstream-задачах (детекция, трекинг, предсказание глубин) производительность оказалась ниже ожидаемой. Анализ attention maps позволил обнаружить появление глобальных токенов на определённом слое.

### Ключевое открытие

Построив график зависимости качества от номера слоя для разных downstream-задач и моделей, исследователи обнаружили, что качество возрастает к эмбеддингам средних слоёв, а к последним слоям - резко падает.

### Решение проблемы

Для решения были использованы два метода после основного обучения:
1. Fine-tuning на 41-м слое с минимизацией косинусного расстояния между ним и последним слоем для сохранения глобальной информации.
2. Fine-tuning на MSE попарного косинусного расстояния между эмбеддингами последнего слоя и логитами SAM для сохранения локальной информации.

Результатом стало создание PE_spatial, достигающей SOTA по многим downstream-задачам.

## Связи с другими темами

- [[perception_encoder.md]] - Подробное описание модели Perception Encoder
- [[neurips_2025.md]] - Общий обзор наград и достижений конференции NeurIPS 2025
- [[multimodal_models.md]] - Связь с CLIP и другими мультимодельными архитектурами
- [[ai/research_advances/meta_ai_layoffs.md]] - Контекст исследований в Meta AI, включая увольнения в лаборатории суперинтеллекта

## Источники

1. CV Time - Разбор статьи Perception Encoder от Vladislav Fahrtdinov на конференции NeurIPS в Мехико
2. Оригинальная статья "Perception Encoder: The best visual embeddings are not at the output of the network" от Meta AI