# Диффузионные модели в пиксельном пространстве: Подход x0-prediction

## Общее описание

Статья "Back to Basics: Let Denoising Generative Models Denoise" предлагает новую перспективу на диффузионные модели, фокусируясь на прямой работе в пиксельном пространстве вместо традиционного подхода с латентным сжатием. Основная идея заключается в использовании x0-prediction (предсказание чистого образца) вместо ε-prediction (предсказание шума) или v-prediction (предсказание скорости), что дает лучшие результаты в высокоразмерном пространстве.

## Контекст проблемы

В области генерации изображений и видео доминируют латентные диффузионные модели, использующие вспомогательный автоэнкодер для отображения в пространство с меньшей пространственно-временной размерностью. Это позволяет дешевле реализовать диффузионный процесс с хорошим качеством. Однако в статье рассматривается возможность работы диффузии непосредственно в пиксельном пространстве на больших пиксельных патчах.

## Основные типы предсказания в диффузионных моделях

В традиционных диффузионных моделях используются три типа предсказания:

1. **x0-prediction** - предсказание чистого образца (clean sample)
2. **ε-prediction** - предсказание шума (noise)
3. **v-prediction** - предсказание скорости (velocity), взвешенная комбинация x0 и ε

Математически все формулировки эквивалентны (с точностью до коэффициентов в лоссе), но на практике они ведут себя по-разному.

## Гипотеза статьи

Ключевое наблюдение статьи заключается в том, что:
- Естественные изображения лежат на подпространстве (manifold), а не занимают всё возможное пространство
- Только x0 (чистый образец) лежит на этом подпространстве
- ε (шум) и v (скорость) занимают всё пространство, включая области вне подпространства изображений

Отсюда следует гипотеза: обучение на x0 должно быть проще, чем на ε или скорости, так как модель работает только с допустимыми точками на подпространстве изображений.

## Экспериментальная валидация

### Синтетические эксперименты
Авторы сначала проверяют свою гипотезу на 2D спирали, вложенной в пространство высокой размерности. Результаты показывают, что начиная с определенного момента ε-prediction и v-prediction работают плохо, в то время как x0-prediction продолжает работать эффективно.

### Эксперименты с реальными изображениями
- Используется Vision Transformer, названный Just Image Transformer (JiT), не путать с JIT-компиляцией
- Изображения нарезаются на большие патчи (16x16, 32x32)
- Модель обучается на ImageNet-256/512 в пиксельном пространстве
- Все варианты, кроме x0-prediction, работают плохо, и тюнинг уровня шума не помогает
- x0-prediction дает приемлемые результаты

### Архитектурные улучшения
- Добавление дополнительного боттлнека после патчеризации улучшает качество
- Введение архитектурных модификаций: SwiGLU, RMSNorm, RoPE и 32 in-context класс токены
- Эти изменения немного улучшают метрики

## Результаты и выводы

- Итоговый результат не достигает SOTA, но вполне пристойный
- Интерпретация через обучение на подпространствах (manifold learning) выглядит перспективной
- Вопрос открыт для случая условной генерации (class-conditional)

## Связь с другими темами

- [[diffusion_models.md|Диффузионные модели]] - общий подход к диффузионным моделям
- [[vision_transformer.md]] - архитектура Vision Transformer, используемая в статье
- [[generative_models.md|Генеративные модели]] - общая категория генеративных моделей
- [[variational_autoencoders.md|Вариационные автоэнкодеры]] - контраст с подходами, использующими латентное пространство

## Иллюстрации и визуализации

![Сравнение подходов к диффузионным моделям в пиксельном пространстве](../../../media/img_1764033898_aqadza9rgzwgul_image_image.jpg)

**Описание:** На изображении представлено сравнение различных подходов к диффузионным моделям: традиционные латентные модели с автоэнкодером (слева) и прямые модели в пиксельном пространстве (справа), как предложено в статье "Back to Basics". На схеме показаны различия в архитектуре и процессе генерации.

## Источники

1. [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720) - оригинальная статья от авторов, включая создателя ResNet