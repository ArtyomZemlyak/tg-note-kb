# SAM 3: Segment Anything Model — Модель для сегментации всего

## Описание

SAM 3 (Segment Anything Model 3) — это новое поколение foundation-модели для задачи Promptable Concept Segmentation (PCS), выпущенной Meta. Модель способна обнаруживать, сегментировать и отслеживать все объекты в изображениях и видео на основе текстовых промптов или примеров изображений. В отличие от предыдущих версий SAM, которые сегментировали один объект за раз, SAM 3 может находить все экземпляры указанного визуального понятия одновременно.

## Архитектура

SAM 3 состоит из детектора и трекера, которые разделяют Perception Encoder (PE) визуальный бэкбон в декаплированной архитектуре:

### Основные компоненты:
- **Детектор**: архитектура на основе DETR для детектирования понятий на уровне изображения
  - Текстовый энкодер для промптов с именными фразами
  - Энкодер примеров для промптов на основе изображений
  - Модуль слияния для адаптации визуальных признаков к промптам
  - Нововведенный «presence head», который разделяет распознавание («что») и локализацию («где»)
  - Mask head для генерации масок сегментации объектов
- **Трекер**: видео-сегментация на основе памяти, унаследованная от SAM 2
  - Энкодер промптов, декодер масок, энкодер памяти
  - Банк памяти для хранения внешнего вида объектов на протяжении кадров
  - Временная дизамбигуация с использованием таких методов, как фильтр Калмана

### Ключевые инновации:
1. **Декаплированное распознавание и локализация**: presence head предсказывает присутствие понятия глобально, в то время как proposal-запросы фокусируются только на локализации
2. **Единые понятийные и визуальные промпты**: поддерживает как PCS (пропт-понятия), так и PVS (визуальные промпты, как у SAM 2)
3. **Интерактивное уточнение примера**: пользователи могут добавлять положительные или отрицательные примеры изображений для итеративного улучшения результатов
4. **Временная дизамбигуация**: справляется с окклюзиями и сбоями трекинга в видео

## Технические характеристики

- **Размер модели**: ~400+ МБ (оценка, больше, чем SAM 2 с 162 МБ)
- **Скорость инференса**: 30 мс на изображение с 100+ обнаруженными объектами (GPU H200)
- **Производительность видео**: почти в реальном времени для ~5 одновременных объектов
- **LVIS Zero-Shot Mask AP**: 47.0 (против предыдущего рекорда 38.5, +22% улучшение)
- **MOSEv2 VOS J&F**: 60.1 (+25.5% по сравнению с SAM 2.1, +17% по сравнению с предыдущим SOTA)

## Данные для обучения

SAM 3 обучается на датасете SA-Co (Segment Anything with Concepts):

### Компоненты обучающих данных:
- **SA-Co/HQ**: 5.2М изображений, 4М уникальных именных фраз (высококачественная ручная разметка)
- **SA-Co/SYN**: 38М именных фраз, 1.4М масок (синтетический датасет)
- **SA-Co/EXT**: 15 внешних датасетов с хард-негативами
- **SA-Co/VIDEO**: 52.5K видео, 24.8K уникальных именных фраз

### Данные для бенчмарка:
- SA-Co оценочный бенчмарк содержит 214K уникальных фраз в 126K изображений/видео
- Более чем в 50 раз больше понятий, чем в существующих бенчмарках
- Включает SA-Co/Gold (7 доменов, размечено тремя экспертами), SA-Co/Silver (10 доменов) и видео-бенчмарки

## Улучшения производительности по сравнению с предыдущими версиями

- **2-кратный прирост производительности** по сравнению с существующими системами в Promptable Concept Segmentation
- **+22% улучшение** в LVIS zero-shot Mask AP (47.0 против 38.5)
- **+25.5% улучшение** в MOSEv2 VOS J&F по сравнению с SAM 2.1
- **+18.6 CGF1 улучшение** после 3 промпт-примеров для интерактивного уточнения
- **88% от оценочной производительности человека** на бенчмарке SA-Co/Gold

### Ключевые результаты абляции:
- Presence head дает улучшение +5.7 CGF1 (+9.9%)
- Хард-негативы улучшают IL_MCC на 54.5% (0.44 → 0.68)
- Все три источника данных вместе достигают 54.3 CGF1 против 30.9 только с внешними

## Возможности

- **Сегментация понятий**: текстовые фразы, примеры изображений — все экземпляры, соответствующие понятию
- **Визуальная сегментация**: точки, прямоугольники, маски — один экземпляр объекта (совместимость с SAM 2)
- **Интерактивное уточнение**: добавление/удаление примеров или кликов итеративно
- **Сегментация видео-понятий**: отслеживание всех экземпляров понятия в видео
- **Обратная совместимость**: полностью сохраняется совместимость с визуальными промптами SAM 2

## Применение

- Модерация контента
- Сегментация товаров в электронной коммерции
- Медицинская визуализация
- Автономные системы
- Видеоаналитика
- Разметка датасетов
- Научные исследования

## Ограничения

- Наилучшие результаты показывает с простыми именными фразами; сложные рассуждения требуют интеграции MLLM
- Требования к вычислительным ресурсам больше, чем у специализированных моделей детекции
- Словарь сосредоточен на атомарных визуальных понятиях
- Производительность может снижаться на экстремально редких понятиях

## Источники

1. [SAM 3: Segment Anything with Concepts - Ultralytics Documentation](https://docs.ultralytics.com/models/sam-3/) - подробная документация об архитектуре, возможностях и производительности модели SAM 3
2. [What Is Segment Anything 3 (SAM 3)? - Roboflow Blog](https://blog.roboflow.com/what-is-sam3/) - анализ SAM 3, включая архитектуру, данные для обучения и ключевые инновации по сравнению с предыдущими версиями
3. [Meta AI Blog - Segment Anything Model 3 Announcement](https://ai.meta.com/blog/segment-anything-model-3/) - официальное объявление от Meta о выпуске SAM 3

## Связи с другими темами

- [[vision_transformer.md]] - Transformer-архитектуры в задачах компьютерного зрения, на которых основаны модели SAM
- [[generative_segmentation_as_editing.md]] - Парадигма генеративной сегментации как редактирования, альтернативный подход к сегментации
- [[multimodal_models.md]] - Мультимодальные модели, включающие возможности текстовых промптов, аналогично SAM 3
- [[object_detection_yolo_ocr.md]] - Другие подходы к детекции объектов в сравнении с подходом SAM 3