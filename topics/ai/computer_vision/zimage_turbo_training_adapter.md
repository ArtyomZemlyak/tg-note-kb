# Z-Image-Turbo Training Adapter

## Описание

Z-Image-Turbo Training Adapter - это адаптер обучения, разработанный для тонкой настройки модели Tongyi-MAI/Z-Image-Turbo. Он был создан для использования в AI Toolkit, но потенциально может использоваться и в других тренерах. Его также можно использовать как общий адаптер LoRA для дедистилляции при инференсе, чтобы убрать "Turbo" из "Z-Image-Turbo".

## Технические детали

- **Тип модели**: Текст-в-изображение, Diffusers, LoRA
- **Шаблон**: diffusion-lora
- **Лицензия**: apache-2.0
- **Базовая модель**: Tongyi-MAI/Z-Image-Turbo
- **Загрузки**: 15,207 за последний месяц
- **Понравилось**: 22

## Зачем он нужен?

Когда вы обучаете напрямую на модели с шаговой дистилляцией, дистилляция очень быстро разрушается. Это приводит к непредсказуемой потере шаговой дистилляции. Адаптер для обучения дедистилляции значительно замедляет этот процесс, позволяя выполнять короткие циклы обучения, сохраняя при этом шаговую дистилляцию (скорость).

## В чем подвох?

Это действительно просто хак, который значительно замедляет дистилляцию при тонкой настройке дистиллированной модели. Дистилляция все равно будет разрушаться со временем. Это означает, что адаптер будет отлично работать для коротких циклов обучения, таких как стили, концепции и персонажи. Однако длительный цикл обучения, скорее всего, приведет к разрушению дистилляции до такой степени, что при удалении адаптера будут возникать артефакты.

## Как он был создан?

Тысячи изображений были сгенерированы при различных размерах и соотношениях сторон с использованием Tongyi-MAI/Z-Image-Turbo. Затем был обучен LoRA на этих изображениях при низкой скорости обучения (1e-5). Это позволило разрушить дистилляцию, сохранив при этом существенные знания модели.

## Как это работает?

Поскольку этот адаптер разрушил дистилляцию, если вы обучаете LoRA поверх него, дистилляция больше не будет разрушаться в вашем новом LoRA, поскольку этот адаптер дедистиллировал модель. Ваш LoRA теперь будет изучать только предмет, который вы обучаете. Когда придет время для инференса/сэмплирования, мы удаляем этот адаптер обучения, что оставляет вашу новую информацию на дистиллированной модели, позволяя вашей новой информации работать на дистиллированных скоростях.

## Связи с другими темами

- [[ai/computer_vision/z_image_turbo.md]] - Базовая модель, для которой разработан адаптер
- [[ai/llm/lora_optimization.md]] - Технология LoRA, используемая в адаптере
- [[ai/tools/ai_toolkit_by_ostris.md]] - Инструмент, для которого был создан адаптер
- [[ai/llm/diffusion_models.md]] - Общая информация о диффузионных моделях

## Источники

1. [Z-Image-Turbo Training Adapter на Hugging Face](https://huggingface.co/ostris/zimage_turbo_training_adapter) - Официальная страница адаптера с описанием и техническими деталями
2. [Z-Image-Turbo Model на Hugging Face](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) - Страница базовой модели
3. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) - Оригинальная статья о методе Low-Rank Adaptation