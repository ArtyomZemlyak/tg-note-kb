# Катастрофическое забывание (Catastrophic Forgetting)

## Определение

Катаstroфическое забывание (также известное как катастрофическая интерференция) - это тенденция искусственных нейронных сетей резко и радикально забывать предыдущую информацию после изучения новой информации. Это серьезная проблема для непрерывного обучения, когда модели должны изучать новые задачи или классы без потери знаний, полученных ранее.

## История и проблема

Проблема катастрофического забывания была впервые описана Макклоски и Коэном (1989) и Рэтклиффом (1990). В их экспериментах нейронные сети, обученные сначала одной задаче (например, сложению чисел), полностью теряли способность выполнять эту задачу после обучения новой задаче. Это явление противоречит человеческой памяти, которая способна сохранять предыдущие знания при изучении нового.

## Причины проблемы

Основные причины катастрофического забывания связаны с перекрытием представлений в скрытых слоях распределенных нейронных сетей. При последовательном обучении входные данные смешиваются, и новые паттерны накладываются поверх старых, изменяя веса, в которых хранилась предыдущая информация.

## Связь с приращением класса

Катастрофическое забывание особенно проблематично в задаче приращения класса (class-incremental learning), когда модель последовательно изучает новые классы, не имея доступа к данным предыдущих задач. При этом модель может утратить способность распознавать старые классы при изучении новых.

## Подходы к решению проблемы

### Методы воспроизведения опыта (Experience Replay)
1. **Псевдо-воспроизведение (Pseudo-rehearsal)**: сеть обучается на сгенерированных внутренних представлениях предыдущих данных, а не на оригинальных данных
2. **Генеративное воспроизведение (Generative replay)**: использование глубоких генеративных моделей для создания "псевдо-данных" для повторного обучения
3. **Спонтанное воспроизведение (Spontaneous replay)**: вдохновленные биологией модели, при которых внутренние представления (воспоминания) спонтанно воспроизводятся во время "сна" нейронной сети

### Регуляризационные методы
1. **Упругая консолидация весов (Elastic Weight Consolidation, EWC)**: метод, который предполагает, что некоторые веса нейронной сети более важны для предыдущих задач, чем другие. Изменения важных весов ограничиваются во время обучения новым задачам
2. **Ортогональность**: создание ортогональных (перпендикулярных) векторов для представления разных задач, что уменьшает интерференцию

### Архитектурные методы
1. **Техника усиления узлов (Node sharpening)**: уменьшение перекрытия активаций в скрытом слое
2. **Предварительное обучение сетей**: обучение на случайной выборке данных перед основным обучением, чтобы создать основу знаний
3. **Прогрессивные нейронные сети**: добавление новых столбцов для новых задач при сохранении старых знаний
4. **Memo (MEmory and MOdular expansion)**: комбинация сохраненных экземпляров и модульного расширения архитектуры

## Дилемма стабильность-пластичность

Катастрофическое забывание является проявлением дилеммы стабильность-пластичность: задача состоит в создании нейронных сетей, чувствительных к новой информации, но не разрушающих предыдущие знания. В то время как табличные структуры данных остаются полностью стабильными при новой информации (но не обобщают), нейронные сети могут обобщать, но чувствительны к новой информации, что приводит к забыванию.

## Современные достижения

Современные методы, такие как генеративное воспроизведение, EWC и Memo, позволяют нейронным сетям изучать задачи последовательно с минимальной потерей производительности на предыдущих задачах. Недавняя разработка Google - вложенное обучение (Nested Learning) - представляет новый подход к решению проблемы катастрофического забывания через иерархическую интеграцию знаний.

Особое внимание заслуживает парадигма вложенного обучения (Nested Learning), которая переосмысливает модели машинного обучения как интегрированную систему вложенных, многоуровневых оптимизационных задач. Вместо простого представления модели как стека слоев, NL представляет её как иерархию взаимосвязанных оптимизационных задач, где каждый компонент работает со своим "потоком контекста" и имеет свою частоту обновления. Это позволяет создавать системы, которые учатся как человек - поэтапно, постепенно и адаптивно.

## Связи с другими темами

- [[../nested_learning.md]] - Вложенное обучение: новая парадигма ИИ от Google, решает проблему катастрофического забывания через слоистую интеграцию знаний
- [[../class_incremental_learning/class_incremental_learning.md]] - Приращение класса: сценарий, в котором проявляется катаstroфическое забывание
- [[../class_incremental_learning/memo_2022.md]] - Memo: один из современных методов решения проблемы
- [[../regularization/elastic_weight_consolidation.md]] - EWC: регуляризационный метод решения проблемы
- [[../rehearsal/experience_replay.md]] - Методы воспроизведения опыта как один из подходов решения
- [[../plasticity/loss_of_plasticity.md]] - Потеря пластичности: связанная, но отличная проблема в continual learning
- [[../../optimization/incremental_learning.md]] - Инкрементальное обучение: прикладной контекст для решения проблемы забывания
- [[../continuum_memory_system.md]] - Система непрерывной памяти из парадигмы NL, решающая проблему хранения информации на разных временных масштабах
- [[../hope_architecture.md]] - Архитектура HOPE как реализация принципов непрерывного обучения
- [[../titan_architecture.md]] - Развитие идей от архитектуры Titans к современным подходам