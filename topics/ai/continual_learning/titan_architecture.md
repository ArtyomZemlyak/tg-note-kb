# Архитектура Titans - нейронный модуль памяти с самообновлением

## Описание

Titans - это новое семейство гибридных архитектур, представленное в статье "Titans: Learning to Memorize at Test Time" исследователями из Google (Ali Behrouz, Peilin Zhong, Vahab Mirrokni). Архитектура решает фундаментальный компромисс между трансформерами, обеспечивающими высокую точность, но страдающими от квадратичной вычислительной сложности, и современными линейными рекуррентными моделями, которые эффективны, но с трудом сжимают очень длинные контексты без потери информации.

Ключевая инновация - это новый модуль нейронной долговременной памяти (Long-Term Memory Module, LMM), глубокий нелинейный рекуррентный модуль, который работает как meta in-context обучатель. LMM не просто обрабатывает данные, а на лету адаптивно учится тому, как запоминать и забывать информацию, оптимизируя собственные веса прямо во время инференса.

## Основная информация

Titans решает критический разрыв между трансформерами и линейными рекуррентными моделями. В отличие от традиционных архитектур, LMM обучается адаптивно запоминать и забывать информацию, оптимизируя свои собственные параметры прямо во время инференса. Это достигается за счёт метрики "удивления" на основе градиента с моментом, что позволяет отслеживать и сохранять важные события, а также с помощью адаптивного механизма забывания, который предотвращает переполнение памяти.

## Ключевые особенности

### 1. Модуль долгосрочной памяти (LMM)
- **Глубокий MLP**: В отличие от простого рекуррентного состояния, LMM - это глубокий многослойный перцептрон
- **Метаобучение**: Функционирует как метамодель, выучивающая собственный алгоритм оптимизации
- **Самообновление**: Оптимизирует свои же параметры во время одного прямого прохода на этапе инференса
- **Механизмы обновления**: Использует момент и распад весов (weight decay) для обновления собственных параметров

### 2. Механизм "удивления" (Surprise-based learning)
- **Формализация через loss**: Использует loss ассоциативной памяти, где модуль памяти пытается предсказать токен-значение по токену-ключу: l(Mₜ₋₁; xₜ) = ||Mₜ₋₁(kₜ) – vₜ||₂
- **Градиентное измерение**: "Удивление" измеряется градиентом этого loss
- **Момент-обновления**: Использует правило обновления на основе момента: Sₜ = ηₜ Sₜ₋₁ - θₜ ∇l(Mₜ₋₁; xₜ)
- **Память об удивлении**: Sₜ действует как элемент момента, позволяющий запомнить всё событие целиком, а не только один токен

### 3. Адаптивный механизм забывания
- **Зависящий от данных гейт**: Конечное обновление памяти регулируется гейтом αₜ
- **Аналог распада весов**: Гейт позволяет модели удалять устаревшую информацию
- **Управление емкостью**: Позволяет управлять ограниченной памятью на миллионах токенов
- **Формула обновления**: Mₜ = (1 - αₜ) Mₜ₋₁ + Sₜ

### 4. Три варианта интеграции (MAC, MAG, MAL)
- **MAC (Memory as Context)**: LMM извлекает релевантную историческую информацию, которая объединяется с постоянной памятью и текущим сегментом. Модуль внимания обрабатывает расширенный контекст.
- **MAG (Memory as Gate)**: LMM и модуль внимания работают параллельно, а их выходы объединяются через нелинейный гейт.
- **MAL (Memory as Layer)**: LMM действует как сжимающий слой, обрабатывая вход перед его подачей в модуль внимания.

## Архитектурные компоненты

### Модуль долгосрочной памяти (LMM)
- Глубокий нейронный модуль, отвечающий за хранение абстрактной истории
- Использует метрику "удивления" на основе градиента с моментом
- Включает адаптивный механизм забывания
- Подробное описание: [[../llm/architectures/lmm_long_term_memory_module.md]]

### Кратковременная память (Ядро)
- Стандартный механизм внимания (часто со скользящим окном)
- Для точного моделирования локальных зависимостей
- Обрабатывает текущий контекст

### Постоянная память
- Набор обучаемых, но не зависящих от данных параметров
- Добавляются в начало входа для хранения абстрактных знаний о задаче

### Три варианта архитектур
- MAC, MAG, MAL: Различные способы интеграции компонентов
- Подробное описание: [[../llm/architectures/mac_mag_mal_architectures.md]]

## Сравнение с другими архитектурами

### Отличия от трансформеров
- Решает проблему квадратичной сложности за счёт гибридной архитектуры
- Возможность обучения во время инференса, а не только на этапе тренировки
- Комбинирует точность внимания с эффективностью рекуррентных подходов

### Отличия от SSM (Mamba и др.)
- Более выразительные для задач отслеживания состояния: Titans способны решать задачи за пределами класса сложности TC⁰
- Динамическое обучение параметров во время инференса
- Комбинация разных временных масштабов памяти

### Сравнение с Gated DeltaNet и TTT
- Превосходит Gated DeltaNet и TTT в задачах с длинным контекстом (например, в задаче NIAH, где Gated DeltaNet и TTT показывают 0.0% точности, а MAC достигает 95.2%)
- Использует более сложный механизм "удивления" на основе градиента с моментом, в отличие от простых обновлений в DeltaNet/TTT
- Включает адаптивный механизм забывания, чего нет в базовых подходах DeltaNet/TTT
- Использует глубокую нелинейную структуру (MLP) вместо линейных обновлений

### Развитие в Nested Learning
- Titans: один специализированный модуль долгосрочной памяти с обучением во время инференса
- NL: многоуровневая система непрерывной памяти (CMS) как обобщение идеи
- NL: применение принципов к оптимизаторам (глубокие оптимизаторы)
- NL: более общая теоретическая основа для объяснения эффективности

## Экспериментальные результаты

### Языковое моделирование и рассуждения
- Варианты Titans стабильно превосходят трансформеры-бейзлайны и SOTA линейные рекуррентные модели
- Превосходят Mamba, RetNet и DeltaNet на широком спектре задач

### Задачи с экстремально длинным контекстом
- В задаче Needle-in-a-Haystack (NIAH) вариант MAC сохраняет высокую точность при увеличении длины контекста до 16 тыс. токенов
- На сложной подзадаче S-NIAH-W достигает 95.2%, в то время как Mamba2, DeltaNet и TTT показывают 0.0%
- На бенчмарке BABILong, требующем рассуждений на основе фактов, разбросанных по документам в миллионы токенов, вариант MAC превосходит все бейзлайны, включая гораздо более крупные модели (GPT-4, Llama3.1-70B)

### Другие области применения
- Превосходство в задачах прогнозирования временных рядов
- Отличные результаты в геномике (на бенчмарке GenomicsBenchmarks)

## Практические аспекты

### Реализация
- Использует быстрый, распараллеливаемый алгоритм обучения с тензоризацией операций
- Применяет parallel associative scan для эффективного вычисления момента
- Вычисления доминируются высокооптимизированными матричными умножениями
- Абляции подтверждают, что каждый компонент (момент, распад весов, глубокая память, свёртка) вносит положительный вклад

### Преимущества
- Возможность масштабирования до более чем 2 миллионов токенов
- Превосходство над гораздо более крупными моделями на задачах с длинным контекстом
- Адаптивность и обучение во время инференса
- Более надёжное масштабирование на длинных последовательностях

### Ограничения
- Вычислительные накладные расходы на градиентные обновления во время инференса
- Потенциальные задержки (latency) по сравнению с чисто state-space моделями
- Выразительность LMM связана с его глубиной, что влияет на скорость обучения
- Компромисс между результативностью и производительностью

## Визуализации

![Иллюстрация процесса параллельного обучения нейронной памяти](../../../images/img_1763025480_AgACAgIA.jpg) <!-- TODO: Broken image path -->

Иллюстрация процесса параллельного обучения нейронной памяти

![Архитектура MAC (Memory as Context) - одна из реализаций Titans](../../../images/img_1763025480_AgACAgIA.jpg) <!-- TODO: Broken image path -->

Архитектура MAC (Memory as Context) - одна из реализаций Titans

![Архитектура MAG (Memory as Gate) - одна из реализаций Titans](../../../images/img_1763015497_AgACAgIA.jpg) <!-- TODO: Broken image path -->

Архитектура MAG (Memory as Gate) - одна из реализаций Titans

![Архитектура MAL (Memory as Layer) - одна из реализаций Titans](../../../images/img_1762961996_AgACAgIA.jpg) <!-- TODO: Broken image path -->

Архитектура MAL (Memory as Layer) - одна из реализаций Titans

![Производительность Titans на бенчмарке BABILong по сравнению с бейзлайнами](../../../images/img_1763025480_AgACAgIA.jpg) <!-- TODO: Broken image path -->

Производительность Titans на бенчмарке BABILong по сравнению с бейзлайнами

![Производительность Titans и базовых моделей на задаче NIAH](../../../images/img_1763025480_AgACAgIA.jpg) <!-- TODO: Broken image path -->

Производительность Titans и базовых моделей на задаче NIAH

![Производительность Titans и базовых моделей на задачах языкового моделирования и рассуждений](../../../images/img_1763025480_AgACAgIA.jpg) <!-- TODO: Broken image path -->

Производительность Titans и базовых моделей на задачах языкового моделирования и рассуждений

![Сравнение сложности архитектур: Titans, трансформеры и SSM](../../../images/img_1763025480_AgACAgIA.jpg) <!-- TODO: Broken image path -->

Сравнение сложности архитектур: Titans, трансформеры и SSM

## Значение для ИИ

Titans представляет собой значительный шаг в создании по-настоящему масштабируемых и эффективных последовательных моделей. Архитектура:
- Объединяет точность внимания с эффективностью рекуррентных подходов
- Вводит новую парадигму для создания последовательных моделей с надёжной, адаптивной памятью
- Показывает, что будущее моделирования последовательностей может лежать в "конфедерации специализированных систем памяти"
- Открывает путь к системам ИИ, способным эффективно обрабатывать и рассуждать над огромными объёмами данных

## Связи с другими темами

- [[nested_learning.md]] - Развитие Titans в рамках более общей теории вложенного обучения
- [[hope_architecture.md]] - Архитектура, развивающая идеи Titans
- [[continuum_memory_system.md]] - Развитие концепции памяти из Titans
- [[lmm_long_term_memory_module.md]] - Подробное описание модуля LMM
- [[mac_mag_mal_architectures.md]] - Подробное описание архитектур MAC, MAG, MAL
- [[../nlp/transformers/neuroscience_principles_in_transformers.md]] - Нейронаучные принципы, использованные в Titans
- [[../nlp/transformers/mat_memory_operations.md]] - Операции памяти, вдохновленные Titans
- [[../nlp/transformers/mat_evolution_history.md]] - Историческое место Titans в развитии MAT
- [[rehearsal/experience_replay.md]] - Альтернативный подход к непрерывному обучению

## Источники

1. [Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663) - Оригинальная статья, описывающая архитектуру Titans, модуль LMM и архитектуры MAC, MAG, MAL
2. [ArXivIQ Review: Titans Learning to Memorize at Test Time](https://arxiviq.substack.com/p/titans-learning-to-memorize-at-test) - Обзор статьи с углубленным анализом ключевых концепций
3. [Nested Learning: The Illusion of Deep Learning Architectures](https://abehrouz.github.io/files/NL.pdf) - Статья, описывающая развитие идеи от Titans до Nested Learning