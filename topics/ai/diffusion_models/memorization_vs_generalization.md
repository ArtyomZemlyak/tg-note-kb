# Меморизация против обобщения в диффузионных моделях

## Общее описание

Одним из ключевых открытий в недавней работе "Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training" стало объяснение парадокса: почему перепараметризованные диффузионные модели обладают хорошей обобщающей способностью, хотя имеют ёмкость для идеального запоминания обучающих данных. Работа получила Best Paper Award на NeurIPS 2025.

## Парадокс идеального скора

Основное напряжение при обучении генеративных моделей глубокого обучения кроется в самой целевой функции. В контексте Denoising Diffusion Probabilistic Models (DDPMs) сеть обучается аппроксимировать функцию скора (градиент логарифма плотности распределения) ∇ₓ log p_t(x). Теоретически, если модель обладает достаточной ёмкостью и обучается до сходимости на конечном датасете из n примеров, оптимальным решением для эмпирического лосса будет поле скора, которое заставляет обратный процесс коллапсировать в конкретные обучающие примеры — то есть чистое запоминание. Это глобальный минимум задачи обучения.

Однако на практике модели типа Stable Diffusion генерируют новые изображения. Предыдущие гипотезы предполагали, что дело в архитектурной регуляризации (например, inductive bias свёрток) или недостаточной ёмкости. Новое исследование предлагает иное решение: модель спасает сама *динамика* градиентного спуска. Система выучивает гладкую геометрию распределения популяции задолго до того, как у неё появляется время разрешить острые минимумы, связанные с переобучением на конкретных точках.

## Две временные шкалы обучения

Исследование выделило два различных временных масштаба в обучении диффузионных моделей:

### τ_gen (Время генерации)
- Время, когда модель учится генерировать валидные сэмплы
- Связано с обучением основной массе спектрального распределения
- Модель генерирует высококачественные разнообразные изображения, которые соответствуют общему многообразию, но не копируют конкретные обучающие примеры
- Время до τ_gen остается константой независимо от размера датасета

### τ_mem (Время меморизации)
- Время, когда модель начинает запоминать конкретные примеры из обучения
- Связано с обучением хвостовой части спектра, соответствующей шуму конечной выборки
- Модель начинает разрешать высокочастотные компоненты эмпирического скора — по сути, "пики" вокруг n обучающих точек
- **Ключевое открытие**: τ_mem пропорционально n (размеру датасета). При фиксированной ёмкости модели удвоение размера датасета эффективно удваивает время до начала запоминания

## Механизм спектрального разложения

Для строгого анализа динамики авторы абстрагировали сложную архитектуру U-Net до многомерной нейронной сети со случайными признаками (Random Features Neural Network, RFNN). В такой постановке функция скора s_A(x) параметризуется обучаемой матрицей весов A, действующей на фиксированные нелинейные признаки.

Аналитическая прозрачность такой установки исходит из трактуемости динамики обучения. Эволюция весов A под действием градиентного потока управляется собственными числами корреляционной матрицы признаков U. Это позволяет авторам использовать теорию случайных матриц для разложения процесса обучения на спектральные компоненты.

Главный математический инсайт: ландшафт лосса имеет "жёсткое" направление, соответствующее распределению популяции (большие собственные числа, выучиваются быстро), и "мягкие" направления, соответствующие шуму конечной выборки (маленькие собственные числа, выучиваются медленно).

## Практические последствия

### Ранняя остановка как необходимость

Практическое следствие этой "неявной динамической регуляризации" заключается в том, что ранняя остановка (early stopping) — это не просто эвристика для предотвращения роста ошибки на тесте, а структурная необходимость для скор-моделирования.

Авторы строят фазовую диаграмму обобщения против запоминания в плоскости (n, p). Они определяют границу n*(p), ниже которой у модели достаточно ёмкости для запоминания. Однако из-за временного лага τ_mem - τ_gen существует огромная операционная область, где n < n* (модель *могла бы* запомнить), но t < τ_mem (модель *ещё не* запомнила). Это объясняет успех массивных моделей, обученных на массивных датасетах: время, необходимое для оверфиттинга датасета, растёт настолько сильно, что эффективно превышает бюджет вычислений, используемый на практике.

### Эффект увеличения датасета

Увеличение размера датасета n расширяет "безопасное окно" для обучения, позволяя огромным моделей надёжно обобщать данные. Это объясняет, почему увеличение датасетов расширяет безопасное окно для обучения, позволяя перепараметризованным моделям обобщать данные, не переобучаясь.

## Математическая структура

В асимптотическом пределе, где d (размерность данных), p (количество параметров), n (размер датасета) стремятся к бесконечности, авторы подтверждают масштабирующий закон. Правило обновления весов в перепараметризованном режиме (где p много больше n) следует градиентному потоку:

`Ȧ(τ) = -d² ∇_A L_train`

Проецируя эту динамику на собственные векторы U (корреляционной матрицы), исследователи доказывают, что время сходимости для компонентов запоминания обратно пропорционально наименьшим собственным числам в основной массе, что приводит к масштабированию:

`τ_mem ≈ ψ_n / Δ_t`, где `ψ_n = n/d`

## Ограничения и нюансы

Хотя анализ RFNN даёт строгий фундамент, он упрощает возможности обучения признаков в глубоких CNN или трансформерах, предполагая фиксированные фичи в первом слое. Исследователи признают, что хотя они тестировали это на U-Nets с SGD, современные реализации часто используют Adam, который может ускорить прохождение по спектру собственных чисел, потенциально сокращая разрыв между τ_gen и τ_mem. Кроме того, анализ проводится в основном для безусловной генерации; динамика с conditional guidance (например, text-to-image) может привнести новые спектральные свойства, не учтённые здесь.

## Значение для практики

Для исследователей, работающих на фронтире обучения крупномасштабных генеративных моделей, это подкрепляет стратегию увеличения размера датасета n не только ради разнообразия, но и для активной отсрочки начала запоминания. Это гарантирует, что модель тратит свой вычислительный бюджет на изучение многообразия, а не отдельных сэмплов.

## Связи с другими темами

- [[../../theory/implicit_dynamical_regularization.md|Неявная динамическая регуляризация]] - теоретический каркас для понимания динамической регуляризации
- [[../llm/diffusion_models.md|Диффузионные модели]] - общие сведения о диффузионных моделях
- [[../../theory/unified_theory_of_diffusion_models.md|Единая теория диффузионных моделей]] - объединяющий теоретический контекст
- [[../../machine_learning.md|Машинное обучение]] - общие концепции об обобщении и переобучении
- [[../../continual_learning/catastrophic_forgetting/catastrophic_forgetting.md|Катастрофическое забывание]] - смежная концепция в области запоминания и обобщения

## Источники

1. [Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training](https://openreview.net/forum?id=BSZqpqgqM0) - основная статья, описывающая временную динамику обобщения и запоминания в диффузионных моделях
2. [Why Diffusion Models Don't Memorize (arXiv)](https://arxiv.org/abs/2505.17638) - полная версия статьи с детальным математическим анализом
3. [NeurIPS 2025 Review](https://arxiviq.substack.com/p/neurips-2025-why-diffusion-models) - ревью статьи, получившей Best Paper Award
4. [GitHub Repository](https://github.com/tbonnair/Why-Diffusion-Models-Don-t-Memorize) - исходный код для воспроизведения результатов исследования