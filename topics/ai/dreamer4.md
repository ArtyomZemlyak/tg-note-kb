# Dreamer 4: Масштабируемая модель мира в реальном времени

## Общее описание

**Dreamer 4** — это агент с 2 миллиардами параметров, представляющий собой важный прорыв в области воплощенного ИИ (embodied AI). Его ключевая особенность — использование масштабируемой модели мира, обученной с новой целевой функцией "shortcut forcing", что позволяет симулировать сложную игровую механику в реальном времени (21 FPS на одной GPU) и эффективно обучаться в "воображении" без необходимости реального взаимодействия со средой.

## Основные достижения

### Решение задачи добычи алмазов в Minecraft
- Первый агент, который решил сложную задачу по добыче алмазов в Minecraft, обучаясь исключительно на оффлайн-датасете
- Требует последовательности из более чем 20,000 дискретных действий
- Вероятность успеха: 0.7% в 1000 эпизодах
- Создание каменной кирки: 90.1% успеха
- Создание железной кирки: 29.0% успеха
- Использует в 100 раз меньше данных, чем агент VPT от OpenAI

### Масштабируемость и эффективность
- Обучение стратегии полностью внутри модели мира ("в воображении")
- Возможность обучения на видео без разметки действий
- Обобщение знаний на новые среды, визуально отличающиеся от обучающих

## Архитектура

### Модель мира
Dreamer 4 состоит из:

1. **Каузального токенизатора (Causal Tokenizer)**
   - Преобразует визуальные наблюдения в токены для обработки трансформером
   - Обеспечивает эффективное представление визуальной информации

2. **Интерактивной динамической модели (Interactive Dynamics Model)**
   - Построена на основе эффективной трансформерной архитектуры
   - Предсказывает эволюцию состояния среды на основе действий агента
   - 2B параметров для высокоточной симуляции

### Инженерные оптимизации
- **Факторизованное внимание**: разделение плотного внимания на отдельные слои внимания по пространству и времени
- **Разреженное временное внимание**: временное внимание применяется только раз в четыре слоя для концентрации вычислений на текущем кадре
- **Grouped-Query Attention (GQA)**: используется во всех слоях внимания для уменьшения размера KV-кэша

## Инновационный подход: Shortcut Forcing

### X-Prediction
- Вместо традиционного предсказания вектора обновления (v-prediction) используется предсказание чистого конечного состояния (x-prediction)
- Предсказание структурированной цели (финальное изображение) проще для обучения и менее склонно к накоплению ошибок
- Критически важно для стабильности на длинных горизонтах предсказания

### Рамповый вес лосса
- Вес целевой функции определяется рамповой функцией: ω(τ) = 0.9τ + 0.1
- Линейно возрастает с уровнем сигнала τ (от чистого шума при τ=0 до чистых данных при τ=1)
- Фокусирует ресурсы модели на уровнях сигнала, несущих наибольшую полезную информацию

### Bootstrap loss для крупных шагов
- Итоговый лосс сочетает компонент flow matching для мелких шагов и bootstrap loss для более крупных шагов
- Все формулировано в пространстве x-предсказаний
- Обучает модель генерировать кадр за один большой шаг, "бутстрэпясь" из двух шагов вдвое меньшего размера

## Тренировка в воображении

### Трехэтапный процесс
1. **Предобучение модели мира**: токенизатор и динамическая модель обучаются на видео (и опционально на действиях)
2. **Файнтюнинг агента**: стратегия и модель вознаграждения обучаются с помощью поведенческого клонирования
3. **Обучение в воображении**: стратегия дообучается с помощью RL на траекториях, полностью сгенерированных моделью мира

### PMPO - робастная целевая функция
- Использует Preference optimization as probabilistic inference
- Игнорирует величину вознаграждений, что естественным образом балансирует мультизадачное обучение
- Использует знак функции преимущества и ограничение KL-дивергенции с априорным поведенческим распределением

## Преимущества подхода

### Безопасность и эффективность
- Обучение без опасного или дорогостоящего онлайн-взаимодействия со средой
- Возможность безопасного обучения на основе статичных наборов данных
- Эффективное использование данных (меньше данных, больше результатов)

### Масштабируемость
- Возможность обучения на огромных массивах видео без разметки
- Обобщение знаний, полученных из одной среды, на другие среды
- Потенциал для использования больших интернет-архивов видеоданных

### Реальная производительность
- Симуляция в реальном времени (21 FPS на одной GPU H100)
- Контекстное окно 9.6 секунд
- Генерация высококачественного видео за K=4 шага (вместо 64 для традиционной диффузии)

## Ограничения

### Память и точность
- Короткая память (ограничена 9.6-секундным контекстом)
- Неточные предсказания инвентаря
- Несовершенная симуляция игровой среды

### Сложность задач
- Низкий показатель успеха в задаче добычи алмазов (0.7%), несмотря на знаковое достижение
- Сложность задачи с длинным горизонтом планирования

## Будущие направления

- Интеграция долговременной памяти
- Включение понимания языка для следования инструкциям
- Автоматический поиск подзадач для декомпозиции задач с длинным горизонтом

## Связи с другими темами

- [[computer_vision/world_models.md|World модели]] - основная концепция, на которой основан Dreamer 4
- [[embodied_ai.md|Embodied AI]] - область, которой принадлежит подход
- [[../reinforcement_learning/deep_rl/index.md]] - методы обучения с подкреплением
- [[../reinforcement_learning/practical_challenges/exploration_exploitation.md#model-based-rl]] - подходы с моделированием среды для планирования
- [[../dreamer.md|Dreamer модели]] - история развития семейства моделей
- [[diffusion_models.md|Диффузионные модели]] - архитектура, используемая в современных world моделях
- [[../flow_matching.md|Flow matching]] - теоретическая основа новой целевой функции
- [[../theory/unified_theory_of_diffusion_models.md|Единая теория диффузионных моделей]] - объединяющая теоретическая основа для VAE, score-функций и flow matching подходов
- [[minecraft_ai.md|Minecraft AI]] - конкретное приложение и тестирование