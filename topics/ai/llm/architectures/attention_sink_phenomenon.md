# Attention Sink: Проблема избыточного внимания к первому токену

## Описание

Attention Sink - это феномен, наблюдаемый в трансформерах, при котором модель избыточно направляет внимание на первый токен последовательности (или специальные "sink tokens"). Это происходит из-за ограничения суммы вероятностей внимания, равной 1.0, накладываемого softmax-нормализацией. Даже когда полезной информации в контексте нет, модель вынуждена "сливать" часть внимания в первый токен, просто чтобы удовлетворить это ограничение.

## Механизм возникновения

### Причины появления

Attention Sink возникает по нескольким причинам:

1. **Свойства Softmax**: Softmax гарантирует, что сумма всех весов внимания будет равна 1.0, что заставляет модель распределять "лишнюю" массу внимания на какие-то токены.

2. **Нормализация**: Попытки стабилизации обучения могут привести к накоплению массы внимания в одном месте.

3. **Геометрические ограничения**: Векторы скрытых состояний могут быть устроены так, что первый токен становится "магнитом" для внимания.

### Последствия

- Ухудшение качества понимания контекста
- Потеря внимания на действительно важных токенах
- Проблемы с экстраполяцией длины контекста
- Нестабильность при обучении больших моделей

## Связь с другими феноменами

Attention Sink часто связан с феноменом Massive Activations - когда определённые каналы скрытых состояний накапливают огромные значения (выбросы). Эти два явления усиливают друг друга, создавая проблемы с обучением и стабильностью моделей.

## Решения и подходы

### Традиционные подходы

Раньше проблему решали костылями, например:
- StreamingLLM - эвристический подход к управлению KV-кешированием
- Специальные "sink tokens" - добавление специальных токенов для поглощения избыточного внимания

### Современные решения

Один из самых перспективных подходов - использование Gated Attention, который механистически устраняет феномен Attention Sink, добавляя обучаемый сигмоидный гейт сразу после выхода Scaled Dot-Product Attention (SDPA). Это позволяет модели "отвергать" неполезный выход внимания, не полагаясь на нормализацию Softmax.

## Влияние на обучение

Attention Sink особенно проблематичен при обучении больших моделей, поскольку:
- Приводит к нестабильности обучения
- Создает артефакты в распределении внимания
- Усложняет экстраполяцию длины контекста
- Увеличивает вероятность спайков лосса

## Экспериментальные наблюдения

Исследования показывают, что в обычных моделях может наблюдаться до 46.7% внимания, направленного на первый токен, в то время как использование Gated Attention снижает этот показатель до ~4.8%.

## Связь с архитектурами

Некоторые современные архитектуры (например, Qwen3-Next) активно используют подходы, которые решают проблему Attention Sink на архитектурном уровне, включая гейтированные механизмы внимания.

## Сравнение с другими архитектурами

- В стандартных трансформерах: сильный Attention Sink
- В моделях с Flash Attention: частично смягчается, но не устраняется
- В моделях с Gated Attention: практически устраняется
- В архитектурах типа Mamba: альтернативный подход, избегающий проблемы за счёт другой структуры

## Визуализации и данные

![Proportion of attention allocated to the initial token per layer](../../../../media/img_1764342547_aqadzw1rgr9sul_figure_2_left_proportion_of_attention.jpg)

**Описание:** График показывает, какая доля внимания выделяется первому токену на каждом слое. В базовой модели наблюдается значительная проблема attention sink - в среднем 46.7% оценок внимания по слоям направляются на первый токен. Введение гейта эффективно устраняет эту проблему, снижая долю до 4.8%. На правом графике показаны средние веса карты внимания для каждой головы. На 21 слое в базовой модели наблюдается сильный attention sink (83% на первом токене), который существенно снижается при использовании гейта (4%). В финальном выходном слое гейт усиливает существующую тенденцию модели к обращению внимания на отдельные токены в последовательности.

## Связи с другими темами

- [[gated_attention_mechanism.md|Gated Attention: Механизм с гейтированием для стабильности LLM]] - архитектурное решение проблемы
- [[massive_activations_problem.md|Massive Activations: Проблема огромных активаций в трансформерах]] - связанная проблема
- [[specialized_attention_mechanisms.md|Специализированные механизмы внимания]] - контекст для других улучшенных механизмов
- [[streamingllm_approach.md|StreamingLLM Approach]] - альтернативное решение проблемы (если существует в базе знаний)

## Источники

1. [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://arxiv.org/abs/2505.06708) - подробный анализ проблемы Attention Sink и решение с помощью Gated Attention
2. [MITIGATING ATTENTION SINKS AND MASSIVE ACTIVATIONS](https://arxiv.org/html/2510.22603v1) - статья о проблемах Attention Sink и Massive Activations
3. [Attention Sink Phenomenon in Transformers](https://www.emergentmind.com/topics/attention-sink-phenomenon) - объяснение феномена Attention Sink
4. [Is "Attention" Really All You Need? This New Paper Says No](https://medium.com/@ninza7/is-attention-really-all-you-need-this-new-paper-says-no-2a451fe1e6f5) - обсуждение проблемы "Attention Sinks" и "Massive Activations"