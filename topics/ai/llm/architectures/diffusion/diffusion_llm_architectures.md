# Диффузионные LLM (DLLM)

## Общее описание

Диффузионные LLM (Diffusion-based Large Language Models, DLLM) - это класс невавторегрессивных языковых моделей, которые генерируют текст, постепенно восстанавливая его из шума с использованием диффузионного процесса. В отличие от традиционных LLM, которые генерируют текст по одному токену за раз, DLLM генерируют весь текст (или большие фрагменты) параллельно, что может значительно ускорить инференс.

## Принцип работы

### Диффузионный процесс
- **Прямой процесс (шумоподобие)**: Последовательность токенов постепенно разрушается путем добавления шума в несколько шагов
- **Обратный процесс (генерация)**: Модель обучается восстанавливать исходный текст из зашумленной версии, шаг за шагом уменьшая уровень шума
- **Параллельная генерация**: На каждом шаге восстановления модель может обновлять все позиции одновременно

### Архитектура модели
- **Условная генерация**: Обычно моделируют P(x|context), где x - целевая последовательность, а context - входной контекст
- **Многошаговый процесс**: Требует нескольких проходов через модель для полной генерации (в отличие от одного прохода в автoregressive моделях)
- **Стохастичность**: Включает элементы случайности на каждом шаге генерации

## Преимущества DLLM

- **Параллельная генерация**: Возможность генерировать все токены последовательности одновременно, потенциально значительно ускоряя инференс
- **Независимость от длины**: Время генерации не зависит от длины выходной последовательности (в отличие от автoregressive моделей)
- **Глобальное моделирование**: Модель может учитывать глобальные зависимости с самого начала генерации
- **Контролируемая генерация**: Возможность более тонкого контроля над процессом генерации через шумовые уровни

## Ограничения DLLM

- **Многошаговый инференс**: Требует нескольких проходов через модель для полной генерации, что может компенсировать выгоду от параллелизма
- **Сложность обучения**: Обучение диффузионных моделей для дискретных данных (токенов) более сложно, чем для непрерывных
- **Качество генерации**: Могут уступать автoregressive моделям в качестве из-за приближенного характера диффузионного процесса
- **Вычислительные затраты**: Общие затраты на генерацию могут быть выше из-за необходимости нескольких проходов

## Техники и реализации

### Классы DLLM
- **Diffusion-LM**: Ранние попытки применения непрерывных диффузионных моделей к генерации текста
- **Discrete Diffusion**: Модели, работающие непосредственно с дискретными токенами
- **Masked Diffusion**: Используют маскирование для поступенчатого раскрытия текста

### Алгоритмы
- **DDPM (Denoising Diffusion Probabilistic Models)**: Основа для многих текстовых диффузионных моделей
- **DDIM (Denoising Diffusion Implicit Models)**: Более быстрая версия с меньшим количеством шагов
- **Classifier-free guidance**: Техника для улучшения качества генерации

## Сравнение с автoregressive моделями

| Аспект | DLLM | Автoregressive LLM |
|--------|------|-------------------|
| Генерация | Параллельная | Последовательная |
| Скорость инференса | Потенциально быстрее для длинных последовательностей | Замедляется с увеличением длины |
| Качество | Менее изучено, может уступать | Хорошо изучено, высокое качество |
| Обучение | Сложнее (дискретные данные) | Хорошо изучено |
| Контроль | Через шумовые уровни | Через промпты и пост-обработку |
| Архитектура | Специфичная для диффузионных моделей | Стандартные трансформеры |

## Применение и перспективы

- **Быстрая генерация**: Особенно полезны в сценариях, где требуется быстрая генерация длинного текста
- **Редактирование текста**: Возможность более точного контроля над содержимым генерации
- **Вариационность**: Естественная способность генерировать разнообразные варианты
- **Устойчивость к ошибкам**: Менее подвержены накоплению ошибок, характерному для длинных автoregressive генераций

## Связи с другими темами

- [[../planned_diffusion.md]] - Гибридный подход, объединяющий AR и диффузионные модели в единой архитектуре
- [[../speed_always_wins_survey.md]] - Обзор "Speed Always Wins", описывающий DLLM как одно из ключевых направлений
- [[../llm_architectures_comparison.md]] - Общее сравнение различных архитектур LLM
- [[../non_autoregressive_models.md]] - Общий класс невавторегрессивных моделей, к которым относятся DLLM
- [[../generative_models.md]] - Общие принципы генеративных моделей, основа для понимания диффузионных моделей
- [[../inference_optimization/index.md]] - Оптимизация инференса, включая нестандартные архитектуры
- [[../text_generation_methods.md]] - Методы генерации текста, сравнивающие разные подходы