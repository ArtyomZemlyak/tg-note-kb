# Эксперименты с RuModernBERT в текстовой диффузии (RuModernBERT Diffusion Experiments)

## Описание

Эксперименты с использованием RuModernBERT, mmBERT и ettin-encoder в контексте диффузионной генерации текста. Эти эксперименты демонстрируют возможности конверсии BERT-подобных моделей (изначально созданных для задач понимания текста) в генеративные диффузионные модели.

## Методология

### Основа подхода

Работа основывается на идее, что маскированное языковое моделирование (MLM), используемое в BERT-подобных моделях, на самом деле является частным случаем диффузионного моделирования - одностадийной диффузии. Один шаг восстановления замаскированных токенов в MLM аналогичен одному шагу дедиффузии.

Для преобразования BERT-моделей в генеративные диффузионные модели используется подход:
- Многократное маскирование токенов в предложении с переменной интенсивностью
- Обучение модели восстанавливать текст за несколько шагов (итеративная денойзинг-процедура)
- Использование расписания маскировки от высоких значений (например, 1.0) к низким (например, 0.1)

### Процесс обучения

Вместо традиционного MLM с фиксированным процентом маскировки (~15%), используется переменная маскировка:
1. На первом этапе модель обучается с различной долей замаскированных токенов (от 100% до ~10%)
2. Модель учится восстанавливать исходный текст из различной степени зашумленного состояния
3. Процесс становится более гибким, чем одностадийный MLM

### Двустадийный подход

Можно использовать двухпроходный подход:
1. **Диффузия**: Последовательное замаскировывание токенов в тексте на множестве шагов (создание "коррумпированного" текста)
2. **Дедиффузия**: Обратный процесс восстановления, обучение модели читать зашумленный текст и восстанавливать оригинальные токены

## Использованные модели

### RuModernBERT-base (150M параметров)
- Русскоязычная модель, аналогичная BERT-base
- Цель: повторение экспериментов оригинального исследования на русском языке
- Результаты: генерирует бредовый текст на русском с сильной тенденцией к цикличности
- Повышение температуры не помогает улучшить качество

### mmBERT-base (307M параметров)
- Многоязычная модель с параметрами 307M
- Цель: проверить гипотезу о влиянии масштаба модели на результаты
- Результаты: ожидается, что больший размер поможет улучшить генерацию

### ettin-encoder-1b (1B параметров)
- Модель с 1 миллиардом параметров (не русскоязычная)
- Цель: проверить пределы масштабирования
- Результаты: генерирует связный текст на английском (с легкой тенденцией к цикличности), циклично на русском (предположительно из-за монолингвального претрейна)
- Интересный результат: при переводе фразы "I have a soft spot for broken and puny things" получилась "Я им жалоту у портых и ненких мудств" - показательный пример гибридного языка, полученного через диффузию

### FRIDA (неудачный эксперимент)
- Модель, обученная с использованием контрастивного обучения
- Результат: не смогла эффективно генерировать текст, вероятно, из-за потери способностей к MLM через контрастивное обучение

## Используемые наборы данных

### Tulu-sft (английский)
- Датасет для SFT (Supervised Fine-Tuning) на английском языке
- Использовался для обучения диффузионных моделей

### Grandmaster-Pro-Max (русский)
- Русскоязычный датасет для обучения
- Смешанный с Tulu-sft в равных пропорциях

## Процесс дообучения

Интересной особенностью является возможность дообучения моделей в диалоговом режиме:
- Зашумление и восстановление ответа ассистента
- Оставление запроса пользователя нетронутым
- Использование SFT (Supervised Fine-Tuning) для обучения

## Результаты

### Общие наблюдения
- Все модели показали способность к генерации текста (ожидаемо для диффузионных моделей)
- Качество генерации варьировалось в зависимости от размера и типа модели
- Проблема цикличности была общей для всех моделей, особенно для меньших

### Качество генерации

| Модель | Язык | Качество генерации | Проблемы | Особенности |
|--------|------|-------------------|----------|-------------|
| RuModernBERT-base | Русский | Низкое | Сильная цикличность, бредовый текст | Не подходящая для генерации |
| mmBERT-base | Многоязычный | Среднее | Требует дополнительной проверки | Потенциал масштаба |
| ettin-encoder-1b | Английский | Высокое (связный текст) | Легкая цикличность | Хорошее качество на английском |
| ettin-encoder-1b | Русский | Низкое | Сильная цикличность | Проблема монолингвального претрейна |

### Особенности генерации

#### Англо-русский гибрид
- ettin-encoder-1b при переводе "I have a soft spot for broken and puny things" выдала "Я им жалоту у портых и ненких мудств"
- Это демонстрирует способность модели к созданию гибридных языковых форм
- Интересный побочный эффект диффузионного подхода

![Пример генерации ettin-encoder-1b](../../../../../../media/img_1764121718_aqad6gtrgxqlkel_image.jpg)

**Описание:** Изображение демонстрирует пример генерации текста моделью ettin-encoder-1b, показывая уникальный результат диффузионного подхода к созданию гибридных языковых форм.

## Практические применения

### Точечные модификации текста
- Подход может использоваться для детоксификации текста (CondBERT-подобные задачи)
- Возможность изменения отдельных фрагментов текста без переписывания всего

### FIM-подобные задачи для кода
- Более естественный подход к FIM (Fill-in-the-Middle) для кода
- Вопрос: как зафиксировать число вставляемых токенов

### Text bridging
- Связывание двух несвязанных абзацев
- Та же концепция FIM, но для обычного текста

## Проблемы и ограничения

### Цикличность
- Общая проблема всех моделей, особенно меньшего размера
- Требуется дополнительная настройка расписания маскировки

### Языковая специфичность
- Модели, претрейненные на одном языке, плохо работают с другими
- Требуется мультилингвальный претрейн для универсальных результатов

### Качество генерации
- В целом уступает специализированным автогрессивным моделям
- Подходит больше для специализированных задач, чем для общего использования

## Подходящие задачи

Несмотря на ограничения, BERT-диффузионные модели хорошо подходят для:
- Генерации с сильным контекстным контролем
- Точечных изменений в тексте
- Интерполяции между текстовыми фрагментами
- Задач, где важен двунаправленный контекст, а не скорость

## Архитектурные соображения

### Encoder-only природа
- Модели сохраняют encoder-only архитектуру, что позволяет использовать весь контекст при каждом шаге
- Отличается от decoder-only автогрессивных моделей

### Параллелизм
- Возможность обработки всего текста параллельно на каждом шаге
- В отличие от автогрессивных моделей, без необходимости ожидания предыдущих токенов

## Сравнение с автогрессивными моделями

| Аспект | BERT-диффузия | Автогрессивные модели (GPT и т.д.) |
|--------|---------------|----------------------------------|
| Архитектура | Encoder-only | Decoder-only |
| Контекст | Двунаправленный на каждом шаге | Однонаправленный (слева направо) |
| Итерации | Множественные шаги для генерации | Один проход (токен за токеном) |
| Скорость генерации | Медленнее | Быстрее |
| Глобальное понимание | Лучше (все предложение видно) | Ограниченное (зависит от длины контекста) |
| Качество текста | Уступает специализированным моделям | Выше на общих задачах |

## Источники

1. [Оригинальная репа для обучения диффузионок](https://github.com/) - репозиторий с трейнерами для диффузионных моделей
2. [Форк репы с фиксами](https://github.com/) - улучшенная версия оригинальной репы
3. [Коллекция с весами на huggingface](https://huggingface.co/) - набор предобученных весов диффузионных моделей

## См. также

- [[../bert_diffusion_connection.md]] - Общая концепция связи BERT и диффузии
- [[../text_diffusion_models.md]] - Общие принципы текстовых диффузионных моделей
- [[../roberta_diffusion_generation.md]] - Генерация текста с помощью диффузии RoBERTa
- [[../../nlp/models/bert.md]] - Подробное описание архитектуры BERT