# Модели для экспериментов с диффузионной генерацией текста

## Описание

Коллекция моделей, использованных в экспериментах по преобразованию BERT-подобных моделей в генеративные диффузионные системы. Включает RuModernBERT, mmBERT, ettin-encoder и FRIDA, каждая с различными архитектурными и тренировочными характеристиками.

## RuModernBERT-base

### Общая информация
- **Параметры**: 150 миллионов
- **Язык**: Русский
- **Архитектура**: BERT-base (encoder-only)
- **Цель**: Повторение экспериментов оригинальной работы на русском языке

### Архитектурные особенности
- Стандартная BERT-архитектура с 12 слоями
- Encoder-only природа
- Использует маскированное языковое моделирование (MLM) для предобучения

### Результаты экспериментов
- **Генерация текста**: Да (с примечаниями)
- **Качество на русском**: Низкое
- **Проблемы**: Сильная склонность к цикличности
- **Реакция на повышение температуры**: Недостаточная эффективность

### Ограничения
- Бредовый текст при генерации
- Сложности с сохранением последовательной логики
- Неэффективность при увеличении температуры генерации

## mmBERT-base

### Общая информация
- **Параметры**: 307 миллионов 
- **Язык**: Многоязычный
- **Архитектура**: BERT-base (encoder-only)
- **Цель**: Проверка гипотезы о влиянии масштаба модели

### Архитектурные особенности
- Многоязычная архитектура
- Encoder-only природа
- Предобучение на многоязычных корпрусах

### Результаты экспериментов
- **Генерация текста**: Да (ожидается)
- **Цель масштаба**: Проверить, поможет ли увеличение параметров
- **Предварительные наблюдения**: Требуются дополнительные эксперименты

## ettin-encoder-1b

### Общая информация
- **Параметры**: 1 миллиард
- **Язык**: Английский (монолингвальный)
- **Архитектура**: Encoder-only
- **Цель**: Проверка экстремального масштабирования

### Архитектурные особенности
- Encoder-only архитектура с большим числом параметров
- Не мультилингвальная (в отличие от ожиданий для экспериментов)

### Результаты экспериментов
- **Качество на английском**: Высокое (связный текст)
- **Проблемы на английском**: Легкая склонность к цикличности
- **Качество на русском**: Низкое
- **Проблемы на русском**: Сильная цикличность
- **Причина русскоязычных проблем**: Подозревается монолингвальный претрейн

### Особенности генерации
- **Пример трансляции**: 
  - Вход: "I have a soft spot for broken and puny things"
  - Выход: "Я им жалоту у портых и ненких мудств"
- **Значение**: Демонстрирует способность к созданию гибридных языковых форм через диффузию

### Ограничения
- Ограниченная способность к мультилингвальной генерации из-за монолингвального претрейна
- Цикличность в генерации, особенно на языках, отличных от претрейн-языка

## FRIDA (неудачный эксперимент)

### Общая информация
- **Архитектура**: Encoder-only
- **Особенность**: Обучена с использованием контрастивного обучения
- **Результат**: Неудачная попытка применения в диффузионной генерации

### Проблемы
- **Потерянная способность к MLM**: Подозревается, что контрастивное обучение ухудшило способности к маскированному языковому моделированию
- **Качество генерации**: Генерировала полностью случайный текст
- **Причина неудачи**: Снижение способностей к MLM из-за контрастивного обучения

### Уроки
- Модели с сильным контрастивным обучением могут терять способности к MLM
- MLM-способности важны для успешной диффузионной генерации
- Не все BERT-подобные архитектуры подходят для преобразования в генеративные модели

## Общие наблюдения

### Размер модели и качество
- **RuModernBERT (150M)**: Низкое качество, сильная цикличность
- **mmBERT (307M)**: Ожидается улучшение с увеличением размера
- **ettin-encoder (1B)**: Высокое качество на претрейн-языке, проблемы на других языках

### Языковая специфичность
- Модели показывают лучшие результаты на языке(ах), на которых они были претрейнены
- Монолингвальный претрейн ограничивает мультилингвальные способности
- Многоязычные модели могут иметь лучший потенциал для универсальных задач

### Связь с обучением
- Модели, претрейненные на MLM, имеют лучшие шансы на успешное преобразование
- Контрастивное обучение может ухудшить способности к диффузионной генерации
- Важна сохранность исходных MLM-способностей

## Используемые датасеты

### Tulu-sft
- **Язык**: Английский
- **Применение**: Использован в равной смеси с русскоязычным датасетом
- **Тип**: Набор данных для SFT (Supervised Fine-Tuning)

### Grandmaster-Pro-Max
- **Язык**: Русский
- **Применение**: Использован в равной смеси с английским датасетом
- **Тип**: Русскоязычный датасет для обучения

## Обучающая методика

### Метод SFT (Supervised Fine-Tuning)
- Зашумление и расшумление ответа ассистента
- Оставление запроса юзера нетронутым
- Обучение в диалоговом режиме

### Количество данных
- Использовано 750 тысяч семплов, что признано недостаточным для полной оптимизации
- Достаточно для проверки концепции возможности генерации

## Практические применения

### Успешные кейсы
- Точечные модификации текста (детоксификация)
- FIM для кода (Fill-in-the-Middle)
- Text bridging (связывание несвязанных абзацев)

### Ограничения применения
- Не рекомендуются для общих генеративных задач
- Подходят больше для специализированных задач понимания и модификации текста

## Источники

1. [Оригинальная репа для обучения диффузионок](https://github.com/) - репозиторий с трейнерами для диффузионных моделей
2. [Форк репы с фиксами](https://github.com/) - улучшенная версия оригинальной репы
3. [Репозиторий RuModernBERT](https://huggingface.co/models?search=rumodernbert) - исходная модель (гипотетическая ссылка)
4. [Репозиторий mmBERT](https://huggingface.co/models?search=mmbert) - многоязычная модель (гипотетическая ссылка)

## См. также

- [[rumodernbert_diffusion_experiments.md]] - Подробные результаты экспериментов
- [[bert_generative_conversion.md]] - Преобразование BERT-моделей в генеративные
- [[gemini_diffusion.md]] - Общая информация о диффузионных моделях
- [[../bert_diffusion_connection.md]] - Связь BERT и диффузионных моделей