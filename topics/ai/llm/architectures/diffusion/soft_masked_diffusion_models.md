# Мягко-маскированные диффузионные языковые модели (Soft-Masked Diffusion Language Models)

## Описание

Мягко-маскированные диффузионные языковые модели (Soft-Masked Diffusion Language Models, SM) представляют собой новую технику для масочных диффузионных языковых моделей (MDLM), предложенную для решения ключевой проблемы потери ценной предсказательной информации в процессе дискретного демаскирования. Вместо стандартного бинарного выбора (сохранить токен [MASK] или заменить одним предсказанным вариантом), SM обогащает обратную связь для последующих шагов декодирования, динамически смешивая эмбеддинг токена [MASK] с взвешенной по уверенности выпуклой комбинацией эмбеддингов топ-k предсказанных токенов с предыдущего шага.

## Авторы

Michael Hersche, Samuel Moor-Smith, Thomas Hofmann, Abbas Rahimi

## Ключевые идеи

### Проблема бинарных решений в диффузионных моделях

Традиционные масочные диффузионные языковые модели (MDLM) имеют фундаментальное ограничение: на каждом шаге для каждого замаскированного токена модель принимает бинарное решение — либо заменить маску одним предсказанным токеном, либо оставить её для следующей итерации. Такой выбор отбрасывает богатое вероятностное распределение по всем остальным потенциальным токенам, заставляя модель на следующем шаге переоценивать всё с чистого листа.

### Подход с мягким маскированием

Основная идея SM — сделать токен [MASK] более информативным. Когда маска сохраняется, её эмбеддинг для следующего шага denoising'а больше не статичен. Вместо этого он становится динамической смесью исходного эмбеддинга маски и лучших предсказаний модели с предыдущего шага. Это позволяет модели "подстраховаться" и распространять неопределённость между шагами.

### Непрерывная обратная связь

В отличие от дискретного демаскирования, SM обеспечивает непрерывную, учитывающую неопределенность обратную связь между шагами декодирования, что значительно улучшает производительность модели.

## Математическая основа

Для замаскированного токена в позиции i, который сохраняется для следующего шага, его новый входной эмбеддинг x̃ⁱₜ₋₁ вычисляется как выпуклая комбинация:

x̃ⁱₜ₋₁ = (1 - λ(pⁱₜ₋₁)) ⋅ m + λ(pⁱₜ₋₁) Σ_{j ∈ top-k(pⁱₜ₋₁)} πⱼ ⋅ vⱼ

где:
- m — эмбеддинг токена [MASK]
- vⱼ — эмбеддинг j-го предсказанного токена
- k — количество рассматриваемых лучших предсказаний (небольшой гиперпараметр)
- πⱼ — нормализованная вероятность j-го токена в наборе топ-k
- λ(pⁱₜ₋₁) — динамический весовой коэффициент, основанный на уверенности модели

Коэффициент смешивания λ определяется уверенностью модели, измеряемой как отрицательная энтропия (-H) распределения вероятностей pⁱₜ₋₁ с предыдущего шага:

λ(pⁱₜ₋₁) = ωₛ ⋅ σ(ωₐ(-H(pⁱₜ₋₁) - ω_b))

Здесь ωₛ, ωₐ, ω_b — дополнительные обучаемые параметры, которые позволяют модели выучить оптимальное количество вносимой обратной связи.

## Обучение с прицелом на параллелизм

Ключевое нововведение — методология обучения, избегающая последовательных узких мест. Авторы предлагают распараллеливаемую двухпроходную процедуру обучения:
1. На первом проходе "отсоединённая" (без градиентов) версия модели генерирует вероятности предсказаний
2. Эти вероятности затем используются для вычисления эмбеддингов с мягкой маской, которые служат входом для второго прохода, уже с включёнными градиентами, где вычисляется лосс и обновляются все параметры

Такой подход сохраняет эффективность обучения, характерную для диффузионных моделей, при этом позволяя использовать непрерывную обратную связь.

## Применение и результаты

### Языковое моделирование

- При продолжении предобучения MDLM со 169 млн параметров SM значительно улучшает производительность
- Снижение перплексии на валидационном датасете OpenWebText
- Повышение оценки MAUVE для неограниченной генерации — метрики, которая учитывает как качество, так и разнообразие
- В сочетании с продвинутым планировщиком демаскирования ReMDM достигает оценки MAUVE 0.6053, что почти на 0.3 пункта лучше базовой модели

### Генерация кода

- На моделях класса Dream-7B и Dream-Coder-7B особенно заметны преимущества SM в режимах с высокой пропускной способностью
- На модели Dream-7B с бюджетом NFE (Number of Function Evaluations) в 1/4 SM улучшает точность на MBPP+ на +9.9 пункта и на HumanEval+ на +7.1 пункта
- Это критически важно для приложений, чувствительных к задержкам, так как модель может выдавать высококачественные результаты за меньшее количество шагов уточнения

## Интеграция с другими методами

SM легко интегрируется с другими техниками повышения эффективности. В сочетании с Fast-dLLM (фреймворком для ускорения диффузионных моделей с помощью кэширования и параллельного декодирования) SM стабильно улучшает производительность, особенно при увеличении пропускной способности токенов.

## Преимущества

1. **Улучшенная производительность**: Значительные улучшения в перплексии и оценках MAUVE
2. **Эффективность в режимах с высокой пропускной способностью**: Лучшие результаты при ограниченном числе шагов декодирования
3. **Снижение задержек**: Достижение хороших результатов за меньшее количество итераций
4. **Легковесное улучшение**: Техника легко интегрируется с существующими моделями
5. **Учет неопределенности**: Непрерывная передача информации между шагами декодирования
6. **Параллелизм**: Сохранение преимуществ параллельного выполнения диффузионных моделей

## Ограничения

1. **Повышенная вычислительная стоимость при обучении**: Двухпроходный процесс удваивает количество прямых проходов, необходимых для вычисления лосса за один шаг
2. **Зависимость от гиперпараметров**: Оптимальная конфигурация, например значение k, кажется зависящей от задачи (k=3 лучше для языкового моделирования, k=1 для генерации кода)
3. **Сложность настройки**: Необходимо подбирать параметры модели для оптимальной работы

## Обоснованная интерполяция

Авторы предлагают обоснованную интерпретацию Soft-Masking как выучиваемой интерполяции между двумя схемами диффузии:
- Стандартная масочная DLM (MDLM) использует состояние поглощения ([MASK]), где информация удаляется
- Uniform DLM искажает токены, заменяя их случайными токенами из словаря
- Soft-Masking работает в спектре между этими двумя подходами: когда вес обратной связи λ равен 0, это чистая MDLM; по мере увеличения λ она начинает вести себя как "дополненная маской uniform DLM"

## Будущие направления

1. Интеграция методов на основе обучения с подкреплением для лучшего использования богатого непрерывного сигнала обратной связи
2. Разработка более продвинутых и эффективных стратегий генерации

## Значение и перспективы

Soft-Masking представляет собой важный шаг в развитии диффузионных моделей, делая их более надёжными, эффективными и конкурентоспособными в широком ландшафте генеративного ИИ. Работа подтверждает растущее понимание в этой области: сохранение и распространение неопределённости, а не преждевременная фиксация на дискретных выборах, является ключом к созданию более мощных систем для рассуждений и генерации.

## Связи с другими темами

- [[text_diffusion_models.md]] - Общая информация о текстовых диффузионных моделях
- [[bert_diffusion_connection.md]] - Связь BERT с диффузионными моделями
- [[diffusion_llm_architectures.md]] - Архитектуры диффузионных LLM
- [[../../nlp/models/bert.md]] - Подробное описание архитектуры BERT