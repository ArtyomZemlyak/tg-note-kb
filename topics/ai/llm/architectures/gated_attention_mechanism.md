# Gated Attention: Механизм с гейтированием для стабильности LLM

## Описание

Gated Attention - это архитектурное изменение стандартного механизма внимания в трансформерах, предложенное командой Qwen в статье "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free". Основная идея заключается в добавлении обучаемого гейта, контролируемого входными данными, сразу после выхода масштабированного скалярного произведения внимания (SDPA), что позволяет модели вводить нелинейность и разреженность перед финальной проекцией.

## Проблемы, которые решает Gated Attention

### 1. Проклятие линейности в трансформерах

Стандартная архитектура трансформеров страдает от теоретических ограничений, особенно на масштабе. Главное узкое место - линейность между проекцией Value (W_V) и выходной проекцией Output (W_O). Внутри головы внимания происходит линейная композиция: X W_V · AttentionMatrix · W_O. Поскольку обе проекции линейны, концептуально они могут "схлопнуться" в одно низкоранговое отображение, ограничивая выразительность блока внимания.

### 2. Проблема Attention Sink

Стандартный Softmax заставляет сумму вероятностей быть равной 1.0. Это приводит к феномену Attention Sink, когда модель сливает "лишнюю" массу внимания на первый токен (или специальные "sink tokens") просто чтобы удовлетворить Softmax, даже если полезной информации в контексте нет.

### 3. Massive Activations и нестабильность обучения

Определённые каналы скрытых состояний накапливают огромные значения (выбросы), и модель "смотрит" на первый токен, чтобы уравновесить распределение Softmax. Это приводит к нестабильности обучения, особенно в Mixture-of-Experts (MoE), которые склонны ловить сильные спайки лосса.

## Технические детали

### Модификация механизма внимания

В стандартном Multi-Head Attention (MHA) выход Y - это линейная комбинация векторов значений (Values), взвешенных скорами внимания. Авторы вводят гейт модуляции, работающий от скрытых состояний X (вход в слой после нормализации).

Правило обновления для Gated Attention выглядит так:
```
Y' = Y ⊙ σ(X W_θ)
```

Где:
- Y - выход SDPA (размерности n×d)
- ⊙ - поэлементное умножение
- σ - функция активации (сигмоида)
- X W_θ - обучаемая линейная проекция входа в скор гейта

### Позиция гейта

Авторы экспериментировали с разными позициями (гейтить Values, Keys или финальный Dense выход), но выяснили, что поэлементный гейтинг конкретно выхода SDPA для каждой головы даёт лучший баланс качества и эффективности.

### Принцип работы

В отличие от стандартного внимания, где "шумный" или нерелевантный контекст всё равно передаётся дальше, в Gated Attention система делает параллельную проверку. Пока механизм внимания считает Y_context, проекция гейта W_θ независимо анализирует исходное представление X, чтобы решить, *сколько* внимания сохранить.

Если гейт выдаёт вектор около нуля, он гасит выход внимания практически в ноль. Это работает как нелинейный фильтр: даже если Softmax заставляет веса суммироваться в 1.0 (находя "sink"), гейт может просто умножить этот "мусорный" выход на ноль, разрешая голове буквально "ничего не выдавать".

## Преимущества

### 1. Нелинейность и разреженность

Метод вносит поэлементную разреженность и нелинейность перед финальной проекцией, добавляя мультипликативное взаимодействие, которого нет в ванильных трансформерах.

### 2. Стабильность обучения

На моделях 1.7B dense и 15B MoE, обученных на объёме до 3.5 триллионов токенов, авторы показали, что Gated Attention радикально стабилизирует ландшафт оптимизации. Gated-модели демонстрируют гладкие кривые сходимости без спайков лосса, что позволяет использовать бóльшие learning rates.

### 3. Устранение Attention Sink

Самый интересный инсайт - механистический: Gated Attention естественно устраняет феномены Attention Sink и Massive Activations. Если бейзлайны отдают ~46.7% внимания первому токену, то модели с Gated Attention снижают это число до ~4.8%.

### 4. Минимальный оверхед

Вычислительный оверхед ничтожный (менее 2% wall-time), так как операция гейтинга почти не добавляет флопсов на фоне квадратичной стоимости внимания или жирных матриц FFN.

## Влияние на архитектуру Qwen

Gated Attention уже интегрирован в архитектуры Qwen3-Next, где используется как часть гибрида с Gated DeltaNet в соотношении 3:1 (3 блока Gated DeltaNet на 1 блок Gated Attention). Это позволило:
- Улучшить стабильность обучения
- Значительно расширить длину контекста до 262k токенов
- Снизить спайки лосса в MoE моделях

## Пример работы

Представим, как через этот механизм проходит один токен, например, слово "Apple". В обычном внимании "Apple" (X_apple) делает запрос и получает контекстный вектор Y_context (допустим, связанный с "pie" или "fruit"). Даже если полученный контекст шумный или нерелевантный, стандартное внимание передаёт вектор дальше, потенциально усиливая его в следующем блоке FFN.

В режиме Gated Attention, пока механизм внимания считает Y_context, проекция гейта W_θ независимо анализирует исходное представление X_apple, чтобы решить, *сколько* внимания сохранить. Если анализ показывает, что контекст шумный, гейт может умножить выход на значение близкое к нулю, эффективно "отклонив" неполезную информацию.

## Экспериментальные результаты

- Улучшение перплексии на 15B MoE и 1.7B dense моделях
- Снижение Attention Sink с ~46.7% до ~4.8% внимания на первый токен
- Радикальная стабилизация ландшафта оптимизации
- Возможность использования более высоких learning rates
- Улучшенная экстраполяция на длинный контекст (в сочетании с YaRN от 32k до 128k)

## Ограничения

Несмотря на мощные результаты, статья опирается в основном на эмпирику, а не на строгое теоретическое доказательство связи нелинейности и обобщения длины. Также это архитектурное изменение: его нельзя ретроактивно применить к существующим чекпоинтам без дообучения под новые параметры.

## Визуализации и данные

![Proportion of attention allocated to the initial token per layer](../../../../media/img_1764342547_aqadzw1rgr9sul_figure_2_left_proportion_of_attention.jpg)

**Описание:** График показывает, какая доля внимания выделяется первому токену на каждом слое. В базовой модели наблюдается значительная проблема attention sink - в среднем 46.7% оценок внимания по слоям направляются на первый токен. Введение гейта эффективно устраняет эту проблему, снижая долю до 4.8%. На правом графике показаны средние веса карты внимания для каждой головы. На 21 слое в базовой модели наблюдается сильный attention sink (83% на первом токене), который существенно снижается при использовании гейта (4%). В финальном выходном слое гейт усиливает существующую тенденцию модели к обращению внимания на отдельные токены в последовательности.

![Investigated positions for applying gating operations](../../../../media/img_1764342547_aqadza1rgr9sul_figure_left_investigated_positions_for.jpg)

**Описание:** Слева показаны исследованные позиции для применения гейтинг-операций внутри слоя self-attention. В середине - сравнение производительности (Test PPL и MMLU) 15B MoE моделей с гейтингом, применяемым в различных позициях. Гейтинг после SDPA (G1) дает наилучшие общие результаты. Гейтинг после слоя Value (G2) также демонстрирует заметные улучшения, особенно по PPL. Справа - сравнение потерь при обучении (сглаженные, коэф. 0.9) на 3T токенах между базовой и SDPA-гейтированной 1.7B dense моделями с одинаковыми гиперпараметрами. Гейтинг приводит к более низким итоговым потерям и значительно улучшает стабильность обучения, устраняя всплески потерь. Эта стабильность позволяет использовать потенциально более высокие learning rates и способствует лучшему масштабированию.

## Экспериментальные результаты

![Performance comparison of different gating variants](../../../../media/img_1764342547_aqadzq1rgr9sul_table_1_gating_variant_performance_and.jpg)

**Описание:** Таблица 1 показывает сравнение производительности различных вариантов гейтинга. Видно, что гейтинг после SDPA (G1) дает наилучшие результаты по большинству метрик. Варианты с элемент-виж гейтингом после SDPA показывают значительное улучшение perplexity (PPL) и других метрик по сравнению с базовой моделью.

![Performance of different methods with varying learning rates](../../../../media/img_1764342547_aqadzg1rgr9sul_table_2_performance_of_different_methods.jpg)

**Описание:** Таблица 2 показывает производительность различных методов с разными learning rates, размерами батча и конфигурациями моделей. Видно, что модели с гейтингом после SDPA consistently показывают лучшие результаты по сравнению с baseline, особенно при более высоких learning rates, где базовая модель может расходиться, а гейтированная остается стабильной.

## Связи с другими темами

- [[qwen3_next.md|Qwen3-Next]] - архитектура, в которой используется гибрид Gated DeltaNet и Gated Attention
- [[specialized_attention_mechanisms.md|Специализированные механизмы внимания]] - контекст для других улучшенных механизмов внимания
- [[next_gen_transformer_architectures.md|Следующее поколение архитектур трансформеров]] - контекст для эволюции архитектур
- [[attention_sink_phenomenon.md|Phenomenon Attention Sink]] - проблема, которую решает механизм
- [[massive_activations_problem.md|Massive Activations Problem]] - вторая проблема, которую решает механизм

## Источники

1. [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://arxiv.org/abs/2505.06708) - оригинальная статья команды Qwen, описывающая механизм Gated Attention, его преимущества и экспериментальные результаты
2. [Gated Attention OpenReview](https://openreview.net/forum?id=1b7whO4SfY) - версия статьи на OpenReview
3. [Official Implementation GitHub](https://github.com/qiuzh20/gated_attention) - официальный репозиторий с реализацией Gated Attention
4. [Qwen3-Next Collection](https://huggingface.co/collections/Qwen/qwen3-next) - коллекция моделей Qwen3-Next, в которых используется Gated Attention
5. [ArXivIQ Review](https://arxiviq.substack.com/p/neurips-2025-gated-attention-for) - обзор статьи о Gated Attention на NeurIPS 2025