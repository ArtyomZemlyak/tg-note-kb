# Kimi Delta Attention (KDA)

## Описание

Kimi Delta Attention (KDA) - это улучшенная версия Gated DeltaNet, которая вводит более эффективный механизм гейтирования и оптимизирует использование памяти конечных автоматов. Это линейный механизм внимания, который уточняет правило gated delta с тонкой настройкой гейтирования. KDA представляет собой основной быстрый механизм внимания в архитектуре Kimi-Linear, улучшая эффективность и рассуждения.

## Технические детали

### Особенности KDA

1. **Линейное внимание**: В отличие от традиционного квадратичного внимания, KDA использует линейный подход, что значительно снижает вычислительные и памятевые затраты.

2. **Уточнение gated delta rule**: Введение тонкой настройки гейтирования для более точного контроля над потоком информации.

3. **Эффективное гейтирование**: Более эффективный механизм гейтирования позволяет оптимизировать использование памяти конечных автоматов.

4. **Фокус на изменениях**: В отличие от традиционного внимания, которое пересчитывает всё внимание для каждого токена, KDA фокусируется на изменениях, что снижает вычислительные затраты.

### Сравнение с MLA

- KDA отвечает за основную эффективность и рассуждения в архитектуре
- MLA (Multi-Head Linear Attention) помогает с точностью и стабильностью
- Архитектура Kimi-Linear использует соотношение ~3 части KDA к 1 части MLA

## Архитектурное применение

В гибридной архитектуре Kimi-Linear:
- KDA составляет основную часть слоёв (примерно 3/4)
- MLA используется для обеспечения качества и стабильности (оставшиеся 1/4)
- Такое сочетание позволяет достичь почти уровня больших LLM на длинных контекстах при значительной экономии памяти и времени

## Преимущества

- **Повышенная эффективность**: Быстрее традиционных механизмов внимания
- **Улучшенные рассуждения**: Лучшая способность к логическим выводам
- **Экономия памяти**: Снижение затрат на KV-кэш до 75%
- **Быстрое декодирование**: До 6.3× более быстрое декодирование на длинных контекстах
- **Оптимизация для длинных контекстов**: Особенно эффективен при обработке длинных последовательностей

## Сравнение с другими архитектурами внимания

- **Традиционное Attention**: KDA обеспечивает линейную сложность вместо квадратичной
- **Grouped-Query Attention (GQA)**: KDA фокусируется на изменениях, а не на совместном использовании проекций
- **MLA**: В то время как MLA сжимает тензоры ключей и значений, KDA оптимизирует процесс фокусировки на изменениях

## Использование в моделях

- **Kimi-Linear**: Основная модель, использующая KDA в сочетании с MLA
- Другие потенциальные LLM, нуждающиеся в эффективном механизме внимания для длинных контекстов

## Контекст в области LLM

KDA представляет собой эволюцию подходов к эффективности внимания, предлагая баланс между производительностью моделирования и использованием памяти. Это позволяет создавать более эффективные архитектуры для обработки длинных последовательностей без значительной потери качества.

## Связи с другими темами

- [[mixture_of_experts.md|MoE]] - Может использоваться в сочетании с архитектурами смеси экспертов
- [[multi_head_latent_attention.md|Multi-Head Latent Attention]] - Второй компонент гибридной архитектуры с KDA
- [[kimi_linear.md|Kimi-Linear]] - Модель, использующая KDA
- [[log_linear_attention.md|Log-linear attention]] - Связанные линейные архитектуры внимания
- [[specialized_attention_mechanisms.md|Специализированные механизмы внимания]] - Альтернативные подходы к эффективному вниманию