# Kimi Delta Attention (KDA)

## Описание

Kimi Delta Attention (KDA) - это улучшенная версия Gated DeltaNet, которая вводит более эффективный мелкозернистый механизм гейтирования и оптимизирует использование памяти конечных автоматов. Это линейный механизм внимания, который уточняет правило Gated Delta Rule из работы Gated DeltaNet, добавляя мелкозернистый, поканальный механизм гейтинга для более точного управления рекуррентной памятью. Для аппаратной эффективности используется специально разработанный параллельный алгоритм, работающий по частям (chunkwise) и основанный на ограниченном варианте структуры Diagonal-Plus-Low-Rank (DPLR). Это позволяет достичь почти вдвое большей скорости работы ядра по сравнению с общими формулировками DPLR.

## Технические детали

### Мелкозернистый контроль памяти

KDA вводит мелкозернистый диагонализированный гейт `Diag(αₜ)`, что является прямым улучшением по сравнению с предыдущими работами, такими как Gated DeltaNet (GDN), где использовался более грубый гейт забывания для всей головы внимания. Поканальный механизм позволяет каждой размерности признаков иметь свою независимую скорость забывания, обеспечивая гораздо более точное управление RNN-памятью модели фиксированного размера.

Основная рекуррентная формула выглядит так:
`Sₜ = (I - βₜ kₜ kₜᵀ) Diag(αₜ) Sₜ₋₁ + βₜ kₜ vₜᵀ`

Здесь обновление включает две ключевые операции:
1. Сначала предыдущее состояние Sₜ₋₁ ослабляется мелкозернистым мультипликативным гейтом Diag(αₜ), избирательно забывая информацию
2. Затем ранговое корректирующее обновление (I - βₜ kₜ kₜᵀ), представленное как шаг градиентного спуска, уточняет память на основе новой пары ключ-значение kₜ, vₜᵀ

### Аппаратно-эффективная формулировка DPLR

Хотя общие структуры Diagonal-Plus-Low-Rank (DPLR) обладают высокой выразительностью, их сложно распараллелить, и они требуют больших вычислений. В KDA используется ограниченный вариант DPLR, где переменные скорости затухания и обучения привязаны к вектору ключа kₜ. Это, казалось бы, простое ограничение позволяет использовать специально разработанный и высокооптимизированный параллельный алgorithm, работающий по частям (chunkwise), который существенно сокращает объём вычислений. Он устраняет несколько вторичных этапов разбиения на части и матричных умножений, в результате чего ядро KDA работает почти в 2 раза быстрее, чем общая реализация DPLR.

### Особенности KDA

1. **Линейное внимание**: В отличие от традиционного квадратичного внимания, KDA использует линейный подход, что значительно снижает вычислительные и памятевые затраты.

2. **Уточнение gated delta rule**: Введение тонкой настройки гейтирования для более точного контроля над потоком информации.

3. **Эффективное гейтирование**: Более эффективный механизм гейтирования позволяет оптимизировать использование памяти конечных автоматов.

4. **Фокус на изменениях**: В отличие от традиционного внимания, которое пересчитывает всё внимание для каждого токена, KDA фокусируется на изменениях, что снижает вычислительные затраты.

5. **Мелкозернистое поканальное гейтирование**: Использует диагонализированный гейт `Diag(αₜ)`, позволяющий каждой размерности признаков иметь свою независимую скорость забывания, обеспечивая гораздо более точное управление RNN-памятью.

6. **Оптимизированный параллельный алгоритм**: На основе ограниченного варианта DPLR с 2-кратным ускорением по сравнению с общими формулировками DPLR.

### Сравнение с MLA

- KDA отвечает за основную эффективность и рассуждения в архитектуре
- MLA (Multi-Head Linear Attention) помогает с точностью и стабильностью
- Архитектура Kimi-Linear использует соотношение 3:1 (3 слоя KDA на 1 слой MLA)

### Сравнение с Gated DeltaNet

- KDA обеспечивает гораздо более точное управление RNN-памятью за счёт мелкозернистого поканального гейтирования
- На синтетических задачах извлечения информации и отслеживания состояния KDA сходится значительно быстрее, чем GDN и Mamba2
- KDA улучшает правило Gated Delta с тонкой настройкой гейтирования

## Архитектурное применение

В гибридной архитектуре Kimi-Linear:
- KDA составляет основную часть слоёв (3 из 4 - соотношение 3:1)
- MLA используется для обеспечения качества и стабильности (оставшиеся 1 из 4)
- Такое сочетание позволяет достичь почти уровня больших LLM на длинных контекстах при значительной экономии памяти и времени

## Преимущества

- **Повышенная эффективность**: Быстрее традиционных механизмов внимания
- **Улучшенные рассуждения**: Лучшая способность к логическим выводам
- **Экономия памяти**: Снижение затрат на KV-кэш до 75%
- **Быстрое декодирование**: До 6.3× более быстрое декодирование на длинных контекстах
- **Оптимизация для длинных контекстов**: Особенно эффективен при обработке длинных последовательностей
- **Точное управление памятью**: Мелкозернистое поканальное гейтирование обеспечивает более точное управление рекуррентной памятью
- **Аппаратная эффективность**: Специально разработанный параллельный алгоритм с 2-кратным ускорением
- **Превосходство над базовыми моделями**: Стабильно превосходит сильный бейзлайн на основе полного внимания в сценариях с коротким и длинным контекстом

## Сравнение с другими архитектурами внимания

- **Традиционное Attention**: KDA обеспечивает линейную сложность вместо квадратичной
- **Grouped-Query Attention (GQA)**: KDA фокусируется на изменениях, а не на совместном использовании проекций
- **MLA**: В то время как MLA сжимает тензоры ключей и значений, KDA оптимизирует процесс фокусировки на изменениях
- **Gated DeltaNet**: KDA улучшает Gated DeltaNet с мелкозернистым поканальным гейтированием
- **Mamba2**: KDA показывает лучшую сходимость на задачах извлечения информации и отслеживания состояния

## Использование в моделях

- **Kimi-Linear**: Основная модель, использующая KDA в сочетании с MLA
- Другие потенциальные LLM, нуждающиеся в эффективном механизме внимания для длинных контекстов

## Контекст в области LLM

KDA представляет собой эволюцию подходов к эффективности внимания, предлагая баланс между производительностью моделирования и использованием памяти. Это позволяет создавать более эффективные архитектуры для обработки длинных последовательностей без значительной потери качества. KDA фактически устраняет давний компромисс между качеством модели и вычислительными затратами, стабильно превосходя сильный бейзлайн на основе полного внимания.

## Связи с другими темами

- [[gated_deltanet.md|Gated DeltaNet]] - основная архитектура, улучшенная в KDA
- [[mixture_of_experts.md|MoE]] - Может использоваться в сочетании с архитектурами смеси экспертов
- [[multi_head_latent_attention.md|Multi-Head Latent Attention]] - Второй компонент гибридной архитектуры с KDA
- [[kimi_linear.md|Kimi-Linear]] - Модель, использующая KDA
- [[log_linear_attention.md|Log-linear attention]] - Связанные линейные архитектуры внимания
- [[specialized_attention_mechanisms.md|Специализированные механизмы внимания]] - Альтернативные подходы к эффективному вниманию