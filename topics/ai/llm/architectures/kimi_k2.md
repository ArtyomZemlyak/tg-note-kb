# Kimi K2

## Описание

Kimi K2 - модель с открытыми весами, вызвавшая большой резонанс в AI-сообществе благодаря своей впечатляющей производительности. По бенчмаркам находится наравне с лучшими проприетарными моделями, такими как Gemini, Claude и GPT. Модель имеет 1 триллион параметров, что делает ее одной из самых крупных LLM 2025 года.

## Ключевые архитектурные особенности

### Архитектура на основе DeepSeek V3

Kimi K2 использует архитектуру DeepSeek V3 в качестве основы, но масштабирует её:
- Использует больше экспертов в модулях смеси экспертов (384 эксперта против 256 у DeepSeek-V3)
- Использует меньше голов в модуле многоголового латентного внимания (64 головы против 128 у DeepSeek-V3)

Концепция "sparsity" (разреженность) вводится как разница между общим количеством экспертов и активными экспертами. У Kimi K2 sparsity составляет 48, а у DeepSeek-V3 — 36. Авторы утверждают, что при увеличении sparsity улучшается validation loss модели, но и растёт её инфраструктурная сложность.

### Использование Muon оптимизатора

Заметным аспектом Kimi K2 является использование варианта относительно нового оптимизатора Muon вместо AdamW. Это первый случай, когда Muon был использован для производственной модели такого размера (ранее тестировался только до 16 миллиардов параметров). Это привело к очень хорошим кривым потерь при обучении.

На претрейне Kimi K2 использовался собственный алгоритм Muon, включающий в себя быстрое преобразование к ортогональной матрице. Однако при применении этого метода происходит "взрыв" логитов аттеншена. Чтобы справиться с этой проблемой, авторы устанавливают максимальные логиты для каждой головы. Дальше, всё, что больше заданного T, клипают. Следом идёт рескейлинг матриц W_k и W_q с gamma_h = min(1 или T/на максимальный логит). В случае с обычным MHA все это домножается на гамму, а в случае с MLA скейлятся только не пошаренные веса голов аттеншена.

### Особенности архитектуры внимания

Количество голов внимания в Kimi K2 составляет всего 64 против 128 в DeepSeek-V3. Это решение связано с тем, что удвоение голов даёт прибавку к validation loss всего в 1,2% и кажется нецелесообразным.

## Методы предобучения

### Перефразировка данных
На претрейне авторы перефразировали данные с помощью промптов — то есть буквально переписывали их, сохраняя семантическое родство. Большие тексты разбивались на отдельные фрагменты, которые затем переписывались и подавались в качестве контекста для следующего фрагмента. После десяти перефразирований и одной эпохи прибавка на SimpleQA получается более чем в пять пунктов по сравнению с использованием "оригинального" текста в течение 10 эпох.

## Методы постобучения

### Использование инструментов
На посттрейне использовали 3000 MCP (Model Context Protocol) инструментов с GitHub и ещё 10 тысяч — синтетических инструментов. По инструментам сгенерировали тысячи агентов. Они получили сгенерированные задачи, оценкой которых происходила в режиме LLM-as-a-Judge. Успешные траектории становились базой для обучения.

[[ai/llm/memory/mcp_model_context_protocol.md]] - Подробное описание протокола контекста модели и его роли в интеграции инструментов

## Методы усиленного обучения (RL)

### Использование актора и критика
Для случая, когда нет верифицируемой награды, модель использовали одновременно и как актора, и как критика. Актор генерировал набор ответов, которые критик попарно сравнивал относительно набора аспектов. Сам критик обновлялся за счёт verifiable-сигналов.

## Архитектурные решения

- **Масштабирование DeepSeek V3**: Использование проверенной архитектуры с увеличением параметров до 1 триллиона
- **Muon оптимизатор**: Замена стандартного AdamW для лучшей стабильности обучения
- **Сохранение MLA**: Продолжение использования многоголового латентного внимания
- **MoE с общим экспертом**: Поддержание подхода DeepSeek V3
- **Уменьшенное количество голов внимания**: 64 вместо 128, так как увеличение дает незначительное улучшение
- **Продвинутая обработка логитов**: Использование клиппинга и рескейлинга для решения проблемы взрыва логитов

## Производительность

- **1 триллион параметров**: Одна из самых больших LLM этого поколения
- **На уровне проприетарных моделей**: Конкурирует с Gemini, Claude и GPT по бенчмаркам
- **Гладкие кривые обучения**: Благодаря оптимизатору Muon
- **Улучшенный validation loss**: Благодаря увеличению sparsity

## Контекст в области LLM

Kimi K2 интересна тем, что доказывает концепцию масштабирования проверенной архитектуры (DeepSeek V3) до экстремальных размеров (1 триллион параметров). Вместо разработки новой архитектуры, команда сосредоточилась на масштабировании существующей, но с ключевыми улучшениями, такими как использование Muon оптимизатора.

## Исторический контекст

- Предшественник Kimi 1.5 также был впечатляющим, но не имел опубликованных весов
- Kimi K2 была выпущена как модель с открытыми весами до выхода DeepSeek R2
- На момент выхода является одной из самых впечатляющих моделей с открытыми весами

## Связи с другими темами

- [[deepseek_v3.md|DeepSeek V3]] - Архитектурная основа для Kimi K2
- [[../techniques/mixture_of_experts.md|MoE]] - Подробное объяснение смеси экспертов
- [[../optimization/muon_optimizer.md|Muon]] - Подробное объяснение нового оптимизатора
- [[../optimization/matrix_whitening_optimizers.md|Матричное отбеливание]] - Общее семейство оптимизаторов, к которому относится Muon
- [[multi_head_latent_attention.md|MLA]] - Архитектура многоголового латентного внимания
- [[../specialized_attention_mechanisms.md|Специализированные механизмы внимания]] - Дополнительные архитектуры внимания
- [[pretraining_techniques.md]] - Методы перефразировки данных при предобучении
- [[post_training_methods.md]] - Методы постобучения с использованием инструментов
- [[reinforcement_learning_in_llms.md]] - Использование модели как актора и критика в RL