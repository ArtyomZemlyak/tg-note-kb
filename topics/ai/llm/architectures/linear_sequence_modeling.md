# Линейное моделирование последовательностей

## Общее описание

Линейное моделирование последовательностей - это подход к разработке эффективных архитектур для LLM, основанный на снижении вычислительной сложности с квадратичной (O(n²)) до линейной (O(n)) по длине последовательности. Это позволяет моделям эффективно обрабатывать гораздо более длинные контексты без экспоненциального роста вычислительных требований.

## Основные ветви подходов

### 1. Линейное внимание

Механизмы, которые изменяют вычисления внимания так, чтобы избежать квадратичной зависимости:

- **Принцип работы**: Переписывает вычисление внимания как скалярное произведение с использованием приближений для упрощения вычислений
- **Преимущества**: Линейная вычислительная сложность O(n), работа с очень длинными контекстами, эффективное использование памяти
- **Примеры**: Linformer, Performer, Linear Transformer

### 2. Линейные RNN

Рекуррентные нейронные сети, оптимизированные для эффективного последовательного моделирования:

- **Принцип работы**: Используют рекуррентные расчеты с линейной сложностью по длине последовательности
- **Преимущества**: Эффективное использование памяти, подходят для потоковой обработки
- **Примеры**: Simple Recurrent Units (SRU), Clockwork RNN

### 3. Модели на основе пространства состояний (SSM)

SSM (State Space Models) - это мощный класс моделей, которые моделируют последовательности путем вычисления линейной рекуррентности в скрытом пространстве состояний:

- **Принцип работы**: Преобразуют входную последовательность в непрерывное представление, а затем дискретизуют его для получения вывода
- **Преимущества**: Могут захватывать долгосрочные зависимости с линейной сложностью
- **Примеры**: Mamba, Mamba-2, Mamba-3, Hyena, RetNet

## Применение и сравнение подходов

| Подход | Сложность | Плюсы | Минусы | Примеры моделей |
|--------|-----------|-------|--------|-----------------|
| Линейное внимание | O(n) | Легко интегрировать в архитектуры трансформеров | Меньшая выразительность для некоторых зависимостей | Performer, Linear Transformer |
| Линейные RNN | O(n) | Эффективное использование памяти | Более сложная оптимизация | SRU |
| SSM | O(n) | Мощные долгосрочные зависимости | Более сложная архитектура и обучение | Mamba, RetNet |

## Связь с другими архитектурами

- **Гибридные архитектуры**: Линейные подходы часто комбинируются с полным вниманием для создания сбалансированных архитектур
- **Масштабируемость**: Позволяют эффективно масштабировать модели до очень длинных контекстов, что невозможно с квадратичным вниманием
- **Инференс**: Улучшают скорость генерации и уменьшают требования к памяти

## Примеры применения

- **Mamba**: Современная SSM-архитектура, которая может сочетать лучшие качества трансформеров и рекуррентных моделей
- **Mamba-2 и Mamba-3**: Улучшенные версии Mamba с повышением эффективности и способности к отслеживанию состояния - [[mamba_3_architecture.md]]
- **RetNet**: Использует линейные рекуррентные вычисления вместо внимания для моделирования последовательностей
- **Hyena**: Использует эффективные свертки и скрытые приближения для моделирования долгосрочных зависимостей

## Сравнение с традиционным вниманием

- **Традиционное внимание** обеспечивает точное, но вычислительно дорогое моделирование всех пар позиций
- **Линейные подходы** жертвуют частью выразительности в обмен на масштабируемость
- **Гибридные архитектуры** сочетают оба подхода для оптимального баланса

## Связи с другими темами

- [[specialized_attention_mechanisms.md]] - Специализированные механизмы внимания, включая линейное внимание
- [[state_space_models.md]] - Более подробное описание моделей на основе пространства состояний
- [[hybrid_architectures.md]] - Гибридные архитектуры, сочетающие линейные и традиционные подходы
- [[speed_always_wins_survey.md]] - Обзор "Speed Always Wins", описывающий этот класс подходов
- [[mamba_architecture.md]] - Подробное описание архитектуры Mamba, одного из важных примеров SSM
- [[mamba_3_architecture.md]] - Архитектура Mamba-3 с улучшенной эффективностью и возможностями
- [[llm_architectures_comparison.md]] - Общее сравнение различных архитектур LLM
- [[../../../ai/recsys/llm_based/vista_architecture.md]] - VISTA: архитектура для рекомендательных систем, использующая квазилинейное внимание для обработки длинной истории пользователей с фиксированной стоимостью инференса