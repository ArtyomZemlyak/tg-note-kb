# Llama 4

## Описание

Llama 4 - следующее поколение архитектуры от Meta, продолжающее подходы, установленные в предыдущих версиях Llama. Архитектура включает в себя встроенные мультимодальные возможности и использует подход смеси экспертов (MoE).

## Ключевые архитектурные особенности

### Мультимодальная поддержка

Включает встроенную мультимодальную поддержку, аналогично таким моделям, как Gemma и Mistral, расширяя возможности за пределы чисто текстового моделирования.

### Смесь экспертов (MoE) с классической конфигурацией

Llama 4 использует более классическую конфигурацию MoE с меньшим количеством, но более крупными экспертами:
- 2 активных эксперта на токен (каждый с промежуточным размером 8192)
- В отличие от DeepSeek-V3: 9 активных экспертов (каждый с промежуточным размером 2048)

### Grouped-Query Attention

Использует GQA, в отличие от DeepSeek-V3, которая использует MLA. Это представляет собой более традиционный подход к эффективности внимания.

### Чередование MoE и плотных блоков

Чередует модули MoE и плотные модули в каждом втором блоке трансформера, в отличие от подхода DeepSeek, где MoE используется в каждом блоке (за исключением первых 3).

## Архитектурные решения

- **MoE с крупными экспертами**: 2 активных эксперта с промежуточным размером 8192 каждый
- **GQA вместо MLA**: Более традиционный подход к эффективности внимания
- **Чередование MoE/плотных блоков**: Повышение разнообразия обработки
- **Мультимодальность**: Встроенная поддержка нескольких типов данных

## Сравнение с DeepSeek V3

- **GQA vs MLA**: Использует традиционное GQA вместо MLA
- **Меньше активных параметров**: 17 миллиардов по сравнению с 37 миллиардами у DeepSeek-V3
- **Крупные, но fewer эксперты**: 2 эксперта против 9 у DeepSeek-V3
- **Чередование**: Вместо MoE в каждом блоке использует смешанный подход

## Контекст в области LLM

Llama 4 представляет собой эволюцию архитектуры Llama, добавляя мультимодальность и современные подходы MoE. Важно отметить, что это показывает две разные философии в MoE - Llama 4 использует fewer, но более крупные эксперты, в то время как DeepSeek использует больше, но меньшие эксперты.

## Связи с другими темами

- [[deepseek_v3.md|DeepSeek V3]] - Сравнение с альтернативной MoE архитектурой
- [[../techniques/mixture_of_experts.md|MoE]] - Подробное объяснение смеси экспертов
- [[../techniques/grouped_query_attention.md|GQA]] - Подробное объяснение Grouped-Query Attention
- [[qwen3.md|Qwen3]] - Другой пример MoE архитектуры