# Модуль долгосрочной памяти (LMM) - ключевой компонент архитектуры Titans

## Описание

Модуль долгосрочной памяти (Long-Term Memory Module, LMM) - это центральный элемент новой архитектуры Titans, представленной в статье "Titans: Learning to Memorize at Test Time" исследователями из Google (Ali Behrouz, Peilin Zhong, Vahab Mirrokni). LMM представляет собой глубокий многослойный перцептрон (MLP), который функционирует как метамодель, способная обучаться в процессе инференса.

## Основная информация

LMM кардинально отличается от традиционных подходов к памяти в нейронных сетях. В отличие от статических или предобученных механизмов памяти, LMM обучается адаптивно запоминать и забывать информацию, оптимизируя свои собственные параметры прямо во время инференса. Это достигается за счет использования метрики "удивления" (surprise) на основе градиентов с моментом, что позволяет модели выделять и сохранять важные события, а также адаптивного механизма забывания, предотвращающего переполнение памяти.

## Технические детали

### Архитектура LMM

LMM представляет собой глубокий нелинейный рекуррентный модуль, который работает как мета in-context обучатель. В отличие от простого рекуррентного состояния, LMM - это полноценный MLP, способный к сложным преобразованиям и обучению во время инференса.

### Механизм "удивления" (Surprise)

Процесс обучения LMM основан на принципе "удивления": событие считается более запоминающимся, если оно нарушает текущие ожидания модели. Это формализуется через loss ассоциативной памяти, где модуль памяти пытается предсказать токен-значение по токену-ключу:

l(Mₜ₋₁; xₜ) = ||Mₜ₋₁(kₜ) – vₜ||₂

"Удивление" измеряется градиентом этого loss. Однако, в отличие от предыдущих работ, которые полагались только на сиюминутное удивление (как в Delta Rule), LMM использует правило обновления на основе момента:

Sₜ = ηₜ Sₜ₋₁ - θₜ ∇l(Mₜ₋₁; xₜ)

Здесь Sₜ - это элемент момента, который действует как "память об удивлении", позволяя модели запомнить всё событие целиком, а не только один "шокирующий" токен.

### Адаптивный механизм забывания

Для управления своей ограниченной ёмкостью на миллионах токенов LMM включает адаптивный механизм забывания. Конечное обновление памяти регулируется зависящим от данных гейтом αₜ, который аналогичен распаду весов и позволяет модели удалять устаревшую информацию:

Mₜ = (1 - αₜ) Mₜ₋₁ + Sₜ

### Эффективное обучение

Несмотря на рекуррентную природу, авторы предлагают быстрый, распараллеливаемый алгоритм обучения. Он включает тензоризацию операций внутри блока данных и использование parallel associative scan для эффективного вычисления момента. Это превращает кажущийся последовательным процесс в вычисления, где доминируют высокооптимизированные матричные умножения.

## Компоненты и их вклад

Абляции в статье подтверждают, что каждый компонент LMM - момент, распад весов (забывание), глубокая память и свёртка - вносит положительный вклад в производительность. Более глубокие модули памяти приводят к лучшей перплексии и более надёжному масштабированию на длинных последовательностях.

## Сравнение с другими подходами

LMM превосходит недавние рекуррентные модели на основе градиентов, такие как Gated DeltaNet и TTT (Test-Time Training), благодаря комбинации метрики удивления на основе момента, глубокой нелинейной структуры и адаптивного гейта забывания.

## Применение в архитектуре Titans

LMM интегрируется в общую архитектуру Titans тремя различными способами:
1. Память как контекст (MAC)
2. Память как гейт (MAG) 
3. Память как слой (MAL)

Каждый подход использует LMM по-своему, но сохраняет его основную функцию адаптивного обучения во время инференса.

## Вычислительные аспекты

### Преимущества
- Позволяет эффективно обрабатывать последовательности длиной более 2 миллионов токенов
- Обеспечивает высокую точность при работе с длинным контекстом
- Позволяет модели адаптироваться к новым данным во время инференса

### Ограничения
- Вычислительные накладные расходы на градиентные обновления во время инференса
- Сложность в реализации из-за необходимости поддержки динамического обучения параметров
- Потенциальные проблемы с нестабильностью из-за постоянного обновления параметров

## Значение для ИИ

LMM представляет собой важный шаг к созданию ИИ-систем, способных к непрерывному обучению и адаптации. Архитектура демонстрирует возможность создания моделей, которые:
1. Обучаются на лету в процессе инференса
2. Хранят и извлекают информацию на различных временных масштабах
3. Адаптируются к новым знаниям без забывания старых
4. Эффективно работают с очень длинными последовательностями

## Связи с другими темами

- [[../memory/llm_memory_overview.md]] - Общие системы памяти в LLM
- [[titan_architecture.md]] - Архитектура, в которой используется LMM
- [[nested_learning.md]] - Связанная парадигма от той же исследовательской группы
- [[gated_deltanet.md]] - Сравниваемая архитектура с рекуррентными механизмами
- [[state_space_models.md]] - Класс архитектур, решаящих схожие задачи обработки последовательностей

## Источники

1. [Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663) - Оригинальная статья, описывающая модуль долгосрочной памяти (LMM) и его интеграцию в архитектуру Titans
2. [ArXivIQ Review: Titans Learning to Memorize at Test Time](https://arxiviq.substack.com/p/titans-learning-to-memorize-at-test) - Обзор статьи с углубленным анализом ключевых концепций