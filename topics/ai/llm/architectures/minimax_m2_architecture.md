# Архитектура MiniMax-M2

## Общие сведения

**MiniMax-M2** использует инновационную архитектуру, объединяющую лучшие практики в области моделей трансформеров. Основной подход — это Mixture of Experts (MoE) с 230 миллиардами параметров при активных только 10 миллиардах, что делает её эффективной как в плане вычислительных ресурсов, так и в производительности.

## GPT-OSS архитектура

MiniMax-M2 построена по принципу **GPT-OSS** и использует сочетание **Full Attention** и **Sliding Window Attention (SWA)**, что помогает эффективно работать с длинным контекстом. Эта архитектура позволяет модели:
- Обрабатывать весь контекст целиком для определенных частей модели (Full Attention)
- Концентрироваться на ближайших фрагментах текста для других частей (SWA)

## Механизмы внимания

### Full Attention
- Обрабатывает весь контекст одновременно
- Используется для задач, требующих глобального понимания
- Обеспечивает связь между удаленными частями входного текста

### Sliding Window Attention (SWA)
- Фокусируется на ограниченном "окне" контекста
- Повышает эффективность обработки длинных последовательностей
- Снижает вычислительные затраты на длинных текстах

## Нормализация (RMSNorm)

Важной особенностью архитектуры является использование **RMSNorm** (Root Mean Square Normalization):

- **Каждая голова внимания** имеет собственный RMSNorm
- Это повышает **гибкость** модели при обработке различных типов информации
- Улучшает **устойчивость** модели к различным входным данным
- Помогает в обучении за счет более стабильных градиентов

## RoPE-параметры

- Блоки **Full Attention** и **SWA** используют **разные RoPE-параметры** (Rotary Position Embedding)
- Это позволяет модели по-разному обрабатывать позиционную информацию для разных типов внимания
- Повышает общую **гибкость** и **адаптивность** архитектуры

## Mixture of Experts (MoE)

- **230 миллиардов** параметров в общей модели
- Только **10 миллиардов** параметров активны при каждом вызове
- Позволяет эффективно масштабировать модель без пропорционального увеличения вычислительных затрат
- Каждый "эксперт" специализируется на определенных типах задач или данных

## Преимущества архитектуры

1. **Эффективность**: Использование только 10 миллиардов активных параметров при общем размере в 230 миллиардов
2. **Гибкость**: Разные механизмы внимания для разных типов задач
3. **Масштабируемость**: Архитектура MoE позволяет масштабировать модель без пропорционального увеличения затрат
4. **Стабильность**: Использование RMSNorm улучшает стабильность во время обучения

## Сравнение с традиционными архитектурами

| Характеристика | Традиционные трансформеры | MiniMax-M2 |
|----------------|---------------------------|------------|
| Активные параметры | Все параметры | Только 10B из 230B |
| Внимание | Только Full Attention | Full Attention + SWA |
| Нормализация | Общая нормализация | RMSNorm для каждой головы |
| Эффективность | Высокое потребление ресурсов | Высокая эффективность |

## Связи с другими темами

- [[ai/llm/models/minimax_m2.md]] - Общая информация о модели
- [[ai/llm/architectures/mixture_of_experts_architecture.md]] - Подробнее о MoE архитектуре, ключевом компоненте MiniMax-M2
- [[ai/llm/architectures/specialized_attention_mechanisms.md]] - Подробнее о механизмах внимания, используемых в модели
- [[ai/llm/architectures/log_linear_attention.md]] - Альтернативные архитектуры внимания, для сравнения с SWA
- [[ai/llm/model_quantization_techniques.md]] - Методы оптимизации моделей, релевантные для эффективности MoE
- [[ai/llm/text_generation_methods.md]] - Методы генерации текста, используемые в подобных архитектурах

## Ссылки и дополнительная информация