# Mistral Small 3.1

## Описание

Mistral Small 3.1 24B - модель, выпущенная в марте 2025 года, которая превосходит Gemma 3 27B по большинству бенчмарков (за исключением математики), при этом обладает более низкой задержкой инференса. Основной фокус архитектуры - на минимальную задержку инференса.

## Ключевые архитектурные особенности

### Grouped-Query Attention (GQA) без скользящего окна

В отличие от своих предшественников, использующих внимание со скользящим окном, Mistral Small 3.1 использует стандартное Grouped-Query Attention, как видно из настройки по умолчанию ("sliding_window": null) в официальном файле конфигурации.

### Оптимизация для низкой задержки

Архитектура оптимизирована для минимальной задержки инференса по сравнению с моделями вроде Gemma 3, вероятно благодаря:
- Пользовательскому токенизатору
- Уменьшению KV-кеша и количества слоев
- Возможности использовать более оптимизированный код (например, FlashAttention)

## Архитектурные решения

- **Стандартное GQA**: Вместо GQA со скользящим окном
- **Фокус на задержку**: Быстродействие важнее экономии памяти
- **Пользовательский токенизатор**: Вклад в снижение задержки
- **Меньше слоев**: Для ускорения инференса

## Сравнение с другими моделями

- В отличие от Gemma 3, не использует скользящее окно внимания
- В отличие от DeepSeek V3, использует GQA вместо MLA
- Более низкая задержка по сравнению с Gemma 3 при сопоставимой производительности
- Меньше параметров (24B) по сравнению с Gemma 3 (27B), но высокая производительность

## Контекст в области LLM

Mistral Small 3.1 показывает интересный компромисс - отказ от скользящего окна внимания ради потенциально более оптимизированного кода инференса, что приводит к более низкой задержке. Это демонстрирует, что разные архитектурные решения могут быть оптимизированы для разных метрик (экономия памяти против задержки).

## Связи с другими темами

- [[gemma_3.md|Gemma 3]] - Альтернативный подход со скользящим окном внимания
- [[../techniques/sliding_window_attention.md|Sliding Window Attention]] - Сравнение с архитектурой, использующей скользящее окно
- [[../techniques/grouped_query_attention.md|GQA]] - Подробное объяснение Grouped-Query Attention