# Смесь экспертов (Mixture-of-Experts, MoE)

## Описание

Смесь экспертов (MoE) - архитектурный подход в LLM, при котором стандартные блоки прямого распространения (FeedForward) заменяются несколькими слоями экспертов. Важно, что не все эксперты активны одновременно - маршрутизатор (router) выбирает только небольшое подмножество экспертов для каждого токена.

## Технические детали

### Принцип работы

1. Заменяет один блок прямого распространения несколькими блоками экспертов
2. Маршрутизатор выбирает только несколько экспертов для каждого токена
3. Это делает архитектуру разреженной (sparse) по сравнению с плотными (dense) модулями
4. Увеличивает емкость модели при сохранении эффективности инференса

### Преимущества

- **Увеличение емкости**: Возможность иметь больше параметров во время обучения
- **Эффективность инференса**: Не все параметры используются одновременно
- **Специализация**: Разные эксперты могут обучаться на разных аспектах данных

## Различные подходы к MoE

### Количество и размер экспертов

- **DeepSeek V3**: 256 экспертов, 9 активных на токен, 37B активных параметров
- **Llama 4**: 2 активных эксперта с промежуточным размером 8192 каждый
- **Qwen3**: 8 активных экспертов с промежуточным размером 2048 каждый
- **GPT-OSS**: 32 эксперта, 4 активных на токен, каждый больше, чем в Qwen3

### Общий эксперт (Shared Expert)

Некоторые архитектуры включают общего эксперта:
- **Используется в**: DeepSeek V3, Kimi K2, GLM-4.5
- **Не используется в**: Qwen3, GPT-OSS, Qwen3-Next (до версии Next)
- **Цель**: Стабилизировать обучение и обрабатывать общие паттерны

## Архитектурные решения

### Плотные слои перед MoE

Некоторые архитектуры (GLM-4.5, некоторые реализации) используют несколько плотных слоев перед блоками MoE для:
- Улучшения стабильности сходимости
- Формирования стабильных низкоуровневых представлений до начала маршрутизации экспертов

### Варианты расположения

- **Каждый блок**: MoE в каждом блоке трансформера
- **Чередование**: Чередование MoE и плотных блоков (Llama 4)
- **Частичное использование**: Только в некоторых слоях

## Использование в моделях

- **DeepSeek V3/R1**: Крупномасштабная реализация с 671B общими параметрами
- **Llama 4**: Классическая конфигурация с крупными экспертами
- **Qwen3**: Различные размеры, фокус на балансе
- **GLM-4.5**: С плотными слоями перед MoE блоками
- **Qwen3-Next**: Возвращение к общему эксперту

## Контекст в области LLM

MoE стал ключевой архитектурой для масштабирования LLM в 2025 году, позволяя моделям иметь сотни миллиардов параметров при управляемых затратах инференса. Это ключевой подход для создания моделей с высокой емкостью, которые можно эффективно развертывать.

## Связи с другими темами

- [[multi_head_latent_attention.md|MLA]] - Часто используется в сочетании с MLA в DeepSeek и Kimi K2
- [[deepseek_v3.md|DeepSeek V3]] - Крупная реализация MoE архитектуры
- [[qwen3.md|Qwen3]] - Альтернативная реализация MoE
- [[llama_4.md|Llama 4]] - Другой подход к MoE
- [[../../mixture_of_experts_architecture.md|Общее описание MoE]] - Более подробное объяснение основных концепций MoE