# DeepSeek V3/R1

## Описание

DeepSeek V3/R1 - флагманская модель с архитектурой, впервые представленной в декабре 2024 года, получившая широкое признание после запуска DeepSeek R1 в январе 2025 года. Основные архитектурные особенности обеспечивают вычислительную эффективность и высокую производительность моделирования.

## Ключевые архитектурные особенности

### Многоголовое латентное внимание (MLA)

Multi-Head Latent Attention - это подход к экономии памяти, при котором тензоры ключей и значений сжимаются в пространство меньшей размерности перед сохранением в KV-кеш. Во время инференса они проецируются обратно к исходному размеру. MLA обеспечивает лучшую производительность моделирования по сравнению как с GQA, так и с многоголовым вниманием.

### Смесь экспертов (Mixture-of-Experts, MoE)

DeepSeek-V3 имеет 256 экспертов на модуль MoE с общим количеством 671 миллиард параметров. Однако во время инференса активны только 9 экспертов (1 общий эксперт + 8, выбранных маршрутизатором), что дает 37 миллиардов активных параметров вместо всех 671 миллиардов.

#### Общий эксперт (Shared Expert)

Эксперт, который всегда активен для каждого токена, что повышает общую производительность моделирования, позволяя общим или повторяющимся паттернам не изучаться несколькими отдельными экспертами.

## Архитектурные решения

- **Высокое количество параметров**: 671 миллиард общих параметров
- **Эффективный инференс**: 37 миллиардов активных параметров на токен
- **MLA вместо GQA**: Лучшая производительность моделирования при сравнении с GQA
- **MoE с общим экспертом**: Баланс между емкостью и эффективностью

## Сравнение с другими моделями

- MLA показывает лучшую производительность моделирования по сравнению с GQA
- Архитектура MoE с 9 активными экспертами на токен обеспечивает высокую емкость при инференсе
- В отличие от некоторых других моделей, сохраняет общего эксперта

## Связи с другими темами

- [[../techniques/multi_head_latent_attention.md|MLA]] - Подробное объяснение многоголового латентного внимания
- [[../techniques/mixture_of_experts.md|MoE]] - Подробное объяснение смеси экспертов
- [[qwen3.md|Qwen3]] - Сравнение с другой MoE-архитектурой
- [[kimi_k2.md|Kimi K2]] - Основа архитектуры для триллионной модели