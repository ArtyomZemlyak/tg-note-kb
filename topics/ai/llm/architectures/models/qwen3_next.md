# Qwen3-Next

## Описание

Qwen3-Next - улучшенная архитектура, выпущенная в сентябре 2025 года (80B-A3B), доступная как в вариантах Instruct, так и Thinking. Архитектура основана на предыдущей Qwen3, но включает несколько важных архитектурных инноваций.

## Ключевые архитектурные особенности

### Увеличенное количество экспертов с общим экспертом

Несмотря на то, что модель в 3 раза меньше предыдущей модели 235B-A22B, Qwen3-Next вводит:
- В 4 раза больше экспертов
- Добавляет общего эксперта (в отличие от оригинальной Qwen3, которая его не использовала)

### Гибрид Gated DeltaNet и Gated Attention

Заменяет обычный механизм внимания гибридом Gated DeltaNet и Gated Attention:
- **Gated DeltaNet**: Блоки с линейными и легковесными сверточными слоями, заменяют внимание быстрым обновлением весов
- **Gated Attention**: Стандартное масштабированное скалярное произведение внимания с выходным gate, RMSNorm с нулевым центром для QK-norm и частичным RoPE
- Блоки расположены в соотношении 3:1 (3 блока DeltaNet на 1 блок Attention)

### Многотокенное предсказание (Multi-Token Prediction)

Обучает LLM предсказывать несколько будущих токенов вместо одного на каждом шаге:
- Дополнительные головы (линейные слои) выдают логиты для t+1...t+k
- Суммируются потери перекрестной энтропии для этих смещений
- Ускоряет обучение и поддерживает спекулятивное многотокенное декодирование

## Архитектурные решения

- **4x больше экспертов**: Повышение специализации при уменьшенном общем размере
- **Добавлен общий эксперт**: Стабилизация обучения и обработка общих паттернов
- **Гибрид внимания**: Улучшенная эффективность при длинном контексте (262k токенов)
- **Multi-Token Prediction**: Ускорение обучения и оптимизация инференса

## Встроенный контекст

- **262k токенов**: Встроенная длина контекста в терминах использования памяти
- Это значительно больше, чем у предыдущей модели 235B-A22B (32k изначально, 131k с YaRN)

## Сравнение с оригинальной Qwen3

- Меньше по размеру (80B vs 235B), но с более экспертов
- Возвращает общего эксперта (в отличие от оригинальной Qwen3)
- Использует новую архитектуру внимания вместо GQA
- Включает multi-token prediction

## Контекст в области LLM

Qwen3-Next представляет эволюцию MoE архитектур, возвращая общего эксперта и увеличивая количество экспертов, что противоречит подходу оригинальной Qwen3. Это показывает, что архитектурные решения продолжают развиваться, и подходы, которые ранее считались устаревшими, могут возвращаться с новыми улучшениями.

Также вводит инновационный гибрид внимания, сочетающий преимущества линейных временных моделей (вроде Mamba) с точностью полного внимания, что позволяет эффективно обрабатывать очень длинные контексты.

## Связи с другими темами

- [[qwen3.md|Qwen3]] - Сравнение с оригинальной архитектурой
- [[../techniques/gated_deltanet.md|Gated DeltaNet]] - Подробное объяснение новой архитектуры внимания
- [[../techniques/multi_token_prediction.md|Multi-Token Prediction]] - Подробное объяснение многотокенного предсказания
- [[../techniques/mixture_of_experts.md|MoE]] - Эволюция подходов в смеси экспертов