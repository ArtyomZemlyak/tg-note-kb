# Многоголовое латентное внимание (Multi-Head Latent Attention, MLA)

## Описание

Многоголовое латентное внимание (MLA) - архитектурная техника для экономии памяти в механизмах внимания трансформеров, применяемая в моделях таких как DeepSeek V3/R1 и Kimi K2. MLA сжимает тензоры ключей и значений в пространство меньшей размерности перед их сохранением в KV-кеше, а затем проецирует их обратно к исходному размеру во время инференса.

## Технические детали

### Сравнение с другими подходами

1. **Стандартное многоголовое внимание**: Каждая голова имеет свой собственный набор ключей и значений
2. **Grouped-Query Attention (GQA)**: Несколько голов запросов совместно используют одни и те же проекции ключей и значений
3. **MLA**: Сжимает тензоры ключей и значений в пространство меньшей размерности перед сохранением в KV-кеше

### Преимущества MLA

- **Экономия памяти**: Снижение использования памяти в KV-кеше
- **Лучшая производительность моделирования**: В отличие от GQA, MLA даже немного превосходит стандартное многоголовое внимание
- **Добавление вычислений**: Требует дополнительного матричного умножения, но уменьшает использование памяти

## Использование в моделях

- **DeepSeek V3/R1**: Первая широко известная модель, использующая MLA
- **Kimi K2**: Основана на архитектуре DeepSeek V3, включая MLA
- **Преемник DeepSeek V2**: MLA был представлен в предшественнике, DeepSeek V2

## Сравнение с GQA

Исследования абляции из статьи DeepSeek V2 показали, что:
- GQA показывает худшую производительность, чем многоголовое внимание
- MLA обеспечивает лучшую производительность моделирования, чем многоголовое внимание
- MLA - более сложен в реализации, но дает лучшие результаты

## Техническое объяснение

Во время обучения запросы также сжимаются, но во время инференса (вывода) - нет. Это позволяет экономить память во время инференса, где KV-кеширование наиболее критично.

## Контекст в области LLM

MLA представляет собой эволюцию подходов к эффективности внимания, предлагая баланс между производительностью моделирования и использованием памяти. Это альтернатива GQA, которая часто используется в других современных моделях, но MLA показывает лучшую производительность моделирования.

## Связи с другими темами

- [[mixture_of_experts.md|MoE]] - MLA часто используется в сочетании с архитектурами смеси экспертов
- [[deepseek_v3.md|DeepSeek V3]] - Первая модель, популяризировавшая MLA
- [[kimi_k2.md|Kimi K2]] - Использует масштабированную версию MLA
- [[grouped_query_attention.md|GQA]] - Альтернативный подход к эффективности внимания