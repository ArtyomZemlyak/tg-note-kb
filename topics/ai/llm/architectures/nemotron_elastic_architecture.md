# Nemotron Elastic: Архитектура для эффективных LLM с возможностями рассуждения

## Описание

Nemotron Elastic - это фреймворк для обучения одной «родительской» LLM (12B), внутри весов которой живут полноценные, высокопроизводительные «дочерние» подсети (9B и 6B). Объединяя State Space Models (Mamba) с Attention в гибридной архитектуре, используется пайплайн на базе curriculum learning и дифференцируемый роутер для одновременной оптимизации нескольких размеров моделей под задачи на рассуждение.

## Архитектурные особенности

### Гибридная структура
- **Комбинация Mamba и Attention**: Архитектура объединяет преимущества линейных моделей пространства состояний (Mamba) и механизмов внимания (Attention)
- **Вложенные подсети**: Малые подсети (6B, 9B) являются непрерывными срезами тензоров родительской модели (12B), а не отдельными сущностями
- **Совместимость с SSM**: Архитектура учитывает ограничения SSM (State Space Models), такие как зависимость между состоянием, проекциями и сверточным слоем

![Обзор архитектуры Nemotron Elastic](../../../../media/img_1764253669_aqadrbfrgwvaoul_figure_2_overview_of_the.jpg)

**Описание:** На изображении представлена схема архитектуры Nemotron Elastic, показывающая как в одной родительской модели (12B) содержатся вложенные подсети (6B, 9B) с возможностью динамической маршрутизации.

### Динамическая маршрутизация
- **Дифференцируемый роутер**: Обучаемый end-to-end компонент, управляющий потоком исполнения и динамически генерирующий конфигурации архитектуры под целевой бюджет
- **Gumbel-Softmax релаксация**: Используется для дифференцируемости дискретного выбора архитектуры:
  π_i = exp((z_i + g_i)/τ) / sum(exp((z_j + g_j)/τ))
- **Температура отжига**: Температура τ отжигается от 1.0 до 0.05, постепенно превращая мягкое вероятностное распределение в четкий выбор архитектуры

### Динамические операторы маскирования
- **Group-aware маскирование**: Если голова Mamba маскируется, система неявно маскирует и соответствующее обновление рекуррентного состояния, что гарантирует математическую валидность динамики пространства состояний для подсети
- **Соблюдение структурных ограничений**: Уважает гибридную природу сети, предотвращая «сломанное состояние», возникающее при переносе методов прунинга трансформеров на SSM

## Методы обучения

### Атомарная эластичность
- **Оценка важности компонентов**:
  - Для адаптации ширины: используются магнитуды активаций
  - Для адаптации глубины: используется нормализованная MSE
  - Для компонентов Mamba: учитываются групповые ограничения
- **Скор важности**: Для конкретного слоя j рассчитывается как:
  s_j = sum((M_full - M_-j)^2) / sum(M_full^2)
- **Критическая нормировка**: Гарантирует, что скоры важности остаются сравнимыми на разных датасетах, обеспечивая стабильное ранжирование слоев

### Двухэтапный учебный план
Для стабилизации рассуждений используется двухэтапный подход:

#### Этап 1: Оптимизация на коротком контексте
- Длина последовательности: 8192
- Равномерное сэмплирование бюджета: роутер выбирает конфигурации 6B, 9B и 12B с равной вероятностью

#### Этап 2: Переход к длинному контексту
- Длина последовательности: 49152
- Неравномерное сэмплирование: смещенная вероятность в сторону самой большой модели (например, 12B — 50%, 9B — 30%, 6B — 20%)
- Работает как стабилизатор, отдавая приоритет градиентам, сохраняющим ёмкость полной модели для сложных цепочек рассуждений

## Применение и преимущества

### Экономическая эффективность
- **Сокращение расхода токенов**: Более чем в 360 раз по сравнению с обучением с нуля, и в 7 раз по сравнению с SOTA методами сжатия
- **Фреймворк требует всего 110B токенов** для создания всего семейства (6B, 9B, 12B)
- **Сравнение с другими методами**: Против расчетных 750B токенов для сопоставимого метода NanoV2 Compression или десятков триллионов для обучения с нуля

### Zero-Shot Slicing
- **Единый слепок памяти**: 24GB слепок может обслуживать запросы с разными требованиями по латентности, просто применяя нужные маски на инференсе
- **Сравнение с независимыми моделями**: В отличие от 42GB, необходимых для обслуживания независимых моделей 9B и 12B

### Качество рассуждений
- **Средний балл 77.41** на бенчмарках рассуждений (MATH-500, GPQA и т.д.)
- **Статистическое совпадение** с бейзлайном NanoV2-12B

![Результаты модели на MATH-500 и AIME-2025](../../../../media/img_1764253669_aqadrxfrgwvaoul_model_math_500_aime_2025.jpg)

**Описание:** График сравнения результатов модели Nemotron Elastic на бенчмарках MATH-500 и AIME-2025, демонстрирующий сохранение способностей к рассуждению даже в уменьшенных подсетях.

![Результаты модели на MATH-500 и AIME-2024](../../../../media/img_1764253669_aqadrhfrgwvaoul_model_math_500_aime_2024.jpg)

**Описание:** Результаты модели на бенчмарках MATH-500 и AIME-2024, подтверждающие стабильность производительности в различных задачах рассуждения.

![Диаграмма конфигураций моделей](../../../../media/img_1764253669_aqadsbfrgwvaoul9_on_models_emor_config.jpg)

**Описание:** Диаграмма, показывающая различные конфигурации моделей (6B, 9B, 12B) и распределение ресурсов в рамках эластичного фреймворка.

## Ограничения

- **Зависимость от предобученного учителя**: Фреймворк сильно зависит от наличия высококачественного предобученного родителя (12B) и надежного пайплайна дистилляции
- **Использование замороженного учителя**: Метод лучше подходит для сжатия и адаптации, чем для пре-трейнинга эластичных семейств с нуля
- **Сложность роутера**: Добавляет слой сложности, требует тщательного тюнинга температуры Gumbel-Softmax
- **Ограниченный масштаб**: Пока проверен только до масштаба 12B, вопросы касательно масштабирования до 70B+ или 405B остаются открытыми

## Сравнение с другими подходами

| Аспект | Nemotron Elastic | Традиционные методы | Матрешка представления |
|--------|------------------|---------------------|------------------------|
| Эффективность | Высокая (единичное обучение, много подсетей) | Низкая (отдельное обучение для каждой модели) | Средняя (в основном для эмбеддингов) |
| Гибридные архитектуры | Да (Mamba + Attention) | Нет (обычно однородные) | Не применимо |
| Динамическое масштабирование | Да (роутер выбирает конфигурацию) | Нет | Частично |
| Рассуждения на длинных контекстах | Высокие способности | Часто деградируют при сжатии | Не применимо |

## Связи с другими темами

- [[mamba_architecture.md]] - Архитектура Mamba, компонент гибридной архитектуры
- [[matryoshka_representation_learning.md]] - Принципы вложенных представлений, на которых базируется концепция эластичных моделей
- [[hybrid_architectures.md]] - Гибридные архитектуры LLM, обобщающая тема
- [[dynamic_sparsity_approaches.md]] - Динамические подходы к разреженности в LLM
- [[reasoning_benchmarks.md]] - Бенчмарки рассуждений, на которых тестировалась модель

## Источники

1. [Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs](https://arxiv.org/abs/2511.16664) - Основная статья о Nemotron Elastic, описывающая фреймворк, архитектурные особенности и результаты
2. [Nemotron-Elastic-12B на HuggingFace](https://huggingface.co/nvidia/Nemotron-Elastic-12B) - Модель Nemotron Elastic 12B
3. [Ревью статьи](https://arxiviq.substack.com/p/nemotron-elastic-towards-efficient) - Обзор и анализ статьи Nemotron Elastic