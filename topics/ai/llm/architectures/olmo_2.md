# OLMo 2

## Описание

OLMo 2 - серия моделей от некоммерческой организации Allen Institute for AI, заслужившая признание за прозрачность в отношении обучающих данных и кода. Модель находится на фронте Парето по соотношению вычислений к производительности, предлагая хорошее качество при значительной прозрачности.

## Ключевые архитектурные особенности

### Пост-нормализация (Post-Normalization)

В отличие от большинства современных LLM, которые используют пре-нормализацию (как GPT-2, Llama 3), OLMo 2 использует пост-нормализацию, размещая слои RMSNorm после модулей внимания и прямого распространения (внутри остаточных связей). Это помогает со стабильностью обучения.

### QK-нормализация

Дополнительные слои RMSNorm размещаются для запросов (q) и ключей (k) внутри механизма многоголового внимания перед применением RoPE. Это также стабилизирует обучение, как показано в исследованиях.

### Традиционное многоголовое внимание

В отличие от многих современных моделей, OLMo 2 сохраняет традиционное многоголовое внимание вместо GQA или MLA.

## Архитектурные решения

- **Пост-нормализация с RMSNorm**: Повышает стабильность обучения
- **QK-нормализация**: Дополнительная стабилизация внутри механизма внимания
- **Традиционное многоголовое внимание**: Вместо GQA или MLA
- **Высокая прозрачность**: Подробные технические отчеты и открытые данные

## Контекст в области LLM

OLMo 2 интересна тем, что представляет альтернативный подход к архитектуре трансформера, возвращаясь к более ранним практикам (пост-нормализация), но добавляя современные улучшения (QK-нормализация). Это контрастирует с тенденцией, принятой в большинстве современных моделей.

## Сравнение с другими моделями

- В отличие от Llama 3 и GPT-2 использует пост-нормализацию
- В отличие от DeepSeek и Gemma использует традиционное многоголовое внимание
- QK-нормализация также используется в Gemma 2 и Gemma 3

## Связи с другими темами

- [[../techniques/qk_normalization.md|QK-нормализация]] - Подробное объяснение нормализации запросов и ключей
- [[gemma_3.md|Gemma 3]] - Также использует QK-нормализацию
- [[../techniques/normalization_placement.md|Размещение нормализации]] - Сравнение пре- и пост-нормализации