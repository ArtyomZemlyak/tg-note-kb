# Planned Diffusion

## Описание

Planned Diffusion - это новый гибридный фреймворк для генерации текста, который объединяет сильные стороны авторегрессионных (AR) и диффузионных моделей в единой архитектуре. Метод решает фундаментальную проблему компромисса между скоростью генерации и качеством вывода в больших языковых моделях (LLM).

## Принцип работы

Planned Diffusion работает путём разделения генерации текста на два отдельных этапа, выполняемых одной унифицированной моделью:

### 1. Авторегрессионное планирование

- Процесс начинается с последовательного, авторегрессионного этапа
- Модель генерирует высокоуровневый план выполнения, состоящий из структурных управляющих тегов
- План очерчивает семантическую структуру ответа и разбивает задачу на условно независимые подзадачи
- Например, на просьбу дать определение северного сияния модель может сгенерировать план вида `<topic="definition" len=30><topic="description" len=30>`

### 2. Параллельная диффузия

- План преобразуется в каркас из маск-токенов
- Содержимое для всех определённых фрагментов генерируется одновременно с помощью дискретного диффузионного процесса
- Каждый запланированный фрагмент текста параллельно очищается от шума (denoising)
- Это резко сокращает количество последовательных прогонов модели, необходимых для генерации полного ответа

## Формальная вероятностная факторизация

Гибридный процесс основан на следующей вероятностной факторизации:
Pₚᴅ(z, x|c) = Pₐᵣ(z|c) ⋅ Pᴅ(x|z, c)

Где:
- z - план
- x - содержимое текста
- c - контекст

Важно, что вероятность диффузии Pᴅ далее факторизуется по независимым фрагментам x⁽ᵏ⁾, что позволяет вести параллельную генерацию:
Pᴅ(x|z, c) = ∏_{k=1}^{b(z)} Pᴅ(x⁽ᵏ⁾|z, c)

## Ключевая архитектурная инновация

Механизм, позволяющий одной модели плавно переключаться между последовательным планированием и параллельной генерацией, кроется в её кастомной маске внимания:

- **На этапе планирования**: модель использует стандартную каузальную маску, где каждый токен может обращать внимание только на предыдущие - отличительная черта авторегрессионных моделей
- **На этапе диффузии**: маска трансформируется - внутри каждого независимого фрагмента токены используют двунаправленное внимание, что позволяет им видеть все остальные токены в том же фрагменте
- При этом сами фрагменты маскируются друг от друга, обеспечивая условную независимость, необходимую для параллельной генерации

## Преимущества

- **Решение компромисса скорость-качество**: Значительно сокращает последовательный критический путь, достигая ускорения от 1.27x до 1.81x по сравнению со стандартной AR-генерацией при минимальном снижении качества
- **Единая архитектура**: Избегает сложностей систем с несколькими моделями (например, спекулятивного декодирования)
- **Динамическое планирование**: Рассматривает генерацию текста как задачу динамического параллельного планирования
- **Настраиваемый инференс**: Обеспечивает тонкий контроль над компромиссом между скоростью и качеством с помощью простых runtime-параметров (например, "step ratio" и "confidence threshold")
- **Лучшая масштабируемость**: Качество модели продолжает расти с увеличением числа эпох обучения, в отличие от AR-бейзлайнов, которые выходят на плато

## Ограничения

- **Фактическое ускорение меньше теоретического**: Разрыв объясняется более тяжёлыми вычислениями на каждом шаге и меньшим переиспользованием KV-кэша на этапе диффузии
- **Компромисс с качеством**: Для достижения максимального ускорения приходится немного жертвовать качеством по сравнению с лучшим AR-бейзлайном
- **Сложность KV-кэширования**: Двунаправленное внимание на этапе диффузии не позволяет кэшировать представления токенов до завершения всего фрагмента

## Экспериментальные результаты

- Относительно сильного авторегрессионного бейзлайна, который набрал 50.0% побед с контролем длины (length-controlled win rate, LCWR), стандартная модель Planned Diffusion (PD) достигла 44.6% LCWR при ускорении в 1.81 раза
- Вариант Planned Diffusion with Dense Attention (PD-DA) достиг 49.2% LCWR (падение всего на 0.8 процентных пункта), сохранив при этом ускорение в 1.27 раза

## Связи с другими темами

- [[diffusion_llm_architectures.md]] - Общие принципы диффузионных LLM, на которых основан Planned Diffusion
- [[non_autoregressive_models.md]] - Невавторегрессивные модели, часть более широкого класса подходов
- [[inference_optimization/index.md]] - Оптимизация инференса, включая гибридные архитектуры
- [[llm_architectures_comparison.md]] - Сравнение различных архитектур LLM, включая гибридные подходы
- [[autoregressive_models.md]] - Авторегрессионные модели, одна из составляющих Planned Diffusion
- [[speed_always_wins_survey.md]] - Обзор методов ускорения генерации в LLM