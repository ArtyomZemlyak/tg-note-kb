# QK-нормализация (QK-Normalization)

## Описание

QK-нормализация - архитектурная техника в моделях трансформеров, при которой дополнительные слои RMSNorm применяются к запросам (q) и ключам (k) внутри механизма многоголового внимания перед применением RoPE (Rotary Positional Embeddings). Эта техника помогает стабилизировать обучение.

## Технические детали

### Принцип работы

- **Дополнительные слои RMSNorm**: Применяются к запросам и ключам внутри модуля внимания
- **Расположение**: До применения RoPE, внутри многоголового внимания
- **Цель**: Стабилизация обучения и снижение потерь
- **Происхождение**: Восходит к статье "Scaling Vision Transformers" 2023 года

### Реализация

QK-нормализация представляет собой RMSNorm, применяемый к q и k перед тем, как они используются в механизме внимания. Это помогает контролировать норму этих векторов и стабилизировать вычисления внутри механизма внимания.

## Использование в моделях

### OLMo 2

- Одна из первых моделей, широко внедривших QK-нормализацию
- Используется в сочетании с пост-нормализацией для дополнительной стабильности обучения
- Демонстрирует значительное улучшение стабильности обучения

### Gemma 3

- Также использует QK-нормализацию
- В сочетании с GQA и скользящим окном внимания
- Дополнительный уровень нормализации в сравнении с другими архитектурами

## Сравнение с другими подходами

- **Стандартное внимание**: Без дополнительной нормализации запросов и ключей
- **Только пред/пост-нормализация**: Только нормализация до/после модулей внимания и FFN
- **QK-нормализация**: Дополнительная нормализация внутри модуля внимания

## Преимущества

- **Стабилизация обучения**: Снижение колебаний в процессе обучения
- **Улучшенная сходимость**: Более стабильные кривые потерь
- **Контроль нормы векторов**: Помогает избежать проблем с градиентами
- **Комплементарность с другими нормами**: Работает вместе с пред/пост-нормализацией

## Архитектурный контекст

QK-нормализация часто используется в сочетании с другими техниками стабилизации обучения, такими как пост-нормализация (в OLMo 2) или уникальное размещение нормализации (в Gemma 3). Это показывает тенденцию к многоуровневому подходу к стабилизации обучения трансформеров.

## Связи с другими темами

- **OLMo 2**: Ключевая особенность архитектуры, в сочетании с пост-нормализацией
- **Gemma 3**: Используется в дополнение к уникальной схеме нормализации
- **RMSNorm**: Тип нормализации, используемой в QK-нормализации
- **Стабилизация обучения**: Часть более широкой стратегии стабилизации обучения LLM

## Контекст в области LLM

QK-нормализация представляет собой относительно новую технику, которая стала важной частью архитектуры некоторых современных моделей. Она демонстрирует, что даже внутри модуля внимания можно внедрять дополнительные механизмы стабилизации для улучшения обучения моделей.

## Связи с другими темами

- [[olmo_2.md|OLMo 2]] - Первая модель, широко использующая QK-нормализацию
- [[gemma_3.md|Gemma 3]] - Также использует QK-нормализацию
- [[normalization_placement.md|Размещение нормализации]] - Сравнение различных подходов к нормализации
- [[rms_norm.md|RMSNorm]] - Основной тип нормализации, используемый в QK-нормализации