# Qwen3

## Описание

Qwen3 - серия высококачественных моделей от команды Qwen с открытыми весами. Серия включает как плотные модели, так и модели с архитектурой смеси экспертов (MoE), обеспечивая гибкость для различных случаев использования.

## Ключевые архитектурные особенности

### Двухуровневый подход: плотные и MoE модели

Qwen3 предлагает оба типа архитектур:
- **Плотные модели**: 0.6B, 1.7B, 4B, 8B, 14B, 32B параметров
- **MoE модели**: 30B-A3B (30B общих параметров, 3B активных), 235B-A22B (235B общих параметров, 22B активных)

Это дает пользователям гибкость:
- Плотные модели: простота дообучения, развертывания и оптимизации
- MoE модели: эффективность инференса с большей емкостью модели

### Архитектурные различия плотных моделей

На примере Qwen3 0.6B:
- Более **глубокая** архитектура: больше блоков трансформера, чем Llama 3
- Более **узкая**: меньше голов внимания и меньшая размерность эмбеддингов, чем Llama 3
- Это приводит к более медленному времени выполнения (меньшей скорости генерации токенов), но меньшему объему памяти

### Отказ от общего эксперта в MoE

В отличие от DeepSeek-V3 и более ранних Qwen моделей (например, Qwen2.5-MoE), Qwen3 MoE модели НЕ используют общего эксперта. Это может быть связано с увеличением количества экспертов (с 2 в Qwen2.5-MoE до 8 в Qwen3), что делает общего эксперта менее необходимым для стабильности.

## Сравнение с другими моделями

### Сравнение с DeepSeek-V3

Qwen3 235B-A22B и DeepSeek-V3 удивительно похожи архитектурно, но с важными отличиями:
- Qwen3 не использует общего эксперта
- DeepSeek-V3 имеет больше активных параметров (37B) по сравнению с Qwen3 (22B)

### Сравнение с Llama 3

- Qwen3 0.6B более глубокая, но уже, чем Llama 3 1B
- Это делает Qwen3 более экономичной по памяти, но медленнее в генерации токенов

## Архитектурные решения

- **Гибкость архитектуры**: Плотные и MoE варианты для разных сценариев
- **Оптимизация для обучения**: Меньшие плотные модели для образовательных целей
- **Экономия ресурсов**: MoE для масштабирования инференса
- **Отказ от общего эксперта**: В отличие от некоторых других MoE архитектур

## Контекст в области LLM

Qwen3 представляет практический подход к архитектуре LLM, предлагая разные варианты для разных потребностей - от маленьких моделей для обучения и локального запуска до крупных MoE моделей для масштабного инференса. Это демонстрирует важность гибкости в архитектуре для широкого спектра применений.

## Связи с другими темами

- [[deepseek_v3.md|DeepSeek V3]] - Сравнение с альтернативной MoE архитектурой
- [[llama_3.md|Llama 3]] - Сравнение с альтернативной плотной архитектурой
- [[../techniques/mixture_of_experts.md|MoE]] - Подробное объяснение смеси экспертов
- [[gpt_oss.md|GPT-OSS]] - Сравнение обеих с MoE архитектурами
- [[../models/multimodal/qwen3-omni.md|Qwen3-Omni]] - Мультимодальная версия Qwen3 с возможностями обработки текста, изображений, аудио и видео
- [[../models/qwen/qwen_deepresearch_2511.md|Qwen DeepResearch 2511]] - Специализированный инструмент для глубокого исследования на основе Qwen3