# Архитектура RWKV (Receptance Weighted Key Value)

## Общее описание

RWKV (Receptance Weighted Key Value) - это архитектура нейронной сети, разработанная как альтернатива традиционным трансформерам, сочетающая лучшие качества рекуррентных нейронных сетей (RNN) и трансформеров. RWKV использует линейные рекуррентные вычисления, которые позволяют достичь линейной сложности O(N) по длине последовательности, при этом сохраняя способность модели к параллельному обучению. Архитектура была разработана для решения проблем квадратичной сложности трансформеров при масштабировании на длинные последовательности.

## Технические детали

### Основной принцип

RWKV разделяет архитектуру на два основных компонента:

1. **Receptance**: Определяет, насколько сильно каждый токен должен учитываться в вычислениях
2. **Weighted Key-Value**: Определяет, какая информация должна быть передана дальше

### Формула RWKV

Вместо стандартного механизма внимания в трансформерах, RWKV использует следующую рекуррентную формулу:

```
wkv_t = k_t * v_t + decay * h_{t-1}  # weighted key-value
r = sigmoid(W_r * x_t)                # receptance
output = r * wkv_t                    # receptance-weighted output
```

Где:
- `k_t`, `v_t` - ключ и значение для позиции t
- `h_t` - скрытое состояние (накопленная информация)
- `decay` - параметр затухания, контролирующий, как быстро забывается старая информация
- `r` - receptance, определяющая, насколько сильно текущий токен должен учитываться

### Обучение и инференс

- **Обучение**: RWKV может быть обучен параллельно, как трансформер, благодаря специальным оптимизациям
- **Инференс**: Использует рекуррентные вычисления с линейной сложностью O(N)
- **Параллельная структура обучения**: Использует специальные блоки для параллельного вычисления во время обучения
- **Рекуррентная структура инференса**: Использует эффективные рекуррентные вычисления во время генерации

## Архитектурные особенности

### Токенизация и эмбеддинги

- RWKV использует стандартные токенизаторы (например, BPE)
- Эмбеддинги инициализируются подобно трансформерам
- Возможность эффективной адаптации к новым языкам

### Нормализация

- Использует RMSNorm (Root Mean Square Normalization) как в современных трансформерах
- Позволяет стабилизировать обучение и улучшить сходимость

### Активации

- ReLU в FFN слоях как в трансформерах
- Сигмоиды в receptance вычислениях
- Гейты для управления информационным потоком

### Рекуррентные блоки

- Каждый блок RWKV включает рекуррентный слой и FFN
- Рекуррентный слой реализует формулу RWKV
- FFN слои аналогичны тем, что используются в трансформерах

## Преимущества RWKV

### Вычислительная эффективность

- **Линейная сложность инференса**: O(N) по длине последовательности
- **Эффективное использование GPU**: Можно эффективно использовать как во время обучения, так и во время инференса
- **Меньшее потребление памяти во время генерации**: Не требует KV кэширования как трансформеры
- **Быстрая генерация**: Особенно эффективно для очень длинных последовательностей

### Масштабируемость

- **Обработка длинных контекстов**: Может эффективно обрабатывать гораздо более длинные последовательности по сравнению с трансформерами
- **Раздельное масштабирование**: Можно масштабировать глубину и ширину независимо
- **Эффективное масштабирование**: Позволяет создавать крупные модели с меньшими вычислительными требованиями

### Архитектурные свойства

- **Нулевая шумовость**: Может быть обучен как трансформер и использовать рекуррентный инференс
- **Гибкость**: Подходит как для исследования, так и для промышленного использования
- **Модульность**: Архитектура может быть легко адаптирована для различных задач

## Сравнение с другими архитектурами

| Архитектура | Сложность | Параллельное обучение | Качество | Особенности |
|-------------|-----------|------------------------|----------|-------------|
| Трансформер | O(N²) | Да | Высокое | Стандартная архитектура |
| RWKV | O(N) | Да | Высокое | Рекуррентная/Трансформерная гибридная |
| Mamba | O(N) | Да (с оптимизациями) | Высокое | State Space Model |
| RetNet | O(N) | Да (с оптимизациями) | Высокое | Линейные рекуррентные вычисления |
| Линейное внимание | O(N) | Да | Среднее-Высокое | Приближенные вычисления |

### RWKV vs Mamba

- **RWKV** использует гибридный подход с рекуррентными и трансформерными элементами
- **Mamba** основана на State Space Models с селективной способностью
- RWKV легче интегрировать в существующие трансформерные рабочие процессы
- Mamba предлагает более глубокую переработку архитектуры

### RWKV vs RetNet

- Обе архитектуры используют линейные рекуррентные вычисления
- RWKV фокусируется на receptance-weighted механизме
- RetNet использует retention mechanism с chunk-wise recurrence

## Применения RWKV

### Языковое моделирование

- Эффективные модели для генерации текста
- Обработка длинных документов и контекстов
- Модели, конкурентоспособные с трансформерами по качеству

### Многоязычные модели

- RWKV эффективно работает с несколькими языками
- Меньшие требования к вычислениям позволяют исследовать многоязычные модели
- Подходит для низкоресурсных языков

### Специализированные задачи

- Математические и логические задачи
- Задачи с длинными цепочками рассуждений
- Задачи, требующие точного понимания контекста

## Варианты и реализации

### RWKV-4

- Первоначальная версия архитектуры
- Демонстрирует основные принципы RWKV
- Показывает конкурентоспособность с трансформерами

### RWKV-5

- Улучшенная версия с лучшей стабильностью обучения
- Оптимизирована для масштабирования
- Лучшие результаты на различных задачах

### Open-source экосистема

- RWKV-LM: Основной репозиторий для RWKV моделей
- Пакеты для обучения и инференса
- Сообщество, разрабатывающее специализированные модели

## Значение для обзора "Speed Always Wins"

RWKV представляет собой важный пример подхода к линейному моделированию последовательностей, описанного в обзоре "Speed Always Wins". Архитектура демонстрирует, как можно объединить преимущества рекуррентных моделей и трансформеров для достижения линейной сложности без существенного ухудшения качества. В обзоре RWKV упоминается как один из примеров линейных RNN, которые являются частью более широкого класса эффективных архитектур для LLM.

## Связи с другими темами

- [[linear_sequence_modeling.md]] - Линейное моделирование последовательностей, к которому относится RWKV
- [[mamba_architecture.md]] - Другая архитектура с линейной сложностью
- [[retnet_architecture.md]] - Сравнение с другими линейными рекуррентными подходами
- [[state_space_models.md]] - Сравнение с моделями в пространстве состояний
- [[speed_always_wins_survey.md]] - Обзор "Speed Always Wins", где RWKV является примером линейных RNN
- [[cross_modality_efficient_architectures.md]] - Применение RWKV к различным модальностям
- [[llm_architectures_comparison.md]] - Общее сравнение различных архитектур LLM
- [[hybrid_architectures.md]] - RWKV как гибридный подход между RNN и трансформерами