# SmolLM3

## Описание

SmolLM3 - компактная модель с 3 миллиардами параметров, предлагающая действительно хорошую производительность моделирования при относительно небольшом размере. Модель находится между Qwen3 на 1.7B и 4B параметров, обеспечивая хорошее соотношение производительности и эффективности.

## Ключевые архитектурные особенности

### Отказ от позиционных эмбеддингов (NoPE)

Основной архитектурной особенностью SmolLM3 является использование NoPE (No Positional Embeddings), что означает отказ от явного внедрения позиционной информации (например, через абсолютные позиционные эмбеддинги или RoPE).

Вместо этого модель полагается на причинную маску внимания (causal attention mask), которая предотвращает обращение каждого токена к будущим токенам. Это сохраняет авторегрессивный порядок через структуру модели, а не через явные позиционные сигналы.

### Селективная реализация NoPE

В отличие от полной реализации NoPE, SmolLM3 применяет этот подход только в каждом 4-м слое, что позволяет модели сохранить некоторые преимущества позиционной информации при тестировании концепции NoPE.

## Архитектурные решения

- **NoPE в селективных слоях**: Реализация без позиционных эмбеддингов только в каждом 4-м слое
- **Компактный размер**: 3 миллиарда параметров для хорошей производительности
- **Фокус на эффективность**: Баланс между размером и производительностью
- **Прозрачность обучения**: Подробные детали обучения, как в OLMo

## Преимущества NoPE

- **Лучшее обобщение по длине**: Производительность ухудшается меньше с увеличением длины последовательности
- **Уменьшенная сложность**: Отказ от позиционного кодирования упрощает архитектуру
- **Потенциальная эффективность**: Меньше параметров, связанных с позиционной информацией

## Сравнение с другими моделями

- В отличие от большинства LLM, не использует RoPE или абсолютные позиционные эмбеддинги
- Более компактная, чем Qwen3 4B, но более производительная
- Подобно OLMo, делится подробностями обучения

## Контекст в области LLM

SmolLM3 представляет интерес как одна из первых моделей, активно тестирующих концепцию NoPE в современном контексте. Это экспериментальный подход, который показывает, что явные позиционные эмбеддинги могут быть не так необходимы, как считалось ранее, особенно когда модель может использовать причинную структуру внимания для понимания порядка.

## Связи с другими темами

- [[../techniques/no_positional_embeddings.md|NoPE]] - Подробное объяснение подхода без позиционных эмбеддингов
- [[olmo_2.md|OLMo 2]] - Также акцент на прозрачность обучения
- [[qwen3.md|Qwen3]] - Сравнение с другой моделью среднего размера