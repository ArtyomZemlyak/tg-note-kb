# Внимание со скользящим окном (Sliding Window Attention)

## Описание

Внимание со скользящим окном (SWA) - архитектурная техника в моделях трансформеров, при которой механизм внимания ограничивается локальным окном вокруг текущей позиции запроса. Это противопоставляется глобальному вниманию, где каждый токен может обращаться ко всем другим токенам.

## Технические детали

### Принцип работы

- **Локальное внимание**: Ограничение размера контекста вокруг текущей позиции запроса
- **Скользящее окно**: Окно движется вместе с позицией запроса
- **Снижение памяти**: Значительное уменьшение требований к памяти в KV-кеше
- **Минимальное влияние на производительность**: Исследования показывают незначительное или нулевое влияние на перплексию

### Варианты соотношения

- **Gemma 3**: Соотношение 5:1 (1 слой полного внимания на 5 слоев скользящего окна)
- **GPT-OSS**: Скользящее окно в каждом втором слое (1:1)
- **Gemma 2**: Соотношение 1:1 (слой скользящего окна на слой глобального внимания)

## Использование в моделях

### Gemma 3

- Размер скользящего окна: 1024 токена (уменьшен с 4096 в Gemma 2)
- Смещение фокуса в сторону более эффективных локализованных вычислений
- Объединено с GQA

### GPT-OSS

- Скользящее окно в каждом втором слое
- В отличие от Gemma 3, не использует соотношение 5:1

### Сравнение с альтернативами

- В отличие от Mistral Small 3.1, которая использует стандартное GQA без скользящего окна
- В отличие от DeepSeek V3, которая использует MLA вместо скользящего окна

## Преимущества

- **Экономия памяти**: Значительное снижение использования KV-кеша
- **Минимальная потеря производительности**: Сравнительно небольшое влияние на моделирование
- **Эффективность инференса**: Уменьшение пропускной способности памяти
- **Поддержка длинных последовательностей**: Позволяет работать с более длинными контекстами

## Технический контекст

SWA был первоначально представлен в статье LongFormer (2020) и уже использовался в Gemma 2 до Gemma 3. Это пример того, как старые идеи реимплементируются и адаптируются в современных архитектурах.

## Связи с другими подходами

- **Альтернатива MLA**: Вместо сжатия KV-тензоров ограничивает их использование
- **В сочетании с GQA**: Как в Gemma 3, для дополнительной эффективности
- **Компромисс производительности**: Меньшая глобальная связность за счет памяти

## Контекст в области LLM

SWA представляет собой важный компромисс в архитектуре LLM: сброс глобальной связанности ради эффективности памяти при минимальной потере качества моделирования. Это ключевая техника для масштабирования моделей с ограниченными ресурсами памяти.

## Связи с другими темами

- [[grouped_query_attention.md|GQA]] - Часто используется в сочетании с GQA (например, в Gemma 3)
- [[mistral_small_31.md|Mistral Small 3.1]] - Альтернативный подход без скользящего окна
- [[gemma_3.md|Gemma 3]] - Основная реализация SWA
- [[gpt_oss.md|GPT-OSS]] - Альтернативная реализация SWA