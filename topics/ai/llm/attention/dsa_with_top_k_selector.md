# DeepSeek Sparse Attention (DSA) с top-K селектором

## Общее описание

![Архитектура внимания DeepSeek-V3.2 с DSA](../../../../media/img_1764931113_aqadcxbrg1gfkel_figure_2_attention_architecture_of.jpg)

*На рисунке 2 из технического отчета показана архитектура внимания DeepSeek-V3.2, где DSA инстанциирована на основе MLA. Зеленая часть иллюстрирует, как DSA выбирает top-k элементы ключ-значение в соответствии с индексом.*

DeepSeek Sparse Attention (DSA) с top-K селектором - это усовершенствованная версия разреженного механизма внимания, используемого в моделях DeepSeek-V3.2. Основное улучшение заключается в использовании механизма тонкого выбора (fine-grained token selection) с top-K селектором, который позволяет динамически определять, на какие токены следует обращать внимание в каждом конкретном окне, что приводит к снижению вычислительной сложности до O(n*<w>) вместо O(n²).

## Технические детали

### Прототип DSA

Прототип DSA состоит из двух основных компонентов:

1. **Lightning Indexer** - легковесная сеть для вычисления индексных оценок
2. **Fine-grained token selection mechanism** - механизм тонкого выбора токенов

### Lightning Indexer

Lightning Indexer вычисляет индексную оценку I_t,s между токеном запроса h_t ∈ R^d и предыдущим токеном h_s ∈ R^d, определяя, какие токены должны быть выбраны токеном запроса:

I_t,s = Σ_j ReLU(q_I_t,j * k_I_s,j + w_I_t,j)

где:
- H_I - количество индексных голов
- q_I_t,j ∈ R^d_I и w_I_t,j ∈ R - производные от токена запроса h_t
- k_I_s ∈ R^d_I - производная от предыдущего токена h_s
- ReLU используется для соображений пропускной способности

Благодаря небольшому количеству голов и возможности реализации в FP8, индексер обладает высокой вычислительной эффективностью.

### Fine-grained token selection

Имея индексные оценки {I_t,s} для каждого токена запроса h_t, механизм тонкого выбора извлекает только те элементы ключ-значение {c_s}, которые соответствуют top-k индексным оценкам. Затем выход внимания u_t вычисляется путем применения механизма внимания между токеном запроса h_t и разреженно выбранными элементами ключ-значение {c_s}.

### Снижение сложности

- Обычное внимание: O(n²) по длине последовательности
- DSA с top-K селектором: O(n*w), где w << n - средний размер окна после тонкого отбора
- W становится в среднем меньше от слайда к слайлу за счет top-K селектора

## Усиление по сравнению с традиционным DSA

### Селектор на роутинг

На отличие от традиционного DSA, DSA с top-K селектором включает специальный механизм "выбора", который определяет, на какие токены должна обращать внимание global часть. Как показано в техническом отчете, на рисунке с зеленым блоком показан этот селектор, который размещается на KV для роутинга.

### Global и sliding window части

- Global часть: внимание от Q0 до Qn по отношению к KV0
- Sliding window: скользящее окно для локального контекста
- Top-K селектор позволяет эффективно выбирать, какие токены из глобального контекста наиболее важны для текущего запроса

## Применение в DeepSeek-V3.2

### Интеграция с MLA

В DeepSeek-V3.2 DSA реализуется на основе Multi Latent Head Attention (MLA) в режиме MQA (Multi-Query Attention), где каждый латентный вектор (элемент ключ-значение MLA) используется совместно всеми головами запроса токена запроса.

### Этапы обучения

1. **Dense Warm-up Stage**: Использование плотного внимания с замороженными параметрами, кроме lightning indexer
2. **Sparse Training Stage**: Введение fine-grained token selection и оптимизация всех параметров модели

### Эффективность

- Снижение основной сложности внимания с O(L²) до O(L*k), где k (≪ L) - количество выбранных токенов
- Lightning indexer сохраняет O(L²) сложность, но требует гораздо меньше вычислений по сравнению с MLA в DeepSeek-V3.1-Terminus
- Значительное ускорение end-to-end в сценариях длинного контекста

## Сравнение с другими подходами

### Традиционное глобальное внимание
- Сложность: O(n²)
- Высокое качество, но дорогостоящее для длинных последовательностей
- Неограниченный контекст

### Sliding Window Attention (SWA)
- Сложность: O(n*w), где w - фиксированный размер окна
- Ограниченный контекст
- Более эффективный, но с ограничениями на долгосрочную зависимость

### DSA с top-K селектором
- Сложность: O(n*<w>), где <w> - средний размер динамического окна
- Эффективность за счет разреженности
- Сохранение способности к глобальному вниманию за счет top-K выбора

## Архитектурные особенности

### Визуализация на рисунке 2

Рисунок 2 из технического отчета показывает архитектуру внимания DeepSeek-V3.2, где DSA реализуется на основе MLA. Зеленая часть иллюстрирует, как DSA выбирает top-k элементы ключ-значение в соответствии с индексом, используя lightning indexer.

![Архитектура внимания DeepSeek-V3.2 с DSA](../../../../media/img_1764931113_aqadcxbrg1gfkel_figure_2_attention_architecture_of.jpg)

![Регулярное каузальное самовнимание с маской](../../../../media/img_1764931113_aqadhxbrg1gfkel9_regular_causal_self_attention_mask_deeps.jpg)

![Расположение синков внимания в конце](../../../../media/img_1764931113_aqadxjrgwtqkul_sink_location_at_the_end.jpg)

### Компоненты архитектуры

1. **Lightning Indexer** - вычисляет индексные оценки для отбора
2. **Top-K Selector** - выбирает самые релевантные KV-пары
3. **Sparse Attention** - вычисляет внимание только на выбранных парах
4. **MLA Integration** - интеграция с Multi Latent Head Attention

## Преимущества

### Вычислительная эффективность
- Значительное снижение вычислительной нагрузки при длинных входных последовательностях
- Уменьшение использования памяти для KV-кеширования
- Более эффективное использование вычислительных ресурсов

### Сохранение качества
- Поддержание сопоставимого уровня точности и производительности
- Лучшая масштабируемость для длинных контекстов
- Сохранение способности модели к пониманию длинных последовательностей

### Адаптивность
- Динамический размер окна в зависимости от контента
- Способность адаптироваться к различным типам задач
- Эффективный баланс между эффективностью и качеством

## Вызовы и ограничения

### Реализация
- Требует сложной координации между компонентами
- Необходимость оптимизации lightning indexer
- Потенциальные трудности с синхронизацией компонентов

### Гиперпараметры
- Необходимость настройки количества выбранных токенов (k)
- Настройка параметров lightning indexer
- Баланс между эффективностью и качеством

## Будущие направления

### Оптимизация
- Улучшение эффективности lightning indexer
- Оптимизация процесса выбора токенов
- Улучшение интеграции с другими компонентами архитектуры

### Теоретическое понимание
- Анализ влияния top-K выбора на качество модели
- Исследование оптимальных стратегий выбора
- Понимание того, как различные типы задач влияют на эффективность селектора

## Связи с другими темами

- [[deepseek_sparse_attention.md]] - основная концепция DeepSeek Sparse Attention
- [[multi_head_latent_attention.md]] - интеграция с MLA архитектурой
- [[specialized_attention_mechanisms.md]] - обзор специализированных механизмов внимания
- [[deepseek_v3_2_speciale.md]] - применение в флагманской модели
- [[mixture_of_sparse_attention.md]] - другой подход к разреженному вниманию
- [[star_attention_mechanism.md]] - альтернативный подход к эффективному вниманию
- [[sliding_window_attention.md]] - базовый подход к эффективному длинному контексту

## Источники

- Технический отчет DeepSeek-V3.2: "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models"
- Оригинальные исследования по DSA и lightning indexer
- Рисунки и архитектурные диаграммы из технического отчета