# Авторегрессивные модели (Autoregressive Models)

## Общее описание

Авторегрессивные модели - это класс генеративных моделей, в которых следующий элемент последовательности предсказывается на основе всех предыдущих элементов. В контексте генерации текста каждому токену предсказывается условная вероятность при заданных всех предыдущих токенах в последовательности.

## Принцип работы

Авторегрессивные модели генерируют последовательности по одному элементу за раз, каждый новый элемент зависит от всех предыдущих. В математическом виде: P(x₁, x₂, ..., xₙ) = ∏ᵢ P(xᵢ|x₁, x₂, ..., xᵢ₋₁)

Для текста это означает, что на каждом шаге модель предсказывает следующий токен на основе всего предыдущего контекста.

## Архитектура и механизм

### Каузальная маска (Causal Masking)
- Ключевая особенность авторегрессивных моделей - каузальная маска внимания
- Каждый токен может обращаться только к предыдущим токенам в последовательности
- Предотвращает "подсказки" из будущего во время генерации
- Обеспечивает последовательный процесс генерации

### Стандартные архитектуры
- **Decoder-only**: GPT, GPT-2, GPT-3, LLaMA, OPT, BLOOM
- Используют каузальные маски в механизме внимания
- Обычно состоят из слоев самовнимания (self-attention) и MLP

## Преимущества

- **Высокое качество генерации**: Детерминированный процесс, хорошая согласованность
- **Длинные зависимости**: Механизм внимания позволяет учитывать длинные контексты
- **Контролируемость**: Возможность управления генерацией через температуру, top-k, top-p
- **Простота реализации**: Прямое обучение на максимизации правдоподобия

## Ограничения

- **Последовательная генерация**: Каждый токен генерируется последовательно, что замедляет процесс
- **Ограничения по скорости**: Невозможно параллелизировать внутри последовательности
- **Зависимость от предыдущих токенов**: Ошибки могут накапливаться в длинных последовательностях
- **Высокие требования к памяти**: KV-cache нуждается в хранении для каждого токена

## Применение

- Генерация текста (статьи, рассказы, код)
- Комплетирование кода
- Чат-боты и ассистенты
- Моделирование временных рядов
- Аудио- и музейная генерация

## Варианты и улучшения

### Спекулятивное декодирование (Speculative Decoding)
- Использует помощника для предсказания нескольких токенов, которые проверяются основной моделью
- Позволяет частично параллелизовать генерацию

### Управление генерацией
- Варианты семплирования: greedy, beam search, nucleus sampling
- Управление через температуру, top-k, top-p параметры
- Руководимая генерация (guided generation)

## Сравнение с другими подходами

| Характеристика | Авторегрессивные | Диффузионные | Невавторегрессивные |
|----------------|------------------|--------------|---------------------|
| Скорость генерации | Низкая | Средняя | Высокая |
| Качество | Высокое | Высокое | Среднее-Высокое |
| Параллелизм | Нет | Да (итерационно) | Да |
| Сложность обучения | Средняя | Высокая | Высокая |

## Экспериментальные результаты

- GPT-3 с 175B параметрами показывает высокое качество генерации и few-shot обучение
- LLaMA-2 достигает сопоставимого качества с меньшим количеством параметров благодаря улучшенной архитектуре
- Современные модели масштабируются эффективно, но требуют значительных вычислительных ресурсов

## Последовательности и развития

- Развитие масштабных языковых моделей (GPT, LLaMA, PaLM)
- Оптимизация инференса (KV-caching, speculative decoding)
- Смешанные архитектуры (Planned Diffusion, Mixture of Experts)

## Связи с другими темами

- [[text_generation_methods.md]] - Методы генерации текста в LLM, включая авторегрессивные
- [[../architectures/diffusion/planned_diffusion.md]] - Гибридный метод, сочетающий авторегрессию и диффузию
- [[hybrid_generation_architectures.md]] - Гибридные архитектуры, объединяющие авторегрессивные и другие подходы
- [[../inference/optimization/parallel_decoding.md]] - Оптимизация генерации, включая параллельные методы
- [[../models/gpt_family.md]] - Примеры конкретных авторегрессивных архитектур
- [[../architectures/transformer_architecture.md]] - Основы трансформерной архитектуры, на которой основаны авторегрессивные модели
- [[research_advances/calm_continuous_autoregressive_language_models.md]] - Непрерывные авторегрессионные языковые модели (CALM), новая парадигма, предсказывающая непрерывные векторы вместо дискретных токенов