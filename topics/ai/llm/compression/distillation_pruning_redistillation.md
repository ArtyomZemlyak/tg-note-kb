# Трехэтапная компрессия LLM: Дистилляция, Прореживание и Редистилляция

## Краткое описание

Методика трехэтапной компрессии больших языковых моделей, включающая дистилляцию, прореживание (pruning) и редистилляцию. Этот подход позволяет создавать компактные Small Language Models (SLM), которые сохраняют качество оригинальных LLM при значительно меньших вычислительных затратах и задержках инференса. Метод активно применяется в промышленных рекомендательных системах, особенно после презентации LinkedIn на EMNLP 2025.

## Основная информация

### Общая концепция

Трехэтапная компрессия представляет собой последовательный процесс, в котором:
1. Сначала знания передаются от большой модели к компактной через дистилляцию
2. Затем структурно уменьшается количество параметров с помощью прореживания
3. Наконец, модель снова дообучается с помощью редистилляции для восстановления потенциальной выразительности

Этот подход позволяет достичь компромисса между производительностью модели и её вычислительной эффективностью.

### 1. Дистилляция (Knowledge Distillation)

#### Определение
Дистилляция знаний — метод машинного обучения, при котором информация, содержащаяся в большой, часто предварительно обученной модели (учитель), передается в меньшую модель (студент).

#### Принцип работы
- **Учитель (Teacher Model)**: Большая, часто предварительно обученная модель, обладающая высокой производительностью
- **Студент (Student Model)**: Меньшая модель, которая обучается копировать поведение учителя
- **Температура (Temperature)**: Параметр, который регулирует "мягкость" вероятностного распределения, полученного через функцию softmax

#### Техники дистилляции
- **Hard-label дистилляция**: Обучение только на предсказанных токенах
- **Soft-label дистилляция**: Обучение на полных распределениях (наиболее информативно)
- **On-policy дистилляция**: Обучение на основе роллаутов с оценкой учителя
- **Off-policy дистилляция**: Обучение на предварительно подготовленных данных

#### Решения проблем дистилляции
- **Exposure Bias**: Проблема, когда модель-студент во время обучения видит разное распределение, чем на инференсе; решается через on-policy дистилляцию
- **Мисалигнмент токенизаторов**: Классические методы требуют одинаковых словарей; решается через кросс-токенизаторные методы (ULD, GOLD)
- **Проблема сильного учителя**: Очень сильный учитель может быть неэффективен для слабого студента

### 2. Прореживание (Pruning)

#### Определение
Прореживание — метод оптимизации нейронных сетей, при котором удаляются не отдельные веса, а целые структурные компоненты, такие как нейроны, каналы, весовые матрицы или их части.

#### Типы прунинга
- **Структурированный прунинг**: Удаляет целые структуры (головы внимания, нейроны, слои)
- **Унитарный (unstructured) прунинг**: Удаляет веса индивидуально (менее эффективен на практике)

#### Техники прунинга
- **Понижение ранга (Low-Rank Adaptation)**: Метод, при котором весовые матрицы разлагаются с помощью сингулярного разложения (SVD), и затем удаляются компоненты, соответствующие наименьшим сингулярным значениям
- **Удаление голов внимания**: Удаление наименее важных голов внимания в трансформерных моделях
- **Блочное прунингование**: Метод, при котором веса группируются в блоки, и целые блоки удаляются
- **Compress to Impress**: Метод, использующий градиенты сингулярных значений для определения компонентов, которые вредны для новой задачи

#### Метрики важности
- **Градиент сингулярных значений**: ∂L/∂σᵢ = uᵢᵀGvᵢ, где G = ∂L/∂W - матричный градиент; измеряет, насколько градиент лосса сонаправлен с компонентом ранга один
- **Отрицательные градиенты**: Указывают, что лосс-функция "хочет" уменьшить компонент, позволяя точно определить, какие части структуры модели вредны

### 3. Редистилляция (Redistillation)

#### Определение
Редистилляция — финальный этап процесса сжатия, при котором сжатая модель снова дообучается. Цель — частично увеличить число параметров и улучшить обобщающую способность, вернув части выражаемости без возврата к исходным затратам на вычисления.

#### Цели редистилляции
- Восстановление выразительности, потерянной при прореживании
- Улучшение обобщающей способности модели
- Балансировка между компактностью и качеством
- Финальная настройка сжатой модели

#### Принцип работы
- Модель, полученная после прореживания, используется как отправная точка
- Применяются техники дистилляции для дообучения модели
- Может включать дополнительные данные или задачи для восстановления потерянной информации
- Позволяет вернуть часть качества без возврата к исходному размеру модели

## Основные проблемы, которые решает метод

### Высокая стоимость инференса
- Большие LLM требуют значительных вычислительных ресурсов для инференса
- Трехэтапная компрессия позволяет создать эффективные SLM с гораздо меньшими затратами

### Высокая задержка инференса
- Большие модели страдают от высокой латентности при генерации результатов
- Компактные модели обеспечивают более быстрые ответы, особенно критично для рекомендательных систем

### Сложность деплоя в продакшене
- Большие модели сложно развертывать на production-инфраструктуре
- Компактные модели проще масштабировать и обслуживать

### Баланс между размером и качеством
- Традиционные методы сжатия часто жертвуют качеством ради размера
- Трехэтапный процесс позволяет сохранить баланс, восстанавливая качество на этапе редистилляции

## Решения и методы

### Комбинированный подход
Трехэтапная компрессия эффективна благодаря сочетанию:
- Агрессивного сжатия на первых двух этапах (дистилляция + прореживание)
- Восстановительного этапа (редистилляция), который компенсирует возможные потери качества

### Метрики для оптимизации
- Использование градиентов лосса для определения важных компонентов моделей
- Применение различных функций потерь на разных этапах
- Контроль равновесия между компактностью и производительностью

### Практические рекомендации
- Агрессивная дистилляция и прореживание, сбалансированные редистилляцией
- Использование малого количества данных для эффективного сжатия (например, 100 примеров для Compress to Impress)
- Фокусировка на стилистическом выравнивании, а не только на статистическом обучении

## Новые концепции и термины

- **Three-stage compression**: Методика сжатия LLM, включающая дистилляцию → прореживание → редистилляцию
- **Redistillation**: Финальный этап процесса сжатия, при котором сжатая модель дообучается для восстановления выразительности
- **Gradient of singular values**: Метрика ∂L/∂σᵢ = uᵢᵀGvᵢ, используемая для определения компонентов весовой матрицы, вредных для новой задачи
- **Stylistic alignment**: Процесс подстройки модели под стиль промптинга, форматирование и структуру ответов целевого домена
- **Matryoshka learning in compression**: Использование иерархии векторов разных размерностей в сжатых моделях для гибкости использования

## Примеры применения

### Рекомендательные системы
- Применение сжатых LLM в системах рекомендаций, как в случае LinkedIn
- Снижение задержек и стоимости инференса при сохранении качества рекомендаций
- Возможность обработки высоконагруженного трафика

### Мобильные и edge-устройства
- Развертывание сложных моделей на устройствах с ограниченными ресурсами
- Ускорение инференса на мобильных платформах
- Снижение энергопотребления

### Массовое развертывание
- Возможность развертывания множества специализированных моделей
- Снижение операционных затрат на облачные вычисления
- Лучшая масштабируемость систем

## Связи с другими темами

- [[../../knowledge_distillation.md]] - Подробное описание дистилляции знаний
- [[../pruning/structured_pruning.md]] - Подробное описание методов прореживания
- [[compress_to_impress_single_gradient_llm_adaptation.md]] - Метод Compress to Impress, связанный с градиентами сингулярных значений
- [[../../optimization/model_quantization_techniques.md]] - Другой подход к оптимизации LLM
- [[../../recsys/llm_based/linkedin_compressed_llm_approach.md]] - Практическое применение метода в рекомендательных системах LinkedIn
- [[../../recsys/llm_based/llm_candidate_generation_approaches.md]] - Описание других подходов к использованию LLM в рекомендациях

## Источники

1. [LinkedIn's Approach to Compressing and Deploying Efficient LLMs for Recommendation Systems at EMNLP 2025](https://arxiv.org/html/2502.14305v2) - Основная статья, описывающая применение трехэтапной компрессии в рекомендательных системах LinkedIn
2. [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800) - Статья, описывающая метод Compress to Impress, в которой используется градиент сингулярных значений, важный компонент современных методов сжатия
3. [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) - Оригинальная статья Хинтона о дистилляции знаний
4. [AI VK Summary of LinkedIn's EMNLP 2025 Paper](https://t.me/ai_vkm1/771) - Обзор статьи, подготовленный командой AI VK, содержащий краткое изложение ключевых аспектов подхода LinkedIn