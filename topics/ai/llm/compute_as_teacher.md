# Compute as Teacher: Превращение вычислений при инференсе в эталонную супервизию без примеров

## Краткое описание

Compute as Teacher (CaT) - это метод обучения больших языковых моделей (LLM), который превращает собственные поисковые результаты модели в высококачественную супервизию без необходимости в эталонных данных. В отличие от методов, выбирающих «лучший» ответ из группы параллельных ответов модели, CaT использует замороженную «якорную» политику для синтеза единого, улучшенного эталона путем устранения противоречий и объединения частичных решений.

## Авторы

- Dulhan Jayalath
- Shashwat Goel
- Thomas Foster
- Parag Jain
- Suchin Gururangan
- Cheng Zhang
- Anirudh Goyal
- Alan Schelten

## Основные концепции

### Синтез вместо отбора

CaT использует парадигму «синтеза вместо отбора» - вместо простого выбора лучшего из доступных ответов, метод синтезирует новый, лучший ответ из частей нескольких несовершенных ответов. Это позволяет достигать корректных результатов даже тогда, когда все индивидуальные ответы модели (роллауты) ошибочны, что недостижимо для методов на основе отбора.

### Трехступенчатый пайплайн

#### 1. Исследование (Exploration)
Текущая политика π_t генерирует набор G параллельных роллаутов {o_i} для заданного промпта q.

#### 2. Синтез (Reference Estimation) 
Замороженная якорная политика π_0 (обычно исходная предобученная модель) условно зависит только от набора роллаутов {o_i} и промпта синтеза p_syn для генерации единого синтезированного эталона s:
s ~ π_0(· | p_syn, o_1:G)

Важно, что якорная политика «не видит вопрос» (question-blind), то есть не получает исходный промпт q. Это заставляет её выполнять настоящие рассуждения на основе сопоставления роллаутов, а не просто генерировать ещё один независимый ответ.

#### 3. Верификация (Reward Generation)
Синтезированный эталон s преобразуется в сигнал вознаграждения для обучения с подкреплением:

- **Для проверяемых задач** (например, математика): простой программный верификатор проверяет, совпадает ли конечный ответ роллаута с ответом в s, что даёт бинарное вознаграждение R_ver(o; s) ∈ {0, 1}.
- **Для непроверяемых задач** (например, здравоохранение): используется механизм самопредложенных критериев (self-proposed rubrics).

## Самопредложенные критерии (Self-Proposed Rubrics)

Для непроверяемых доменов, таких как диалоги в сфере здравоохранения, CaT вводит инновационный механизм самопредложенных критериев. Якорная политика π_0 генерирует специфичный для ответа набор критериев R = {r_i} - бинарных, проверяемых правил - на основе синтезированного эталона s. Затем независимый LLM-судья π_j оценивает, удовлетворяет ли роллаут o каждому критерию. Вознаграждение — это доля удовлетворённых критериев:

R_rub(o; R) = (1/n) Σ_i=1^n I[π_j(p_j; o, r_i) = "да"]

Этот подход декомпозирует грубое и часто нестабильное целостное суждение («хороший ли это ответ?») на набор детализированных, проверяемых и бинарных критериев, что помогает смягчить нестабильности и смещения, связанные с LLM-судьями.

## Обучение с подкреплением (CaT-RL)

Весь процесс интегрирован в цикл Group Relative Policy Optimization (GRPO) - эффективного по памяти варианта PPO, который обходится без обучаемой функции ценности, используя среднее вознаграждение группы роллаутов в качестве базовой линии. Это делает его особенно подходящим для структуры CaT с несколькими роллаутами, позволяя дообучать политику π_t без каких-либо внешних эталонных данных.

## Экспериментальные результаты

- **Значительный прирост производительности**: CaT-RL обеспечивает существенные улучшения по сравнению с исходной политикой, достигая до 33% относительного прироста на MATH-500 и 30% на HealthBench с моделью Llama 3.1 8B.
- **Синтез превосходит отбор**: При использовании на этапе инференса подход CaT с синтезом превосходит все протестированные бейзлайны, основанные на отборе, включая одиночный сэмпл, минимальную перплексию (minPPL) и голосование большинством. На MATH-500 он даёт до 27% относительного улучшения по сравнению с одиночным сэмплом.
- **Настоящая самокоррекция**: CaT может генерировать правильный ответ, даже когда каждый из входных роллаутов ошибочен. Это происходило примерно в 1% сложных задач из датасета MATH-500.
- **Масштабируемость**: Производительность положительно масштабируется с увеличением числа роллаутов (G).

## Ограничения

- Эффективность CaT зависит от достаточно компетентной исходной политики, поскольку слабая якорная модель не сможет надёжно синтезировать высококачественные эталоны.
- По мере того как политика становится более умелой, её роллауты становятся менее разнообразными, что лишает этап синтеза разногласий, необходимых для генерации превосходного сигнала.

## Применение

- Решение проблемы нехватки супервизии при дообучении специализированных LLM
- Области, где эталонная разметка дорога, субъективна или отсутствует
- Математические задачи и другие проверяемые домены
- Диалоги в сфере здравоохранения и другие непроверяемые домены

## Связи с другими темами

- [[../reinforcement_learning/self_proposed_rubrics.md]] - Самопредложенные критерии в контексте обучения с подкреплением
- [[../reinforcement_learning/laser_reinforcement_learning.md]] - Другой метод самонаграждения в LLM
- [[reference_free_learning.md]] - Обучение без эталонных данных
- [[synthetic_training_data.md]] - Синтетические обучающие данные для LLM
- [[llm_alignment.md]] - Выравнивание LLM
- [[group_relative_policy_optimization.md]] - GRPO, используемый в CaT

## Ссылки на источники

- Основная статья: https://arxiv.org/abs/2509.14234
- Обзор: https://arxiviq.substack.com/p/compute-as-teacher-turning-inference