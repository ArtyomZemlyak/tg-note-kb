# Диффузионные модели в контексте LLM (Large Language Models)

## Общее описание

Диффузионные модели - это класс генеративных моделей, которые научились создавать высококачественные данные (например, изображения, текст или аудио) путем постепенного добавления и удаления шума. Хотя первоначально они были разработаны в основном для генерации изображений (например, DALL-E 2, Stable Diffusion), в последние годы активно развиваются диффузионные модели для генерации текста.

## Основные принципы

### Диффузия и реверс диффузии

Диффузионные модели работают в два этапа:

1. **Прямой процесс диффузии (Forward Diffusion Process)**: Последовательность шагов, где к исходным данным постепенно добавляется шум до тех пор, пока данные не станут полностью зашумленными.

2. **Обратный процесс диффузии (Reverse Diffusion Process)**: Обученная нейронная сеть учится постепенно удалять шум из случайного шума, чтобы сгенерировать целевые данные.

### Единая теория диффузионных моделей

Согласно единой теории, существуют три основных подхода к диффузионным моделям, которые математически эквивалентны:

1. **Вариационный подход (VAE)**:
   - Происходит от вариационных автоэнкодеров (VAE)
   - Приводит к Denoising Diffusion Probabilistic Models (DDPM)
   - Оптимизация через максимизацию вариационной нижней оценки (ELBO) правдоподобия данных

2. **Подход на основе score-функции**:
   - Основан на энергетических моделях (EBM)
   - Фокусируется на выучивании score-функции (∇ₓlog p(x)) - градиента логарифма плотности данных
   - Приводит к моделям Noise Conditional Score Networks (NCSN) и фреймворку Score SDE

3. **Потоковый подход (Flow Matching)**:
   - Вдохновлён нормализующими потоками (Normalizing Flows) и нейронными ОДУ
   - Моделирует генерацию как детерминированный процесс переноса

### Probability Flow ODE (PF-ODE)

Все три подхода описываются одним детерминированным дифференциальным уравнением:
dx(t)/dt = f(x(t), t) - ½ g²(t) ∇ₓ log pₜ(x(t))

Где:
- x(t) - состояние в момент времени t
- f(x(t), t) - векторное поле
- g(t) - функция шума
- ∇ₓ log pₜ(x(t)) - score-функция плотности в момент t

Это основное уравнение описывает, как плотность вероятности pₜ(x) эволюционирует во времени для прямого SDE. PF-ODE конструируется так, чтобы поток его семплов подчинялся тому же закону, что гарантирует одинаковые маргинальные распределения для обоих процессов.

### "Трюк с обусловливанием" для эффективного обучения

Ключевая идея, позволяющая обучать диффузионные модели эффективно - трюк с обусловливанием. Целью регрессии становится score-функция условной плотности pₜ(xₜ|x₀), которая является известным гауссианом и имеет простую аналитическую форму.

Все функции потерь для диффузионных моделей в конечном счёте сводятся к единому шаблону:
L(φ) := Eₓ₀,ε,t [ ω(t) || NN_φ(xₜ, t) - (Aₜx₀ + Bₜε) ||₂² ]
где xₜ = αₜx₀ + σₜε. Нейросеть NN_φ обучается предсказывать определённую цель, сконструированную из чистых данных x₀ и шума ε.

### Алгебраическая эквивалентность параметризаций

Различные цели при обучении - шум (e-prediction), чистые данные (x-prediction), score-функция (s-prediction) или скорость (v-prediction) - алгебраически взаимозаменяемы. Их различия - вопрос реализации и стабильности, а не фундаментальных возможностей моделирования.

Однако, недавние исследования показывают, что в высокоразмерных пространствах, особенно при работе в пиксельном пространстве без латентного сжатия, различные подходы могут вести себя по-разному. Согласно статье "Back to Basics: Let Denoising Generative Models Denoise", предсказание чистого образца (x0-prediction) может быть более эффективным, чем предсказание шума (ε-prediction) или скорости (v-prediction), особенно в контексте обучения на подпространствах (manifold learning). Это объясняется тем, что чистые данные лежат на подпространстве естественных изображений, в то время как шум и скорость занимают всё пространство.

## Применение диффузионных моделей к различным модальностям

Хотя диффузионные модели были впервые успешно применены к генерации изображений, их применение распространилось на другие модальности, включая текст и видео. В контексте видео, существуют альтернативные подходы, такие как авторегрессивные LLM (например, [[models/multimodal/loong_video_generation.md|Loong]]).

### Применение к тексту
Для текста основной проблемой является **дискретность** - в отличие от изображений, текст состоит из дискретных символов/токенов, что затрудняет применение стандартных диффузионных процессов, разработанных для непрерывных данных.

Для преодоления этой проблемы разработаны следующие методы:

1. **Категориальная диффузия** - адаптирует процесс для работы с дискретными токенами
2. **Семантическая диффузия** - преобразует текст в непрерывное пространство (например, с помощью эмбеддингов)
3. **Диффузия в скрытом пространстве** - работает с промежуточными представлениями из предварительно обученных моделей

Для подробного описания текстовых диффузионных моделей см.:
- [[architectures/diffusion/text_diffusion_models.md]] - Подробное описание текстовых диффузионных моделей
- [[architectures/diffusion/bert_diffusion_connection.md]] - Связь BERT и диффузионных моделей
- [[architectures/diffusion/soft_masked_diffusion_models.md]] - Мягко маскировочные диффузионные модели
- [[architectures/diffusion/planned_diffusion.md]] - Гибридный метод, сочетающий диффузию и авторегрессивные подходы
- [[hybrid_generation_architectures.md]] - Гибридные архитектуры, включающие диффузионные компоненты
- [[../../consistency_models.md|Consistency Models]] - следующее поколение диффузионных моделей с ускоренной генерацией

### Применение к видео
Для генерации видео, помимо диффузионных моделей, существуют альтернативные подходы с использованием авторегрессивных языковых моделей, как в случае с [[models/multimodal/loong_video_generation.md|Loong]].

Для изучения архитектурных аспектов диффузионных моделей:
- [[../../computer_vision/diffusion_pixel_space.md|Диффузионные модели в пиксельном пространстве]] - подход к работе с диффузионными моделями напрямую в пиксельном пространстве с использованием x0-prediction
- [[../../computer_vision/diffusion_transformer.md|Diffusion Transformer]] - применение архитектуры трансформеров в диффузионных моделях
- [[../../computer_vision/jit_diffusion_models.md|Just Image Transformer в диффузионных моделях]] - разновидность DiT, работающая в пиксельном пространстве

## Ускорение и контроль генерации

Согласно единой теории, существуют два основных подхода к ускорению диффузионных моделей:

1. **Ускорение без обучения (улучшение солвера)**: Рассматривает обученную диффузионную модель как фиксированное векторное поле и фокусируется на разработке лучших численных решателей ОДУ. Примеры: DDIM, DEIS, DPM-Solver.

2. **Ускорение на основе обучения (выучивание карты решений)**: Выучивает карту решений ОДУ, Ψₛ→ₜ, напрямую. Примеры: Consistency Models, которые обучают быстрый, малошаговый генератор без модели-учителя.

### Управляемая генерация (guidance)
Техники управления генерацией элегантно объясняются через байесовское разложение условной score-функции. Например, Classifier-Free Guidance (CFG) моделирует условную score-функцию как линейную комбинацию безусловной и условной:
∇ₓₜ log pₜ(xₜ|c, ω) ≈ ω s_φˣ(xₜ, t, c) + (1 - ω) s_φˣ(xₜ, t, ∅)

## Сравнение с традиционными LLM

| Характеристика | Традиционные LLM | Диффузионные модели для текста |
|----------------|------------------|--------------------------------|
| Процесс генерации | Авторегрессивный | Итеративное уточнение |
| Контроль над генерацией | Средний | Высокий |
| Склонность к повторениям | Может повторять | Менее склонна к повторениям |
| Обработка условий | Сложнее для сложных условий | Лучше подходят для сложных условий |

## Заключение

Диффузионные модели для текста представляют собой перспективное направление в области генерации естественного языка, которое может предоставить альтернативу или дополнение к традиционным авторегрессивным LLM. Несмотря на существующие вызовы, особенно в области скорости генерации, потенциал этих моделей для создания разнообразного и высококачественного текста делает их важным направлением исследований.

## Современные открытия об обобщении и запоминании

Недавнее открытие, описанное в статье "Why Diffusion Models Don't Memorize" (NeurIPS 2025 Best Paper Award), объясняет, почему перепараметризованные диффузионные модели обладают хорошей обобщающей способностью, хотя имеют ёмкость для идеального запоминания обучающих данных. Исследование выявило два различных временных масштаба в обучении:

- **τ_gen** (время генерации): когда модель учится генерировать валидные сэмплы
- **τ_mem** (время меморизации): когда модель начинает запоминать конкретные примеры из обучения

Важно, что τ_mem растёт линейно с размером датасета n, а τ_gen остаётся константой. Это означает, что "ранняя остановка" (early stopping) — это не просто эвристика, а структурная необходимость, обусловленная неявной динамической регуляризацией. Подробнее об этом в [[../diffusion_models/memorization_vs_generalization.md|Меморизация против обобщения в диффузионных моделях]].

## Практическая реализация

Для практической реализации и изучения диффузионного языкового моделирования см.:
- [[../tools/dllm_library.md]] - Библиотека DLLM для унифицированного обучения и оценки диффузионных языковых моделей
- [[../tools/ai_toolkit_by_ostris.md]] - Инструмент AI Toolkit для обучения диффузионных моделей, включая поддержку LoRA для Z-Image Turbo
- [[../computer_vision/z_image_turbo.md]] - Пример современной диффузионной модели с архитектурой Single-Stream Diffusion Transformer