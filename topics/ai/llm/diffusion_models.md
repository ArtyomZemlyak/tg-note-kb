# Диффузионные модели в контексте LLM (Large Language Models)

## Общее описание

Диффузионные модели - это класс генеративных моделей, которые научились создавать высококачественные данные (например, изображения, текст или аудио) путем постепенного добавления и удаления шума. Хотя первоначально они были разработаны в основном для генерации изображений (например, DALL-E 2, Stable Diffusion), в последние годы активно развиваются диффузионные модели для генерации текста.

## Основные принципы

### Диффузия и реверс диффузии

Диффузионные модели работают в два этапа:

1. **Прямой процесс диффузии (Forward Diffusion Process)**: Последовательность шагов, где к исходным данным постепенно добавляется шум до тех пор, пока данные не станут полностью зашумленными.

2. **Обратный процесс диффузии (Reverse Diffusion Process)**: Обученная нейронная сеть учится постепенно удалять шум из случайного шума, чтобы сгенерировать целевые данные.

### Единая теория диффузионных моделей

Согласно единой теории, существуют три основных подхода к диффузионным моделям, которые математически эквивалентны:

1. **Вариационный подход (VAE)**:
   - Происходит от вариационных автоэнкодеров (VAE)
   - Приводит к Denoising Diffusion Probabilistic Models (DDPM)
   - Оптимизация через максимизацию вариационной нижней оценки (ELBO) правдоподобия данных

2. **Подход на основе score-функции**:
   - Основан на энергетических моделях (EBM)
   - Фокусируется на выучивании score-функции (∇ₓlog p(x)) - градиента логарифма плотности данных
   - Приводит к моделям Noise Conditional Score Networks (NCSN) и фреймворку Score SDE

3. **Потоковый подход (Flow Matching)**:
   - Вдохновлён нормализующими потоками (Normalizing Flows) и нейронными ОДУ
   - Моделирует генерацию как детерминированный процесс переноса

### Probability Flow ODE (PF-ODE)

Все три подхода описываются одним детерминированным дифференциальным уравнением:
dx(t)/dt = f(x(t), t) - ½ g²(t) ∇ₓ log pₜ(x(t))

Где:
- x(t) - состояние в момент времени t
- f(x(t), t) - векторное поле
- g(t) - функция шума
- ∇ₓ log pₜ(x(t)) - score-функция плотности в момент t

Это основное уравнение описывает, как плотность вероятности pₜ(x) эволюционирует во времени для прямого SDE. PF-ODE конструируется так, чтобы поток его семплов подчинялся тому же закону, что гарантирует одинаковые маргинальные распределения для обоих процессов.

### "Трюк с обусловливанием" для эффективного обучения

Ключевая идея, позволяющая обучать диффузионные модели эффективно - трюк с обусловливанием. Целью регрессии становится score-функция условной плотности pₜ(xₜ|x₀), которая является известным гауссианом и имеет простую аналитическую форму.

Все функции потерь для диффузионных моделей в конечном счёте сводятся к единому шаблону:
L(φ) := Eₓ₀,ε,t [ ω(t) || NN_φ(xₜ, t) - (Aₜx₀ + Bₜε) ||₂² ]
где xₜ = αₜx₀ + σₜε. Нейросеть NN_φ обучается предсказывать определённую цель, сконструированную из чистых данных x₀ и шума ε.

### Алгебраическая эквивалентность параметризаций

Различные цели при обучении - шум (e-prediction), чистые данные (x-prediction), score-функция (s-prediction) или скорость (v-prediction) - алгебраически взаимозаменяемы. Их различия - вопрос реализации и стабильности, а не фундаментальных возможностей моделирования.

## Применение диффузионных моделей к тексту

Хотя диффузионные модели были впервые успешно применены к генерации изображений, их применение к тексту представляет собой отдельную задачу из-за дискретной природы текстовых данных. Основной проблемой является **дискретность** текстовых данных - в отличие от изображений, текст состоит из дискретных символов/токенов, что затрудняет применение стандартных диффузионных процессов, разработанных для непрерывных данных.

Для преодоления этой проблемы разработаны следующие методы:

1. **Категориальная диффузия** - адаптирует процесс для работы с дискретными токенами
2. **Семантическая диффузия** - преобразует текст в непрерывное пространство (например, с помощью эмбеддингов)
3. **Диффузия в скрытом пространстве** - работает с промежуточными представлениями из предварительно обученных моделей

Для подробного описания текстовых диффузионных моделей см.:
- [[architectures/diffusion/text_diffusion_models.md]] - Подробное описание текстовых диффузионных моделей
- [[architectures/diffusion/bert_diffusion_connection.md]] - Связь BERT и диффузионных моделей
- [[architectures/diffusion/soft_masked_diffusion_models.md]] - Мягко маскировочные диффузионные модели
- [[architectures/diffusion/planned_diffusion.md]] - Гибридный метод, сочетающий диффузию и авторегрессивные подходы
- [[hybrid_generation_architectures.md]] - Гибридные архитектуры, включающие диффузионные компоненты
- [[../../consistency_models.md|Consistency Models]] - следующее поколение диффузионных моделей с ускоренной генерацией

## Ускорение и контроль генерации

Согласно единой теории, существуют два основных подхода к ускорению диффузионных моделей:

1. **Ускорение без обучения (улучшение солвера)**: Рассматривает обученную диффузионную модель как фиксированное векторное поле и фокусируется на разработке лучших численных решателей ОДУ. Примеры: DDIM, DEIS, DPM-Solver.

2. **Ускорение на основе обучения (выучивание карты решений)**: Выучивает карту решений ОДУ, Ψₛ→ₜ, напрямую. Примеры: Consistency Models, которые обучают быстрый, малошаговый генератор без модели-учителя.

### Управляемая генерация (guidance)
Техники управления генерацией элегантно объясняются через байесовское разложение условной score-функции. Например, Classifier-Free Guidance (CFG) моделирует условную score-функцию как линейную комбинацию безусловной и условной:
∇ₓₜ log pₜ(xₜ|c, ω) ≈ ω s_φˣ(xₜ, t, c) + (1 - ω) s_φˣ(xₜ, t, ∅)

## Сравнение с традиционными LLM

| Характеристика | Традиционные LLM | Диффузионные модели для текста |
|----------------|------------------|--------------------------------|
| Процесс генерации | Авторегрессивный | Итеративное уточнение |
| Контроль над генерацией | Средний | Высокий |
| Склонность к повторениям | Может повторять | Менее склонна к повторениям |
| Обработка условий | Сложнее для сложных условий | Лучше подходят для сложных условий |

## Заключение

Диффузионные модели для текста представляют собой перспективное направление в области генерации естественного языка, которое может предоставить альтернативу или дополнение к традиционным авторегрессивным LLM. Несмотря на существующие вызовы, особенно в области скорости генерации, потенциал этих моделей для создания разнообразного и высококачественного текста делает их важным направлением исследований.

## Практическая реализация

Для практической реализации и изучения диффузионного языкового моделирования см.:
- [[../tools/dllm_library.md]] - Библиотека DLLM для унифицированного обучения и оценки диффузионных языковых моделей