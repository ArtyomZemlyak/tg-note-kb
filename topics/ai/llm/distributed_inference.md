# Распределённый инференс

## Общее описание

Распределённый инференс - это подход к выполнению инференса больших языковых моделей с использованием нескольких вычислительных узлов или устройств. Это позволяет обрабатывать модели, которые слишком велики для одного устройства, а также улучшает производительность за счёт параллелизма.

## Типы параллелизма

### Тензорный параллелизм (Tensor Parallelism)
- Разделение весов модели на несколько устройств
- Каждое устройство обрабатывает подмножество тензорных операций
- Позволяет распределить вычисления внутри слоёв модели

### Пайплайн-параллелизм (Pipeline Parallelism) 
- Разделение модели на этапы, выполняемые на разных устройствах
- Каждое устройство обрабатывает разные слои модели
- Позволяет увеличить эффективность за счёт конвейерной обработки

### Данный параллелизм (Data Parallelism)
- Копирование модели на несколько устройств
- Параллельная обработка разных батчей данных
- Увеличивает пропускную способность за счёт параллельной обработки

### Параллелизм экспертов (Expert Parallelism)
- Используется в моделях Mixture-of-Experts (MoE)
- Разные эксперты модели распределены по разным устройствам
- Позволяет масштабировать модели MoE до очень больших размеров

## Применение в vLLM

vLLM поддерживает несколько типов параллелизма:
- Тензорный параллелизм для распределения вычислений между GPU
- Пайплайн-параллелизм для оптимизации выполнения длинных моделей
- Параллелизм экспертов для моделей MoE (например, Mixtral)

## Современные достижения: Оптимизация для AWS EFA

Недавние исследования Perplexity представили прорыв в распределённом инференсе MoE моделей на облачной инфраструктуре AWS. Основная проблема заключалась в том, что AWS EFA (Elastic Fabric Adapter) не поддерживает GPUDirect Async, что делало невозможным эффективную маршрутизацию MoE с задержками ниже 1 мс.

### Решение: Гибридное CPU-GPU взаимодействие

Инженеры Perplexity разработали новый подход:
- Использование CPU-координации для обеспечения синхронизации GPU почти напрямую
- Специализированные ядра упаковывают токены в единичные RDMA-записи прямо с GPU
- Специальные CPU-потоки запускают передачу и перекрывают её с вычислениями GEMM
- Использование TransferEngine, изначально разработанного для передач KV-кэша

### Результаты

- Эффективный инференс моделей с 1 триллионом параметров на стандартных AWS-кластерах
- MoE с 1T параметрами работает практически без деградации
- Многонодовый режим сопоставим или быстрее однонодового на 671B DeepSeek V3 при средних батчах
- Открыт путь к сервингу Kimi K2 (1T параметров) на AWS

## Преимущества

- Возможность инференса моделей, превышающих память одного устройства
- Повышенная пропускная способность за счёт параллельной обработки
- Более эффективное использование доступных вычислительных ресурсов
- Масштабируемость в зависимости от доступной инфраструктуры

## Вызовы и ограничения

- Усложнение архитектуры и управления
- Коммуникационные издержки между устройствами
- Необходимость синхронизации между этапами
- Потенциальное увеличение задержки из-за межузловых коммуникаций
- Сложности масштабирования на GPU-кластерах в облаке из-за ограничений сетевой инфраструктуры (например, AWS EFA не поддерживает GPUDirect Async)
- Требования к специализированным решениям для эффективного распределённого инференса MoE моделей

## Связи с другими темами

- [[gpu_memory_management.md]] - Управление памятью в распределённой среде
- [[model_parallelization_strategies.md]] - Стратегии распараллеливания моделей
- [[mixture_of_experts_architecture.md]] - Архитектура MoE и эксперта-параллелизм
- [[vllm_integration.md]] - Поддержка распределённого инференса в vLLM
- [[distributed_training.md]] - Сравнение с распределённым обучением
- [[../../hardware/aws_efa_networking_for_ai.md]] - Оптимизация распределённого инференса с использованием AWS EFA

## Источники

- Документация по распределённому обучению и инференсу
- Исследования по различным стратегиям параллелизма
- Практики использования в производственных системах