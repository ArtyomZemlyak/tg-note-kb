# Эффективность инференса: Сравнение архитектур LLM

## Обзор

Эффективность инференса (inference efficiency) - критический фактор при развертывании больших языковых моделей (LLM) в реальных приложениях. Она охватывает как вычислительные затраты, так и пропускную способность, задержки и энергопотребление систем, использующих LLM.

## Показатели эффективности инференса

### Основные метрики
- **Пропускная способность (Throughput)**: количество токенов, генерируемых в единицу времени
- **Задержка (Latency)**: время, необходимое для генерации первого токена или всей последовательности
- **FLOPs (Floating Point Operations)**: количество операций с плавающей запятой, требуемых для генерации
- **Потребление памяти**: объем VRAM или RAM, необходимый для выполнения инференса
- **Энергопотребление**: количество энергии, затрачиваемое на генерацию текста

### Парето-оптимальность
- В идеальном сценарии модель должна находиться на Парето-фронте, балансируя между качеством и вычислительными затратами
- Модели, доминирующие на Парето-фронте "качество/затраты на инференс", являются наиболее эффективными

## Сравнение архитектур по эффективности инференса

### Энкодер-декодер архитектура (T5-подобные)

**Преимущества эффективности**:
- **Высокая пропускная способность**: модели показывают значительно более высокую пропускную способность как при обучении, так и при инференсе
- **Доминирование на Парето-фронте**: на осях качество/затраты на инференс (FLOPs) энкодер-декодер архитектуры практически полностью доминируют на Парето-фронте
- **Эффективное использование контекста**: энкодер обрабатывает весь промпт за один раз параллельно, а декодер эффективно обращается к закодированному контексту через cross-attention
- **Лучшая локальность данных**: cross-attention обращается только к финальному выходу энкодера в каждом слое декодера, что обеспечивает лучшую локальность и более быстрые вычисления
- **Оптимизация KV-кеша**: как продемонстрировано в работе YOCO, архитектуры с cross-attention могут быть более эффективными в использовании памяти по сравнению с традиционными KV-кешами декодер-только моделей

**Примеры эффективности**:
- В работе T5Gemma энкодер-декодер модели (например, end-dec 9B-9B и 2B-2B) имели схожую латентность с декодер-только моделями (dec 9B и 2B соответственно), но строго лучший перформанс
- Несимметричный вариант с большим энкодером и маленьким декодером (9B-2B) был аналогичен декодеру 2B по латентности, но лучше по перформансу чем 2B-2B

### Декодер-только архитектура (GPT-подобные)

**Характеристики эффективности**:
- **Традиционно высокая эффективность**: долгое время считалось, что декодер-только модели более эффективны
- **KV-кеширование**: эффективно используется на этапе prefilling и при последующей генерации
- **Каузальное внимание**: каждый токен может обращаться только к предыдущим токенам, что ограничивает вычисления

**Потенциальные ограничения**:
- **Квадратичная сложность**: с ростом длины контекста сложность внимания увеличивается квадратично
- **Проблемы с длинными контекстами**: "затухание локальности" - токены уделяют меньше внимания удалённым токенам по мере удлинения последовательности
- **Меньшая архитектурная оптимизация**: по сравнению с энкодер-декодером, декодер-только модели могут быть менее оптимизированы для некоторых задач

### Encoder-only архитектура (BERT-подобные)

**Область применения**:
- **Не для генерации**: архитектура не может генерировать текст автoregressive
- **Эффективна для задач понимания**: классификация, эмбеддинги, анализ текста
- **Параллельная обработка**: вся входная последовательность обрабатывается параллельно с двунаправленным вниманием

## Техники оптимизации эффективности

### Специализированные механизмы внимания
- **Multi-Query Attention (MQA)** и **Grouped-Query Attention (GQA)**: снижают требования к KV кэшу для автoregressive генерации
- **Multihead Latent Attention (MLA)**: использует низкоразмерные "латентные пространства", уменьшая KV кэш
- **DeepSeek Sparse Attention (DSA)**: снижает вычислительные затраты при сохранении качества
- **Log-Linear Attention**: снижает сложность с O(n²) до O(n log n)

### Mixture of Experts (MoE)
- **Разреженные вычисления**: только некоторые параметры активируются во время инференса
- **Более быстрый инференс**: по сравнению с плотными моделями с тем же количеством параметров
- **Эффективность**: позволяет масштабировать модели при ограниченных вычислительных ресурсах

### Оптимизации кэширования
- **KV-кеширование**: кеширование ключевых и значимых векторов для избежания повторных вычислений
- **Cross-attention кэширование**: в энкодер-декодер архитектурах кешируется выход энкодера
- **Префильтрация кеша**: удаление или сжатие наименее важных элементов кеша

## Модели с высокой эффективностью инференса

### Энкодер-декодер
- **T5 и его варианты**: исторически эффективные для задач преобразования текста
- **T5Gemma (Encoder-Decoder Gemma)**: показали доминирование по балансу качество/эффективность
- **BART**: комбинирует преимущества энкодера и декодера

### Декодер-только
- **LLaMA, Mistral**: оптимизированные для эффективности декодер-только модели
- **GPT-3.5, GPT-4**: оптимизированные для генерации, но с высокими затратами

## Влияние архитектуры на эффективность

Согласно недавним исследованиям:
- **Архитектурные различия** играют ключевую роль в эффективности инференса
- **Энкодер-декодер архитектуры** могут превосходить декодер-только по эффективности, особенно после файнтюнинга
- **Специализированная архитектура** предлагает дополнительные выгоды помимо общих оптимизаций

## Будущие направления

- **Гибридные архитектуры**: объединение различных подходов для оптимизации эффективности
- **Адаптивные архитектуры**: изменение структуры в реальном времени в зависимости от задачи
- **Специализированные аппаратные решения**: оптимизированные для конкретных архитектур

## Связи с другими темами

- [[../llm_architectures_comparison.md]] - Общее сравнение архитектур LLM
- [[../architectures/encoder_decoder_vs_decoder_only.md]] - Сравнение архитектур с акцентом на эффективность
- [[../../nlp/transformers/transformer_architecture.md]] - Подробное описание архитектуры трансформеров
- [[../scaling/llm_scaling_architectures.md]] - Скейлинг различных архитектур LLM и его влияние на эффективность
- [[model_quantization.md]] - Квантование моделей для повышения эффективности
- [[kv_cache_optimization.md]] - Оптимизация KV-кеширования
- [[hardware_acceleration.md]] - Аппаратные ускорители для инференса

## Источники

1. [Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model](https://arxiv.org/abs/2510.26622) - исследования, показывающие доминирование энкодер-декодер архитектур на Парето-фронте "качество/затраты на инференс"
2. [Encoder-Decoder Gemma (T5Gemma)](https://arxiv.org/abs/2504.06225) - работа о преимуществах эффективности энкодер-декодер моделей
3. "Attention is All You Need" - оригинальная статья о трансформерах
4. Исследования по Mixture of Experts архитектурам
5. Исследования по оптимизации KV-кеширования