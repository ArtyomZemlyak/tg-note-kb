# Group Relative Policy Optimization (GRPO)

## Краткое описание

Group Relative Policy Optimization (GRPO) - это эффективный по памяти вариант алгоритма PPO (Proximal Policy Optimization), который используется в методе Compute as Teacher (CaT) для обучения больших языковых моделей. В отличие от стандартного PPO, GRPO обходится без обучаемой функции ценности, используя среднее вознаграждение группы роллаутов в качестве базовой линии.

## Контекст и мотивация

При обучении с подкреплением (RL) для больших языковых моделей часто используется алгоритм PPO, который требует отдельной функции ценности для оценки качества принимаемых решений. Однако обучение отдельной функции ценности требует дополнительных вычислительных ресурсов и усложняет процесс обучения. GRPO предлагает упрощенный подход, который особенно хорошо работает в контексте методов, использующих несколько параллельных роллаутов, таких как Compute as Teacher и ToolOrchestra.

## Механизм работы

### Базовая линия на основе группы
- Вместо использования отдельной обучаемой функции ценности как baseline
- GRPO использует среднее вознаграждение текущей группы роллаутов как baseline
- Это устраняет необходимость в отдельной функции ценности

### Подходит для сценариев с множественными роллаутами
- Особенно эффективен для методов, которые генерируют несколько параллельных ответов (роллаутов)
- Идеально подходит для архитектуры Compute as Teacher, где одновременно генерируется G роллаутов
- Позволяет эффективно использовать информацию из всех роллаутов в группе

### Экономия памяти
- Отсутствие необходимости хранить параметры отдельной функции ценности
- Уменьшение общего объема вычислений и требований к памяти
- Упрощение архитектуры обучения

## Применение в Compute as Teacher

В рамках метода Compute as Teacher:
- Текущая политика π_t генерирует G параллельных роллаутов
- Каждый роллаут получает вознаграждение на основе синтезированного эталона
- GRPO использует среднее вознаграждение по всем G роллаутам как baseline
- Это позволяет дообучать политику π_t без эталонных данных

## Применение в ToolOrchestra

В рамках фреймворка ToolOrchestra:
- GRPO решает проблему масштабирования наград в традиционном PPO, нормализуя награды внутри групп траекторий для одного и того же входа
- GRPO нормализует награды внутри группы траекторий для одного и того же входа. Преимущество (advantage) A считается как: A(τ) = (R(τ) - mean(R)) / std(R)
- Это позволяет модели учиться понимать, что *относительно других вариантов* в данных условиях вызов мат-модели был лучше, чем вызов GPT-5
- Позволяет обучать политику на основе относительного сравнения траекторий, вместо абсолютной шкалы награды
- Особенно эффективен в условиях вектора предпочтений P = [p_outcome, p_compute, p_latency, ...], где масштаб награды может сильно колебаться
- Используется для обучения оркестраторов, которые должны балансировать между точностью решения, ценой вычислений и предпочтениями пользователя

## Улучшения и стабилизация

### Unbiased KL Estimate
В процессе практического применения GRPO в моделях, таких как DeepSeek-V3.2, был внедрен Unbiased KL Estimate - улучшение, направленное на устранение систематической ошибки в оценке дивергенции Кульбака-Лейблера. Это исправление использует тот же коэффициент важности (importance ratio), что и для основной функции потерь, для перевзвешивания KL-члена. Это делает градиент KL-ошибки несмещенным, предотвращая градиентный взрыв и обеспечивая более стабильное обучение.

Проблема заключалась в том, что в оригинальном GRPO, когда токены имели значительно более низкую вероятность под текущей политикой πθ по сравнению с предыдущей политикой πold, градиент лосса назначал непропорционально большие веса для максимизации правдоподобия этих токенов, что приводило к:
- Шумным градиентным обновлениям
- Нестабильной динамике обучения  
- Деградации качества сэмплов на последующих итерациях

Unbiased KL Estimate решает эти проблемы и улучшает стабильность алгоритма.

## Преимущества

### Вычислительная эффективность
- Уменьшение вычислительных затрат за счет отказа от отдельной функции ценности
- Эффективное использование ресурсов при обучении с несколькими роллаутами

### Простота реализации
- Упрощенная архитектура по сравнению с традиционным PPO
- Меньше параметров для настройки и управления

### Совместимость с методами синтеза
- Идеально подходит для методов, использующих синтез информации из нескольких роллаутов
- Обеспечивает стабильное обучение в контексте синтеза эталонных ответов

## Сравнение с PPO

| Аспект | PPO | GRPO |
|--------|-----|-----|
| Функция ценности | Отдельная обучаемая функция | Среднее по группе роллаутов |
| Вычислительные затраты | Выше (дополнительная сеть) | Ниже (без отдельной функции) |
| Память | Больше (параметры функции ценности) | Меньше (без функции ценности) |
| Применимость к CaT | Возможна, но не оптимальна | Идеально подходит |

## Связи с другими темами

- [[compute_as_teacher.md]] - Применение GRPO в методе Compute as Teacher
- [[../agents/toolorchestra_framework.md]] - Применение GRPO в фреймворке ToolOrchestra для оркестрации моделей
- [[reinforcement_learning_from_human_feedback.md]] - Контекст более широких методов RL
- [[ppo_algorithm.md]] - Базовый алгоритм, на котором основан GRPO
- [[reference_free_learning.md]] - Общий контекст обучения без эталонов
- [[../reinforcement_learning/practical_challenges/fp16_bf16_precision_in_rl.md]] - Использование FP16 вместо BF16 для стабилизации GRPO и других RL-алгоритмов
- [[unbiased_kl_estimate_in_grpo.md]] - Улучшение GRPO для устранения систематической ошибки в KL-дивергенции
- [[deepseek_v3_2_reinforcement_learning_agent_training.md]] - Использование улучшенного GRPO в обучении DeepSeek-V3.2

## Ссылки на источники

- Основная статья о Compute as Teacher: https://arxiv.org/abs/2509.14234
- Описание PPO: Proximal Policy Optimization Algorithms