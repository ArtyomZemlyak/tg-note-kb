# Group Relative Policy Optimization (GRPO)

## Краткое описание

Group Relative Policy Optimization (GRPO) - это эффективный по памяти вариант алгоритма PPO (Proximal Policy Optimization), который используется в методе Compute as Teacher (CaT) для обучения больших языковых моделей. В отличие от стандартного PPO, GRPO обходится без обучаемой функции ценности, используя среднее вознаграждение группы роллаутов в качестве базовой линии.

## Контекст и мотивация

При обучении с подкреплением (RL) для больших языковых моделей часто используется алгоритм PPO, который требует отдельной функции ценности для оценки качества принимаемых решений. Однако обучение отдельной функции ценности требует дополнительных вычислительных ресурсов и усложняет процесс обучения. GRPO предлагает упрощенный подход, который особенно хорошо работает в контексте методов, использующих несколько параллельных роллаутов, таких как Compute as Teacher.

## Механизм работы

### Базовая линия на основе группы
- Вместо использования отдельной обучаемой функции ценности как baseline
- GRPO использует среднее вознаграждение текущей группы роллаутов как baseline
- Это устраняет необходимость в отдельной функции ценности

### Подходит для сценариев с множественными роллаутами
- Особенно эффективен для методов, которые генерируют несколько параллельных ответов (роллаутов)
- Идеально подходит для архитектуры Compute as Teacher, где одновременно генерируется G роллаутов
- Позволяет эффективно использовать информацию из всех роллаутов в группе

### Экономия памяти
- Отсутствие необходимости хранить параметры отдельной функции ценности
- Уменьшение общего объема вычислений и требований к памяти
- Упрощение архитектуры обучения

## Применение в Compute as Teacher

В рамках метода Compute as Teacher:
- Текущая политика π_t генерирует G параллельных роллаутов
- Каждый роллаут получает вознаграждение на основе синтезированного эталона
- GRPO использует среднее вознаграждение по всем G роллаутам как baseline
- Это позволяет дообучать политику π_t без эталонных данных

## Преимущества

### Вычислительная эффективность
- Уменьшение вычислительных затрат за счет отказа от отдельной функции ценности
- Эффективное использование ресурсов при обучении с несколькими роллаутами

### Простота реализации
- Упрощенная архитектура по сравнению с традиционным PPO
- Меньше параметров для настройки и управления

### Совместимость с методами синтеза
- Идеально подходит для методов, использующих синтез информации из нескольких роллаутов
- Обеспечивает стабильное обучение в контексте синтеза эталонных ответов

## Сравнение с PPO

| Аспект | PPO | GRPO |
|--------|-----|-----|
| Функция ценности | Отдельная обучаемая функция | Среднее по группе роллаутов |
| Вычислительные затраты | Выше (дополнительная сеть) | Ниже (без отдельной функции) |
| Память | Больше (параметры функции ценности) | Меньше (без функции ценности) |
| Применимость к CaT | Возможна, но не оптимальна | Идеально подходит |

## Связи с другими темами

- [[compute_as_teacher.md]] - Главное применение GRPO
- [[reinforcement_learning_from_human_feedback.md]] - Контекст более широких методов RL
- [[ppo_algorithm.md]] - Базовый алгоритм, на котором основан GRPO
- [[reference_free_learning.md]] - Общий контекст обучения без эталонов

## Ссылки на источники

- Основная статья о Compute as Teacher: https://arxiv.org/abs/2509.14234
- Описание PPO: Proximal Policy Optimization Algorithms