# Binary Retrieval-Augmented Reward (Binary RAR) - Бинарное вознаграждение с дополненной выдачей

## Описание

Binary Retrieval-Augmented Reward (Binary RAR) - это новая методология онлайн-обучения с подкреплением для снижения фактических ошибок (галлюцинаций) в языковых моделях. В отличие от традиционных подходов, использующих сложные непрерывные оценки, Binary RAR использует простой бинарный сигнал: 1, если весь ответ модели фактологически верен при проверке по найденным документам, и 0, если обнаружено хоть одно противоречие.

## Ключевая инновация

Основная новизна метода заключается в использовании строгого бинарного вознаграждения, которое эффективно решает проблему "взлома вознаграждения" (reward hacking), характерную для систем с непрерывными оценками. Модель получает полное вознаграждение только если весь её ответ свободен от противоречий, что побуждает её разрабатывать более сложные внутренние механизмы оценки неопределенности.

## Архитектура

### Основные компоненты

1. **Генерация** - Политика модели (например, Qwen3) генерирует ответ на запрос
2. **Извлечение** - Ретривер BM25 извлекает релевантные доказательства из предварительно кэшированного хранилища документов
3. **Проверка** - Мощная модель-верификатор (например, Qwen3-32B) сравнивает весь ответ с извлечёнными документами
4. **Вознаграждение** - Если верификатор находит хоть одно фактическое противоречие, вознаграждение равно 0. Только полностью свободный от противоречий ответ получает вознаграждение, равное 1

### Математическая формулировка

Целевая функция основана на оптимизации с ограничением KL-дивергенции:

$$\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)} [r(x, y) - \beta D_{KL}(\pi_\theta(\cdot | x) || \pi_{\text{ref}}(\cdot | x))]$$

где функция вознаграждения определяется как:

$$r(x,y) = \begin{cases} 1 & \text{если между } (x, y) \text{ и найденными доказательствами нет противоречий} \\ 0 & \text{в противном случае} \end{cases}$$

## Преимущества

### 1. Снижение галлюцинаций
- Существенное снижение уровня галлюцинаций (на 39.3% в задачах генерации длинных текстов)
- Эффективное уменьшение фактических ошибок в сравнении с SFT, DPO и RL с непрерывными вознаграждениями

### 2. Сохранение полезности
- Уникально сохраняет способности модели, такие как следование инструкциям и рассуждения
- Избегает типичного компромисса между галлюцинациями и полезностью
- Средняя оценка полезности модели Qwen3-8B, обученной с помощью Binary RAR, составила 62.2, что сопоставимо с 61.6 у базовой модели

### 3. Развитие сложного поведения
- **Избирательная фильтрация**: В генерации длинных текстов модель учится быть более точной, а не просто более лаконичной
- **Калиброванный отказ от ответа**: Модель стратегически использует фразу "Я не знаю" для вопросов, на которые она в противном случае ответила бы неверно

## Алгоритм

Binary RAR использует алгоритм Group Relative Policy Optimization (GRPO) для оптимизации политики модели при ограничении KL-дивергенции. Этот подход позволяет модели учиться на собственных ошибках в режиме реального времени, не отклоняясь слишком сильно от исходных способностей.

## Сравнение с другими методами

| Метод | Снижение галлюцинаций | Сохранение полезности | Устойчивость к "взлому вознаграждения" |
|-------|----------------------|--------------------|-----------------------------------|
| SFT | Умеренное | Слабое | Н/Д |
| DPO | Умеренное | Умеренное | Н/Д |
| RL с VeriScore | Умеренное | Слабое | Низкая |
| Binary RAR | **Высокое** | **Высокое** | **Высокая** |

## Экспериментальные результаты

- В генерации длинных текстов уровень галлюцинаций упал с 61.9% до 37.5% для модели Qwen3-8B
- В задачах "вопрос-ответ" количество неверных ответов на датасете POPQA уменьшилось на 44.4%
- Повышение точности без значительного снижения полезности

## Ограничения

- Зависимость от качества внешнего ретривера и модели-верификатора
- Необходимость тщательной настройки коэффициента KL-регуляризации
- Возможность генерации вырожденных, слишком коротких ответов при неправильной настройке

## Будущие направления

- Переход от предварительно кэшированного хранилища документов к полностью динамической среде с извлечением информации из интернета в реальном времени
- Решение проблем с задержкой и зашумленными результатами поиска
- Повышение робастности внешнего ретривера и модели-верификатора

## Связи с другими темами

- [[../reinforcement_learning/laser_reinforcement_learning.md]] - Другой метод обучения с подкреплением для LLM
- [[hallucinations_in_llm.md]] - Общая информация о галлюцинациях в языковых моделях
- [[psiloqa.md]] - Методы обнаружения галлюцинаций
- [[../rlhf.md]] - Обучение с подкреплением с человеческой обратной связью