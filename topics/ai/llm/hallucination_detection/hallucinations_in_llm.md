# Обнаружение галлюцинаций в языковых моделях (LLM)

## Определение
**Галлюцинация в языковой модели** — это когда модель генерирует информацию, которая кажется правдоподобной, но на самом деле неверна, вымышленна или не подтверждена входными данными или контекстом.

## Типы галлюцинаций
- **Фактические галлюцинации** — модель генерирует ложную информацию о реальных событиях, людях или фактах
- **Логические галлюцинации** — модель делает логические выводы, не поддерживаемые аргументами
- **Извлечение спанов (span-level)** — галлюцинации, происходящие в конкретных частях текста (спанах), а не на уровне всего ответа

## Вызовы и проблемы
- Галлюцинации снижают надежность и доверие к языковым моделям
- Они особенно проблематичны в задачах, требующих фактической точности
- Автоматическое обнаружение галлюцинаций остается сложной задачей

## Методы обнаружения
- Статистические методы на основе уверенности модели
- Сравнение с внешними базами знаний
- Обучение специализированных детекторов галлюцинаций
- Постпроцессинговые методы верификации

## Современные подходы
- **PsiloQA** - многоязычная система обнаружения галлюцинаций на уровне спанов [[ai/llm/hallucination_detection/psiloqa.md]] - крупнейший бенчмарк с автоматической разметкой
- **Binary RAR** - метод обучения с подкреплением с бинарным вознаграждением для снижения галлюцинаций [[binary_rar.md]] - обучение модели избегать генерации неверной информации через строгую бинарную проверку по внешним источникам
- Другие системы и методы активно разрабатываются в области NLP и LLM

## Применения
- Повышение надежности QA-систем
- Фактчекинг сгенерированного контента
- Повышение качества чат-ботов и ассистентов
- Академические исследования в области безопасности ИИ