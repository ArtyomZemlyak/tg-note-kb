# Система Alibaba Aegaeon

## Описание

Aegaeon - это система авто-масштабирования на уровне токенов для мультимодального инференса, представленная Alibaba Cloud. Система позволяет достичь значительного ускорения инференса и снижения потребления GPU ресурсов.

## Основные характеристики

- **Тип системы**: Авто-масштабирование на уровне токенов для мультимодального инференса
- **Производительность**: 84% ускорение мультимодального инференса
- **Экономия ресурсов**: 82% снижение потребления GPU (с 1,192 до 213 H200)
- **Улучшение производительности**: 1.5-9x рост "goodput" (RPS в рамках Service Level Objective - SLO)
- **Производственный кластер**: 28 моделей размером 1.8-7B (TP=1) и 19 моделей размером 32-72B (TP=4)

## Архитектурные решения

### 1. Ручное управление памятью
- Реализована ручная аллокация и управление памятью
- Это дает ускорение само по себе (см. YaFSDP)
- Оптимизация позволяет более эффективно использовать доступную GPU память

### 2. Отказ от кластерного авто-масштабирования
- Вместо масштабирования по количеству машин используется пуллинг одного образа
- Машины запускаются один раз с одним образом
- Ресурсы затем разделяются между разными моделями
- Это позволяет избежать проблем с зависимостями между моделями
- Используется vLLM для обеспечения совместимости зависимостей

### 3. Глубокая интеграция с vLLM
- Инициализация происходит один раз при старте (vLLM/TensorRT инициализируются примерно за 30 секунд)
- Загрузка и выгрузка моделей и кешей происходит вручную
- Это позволяет избежать задержек при переключении между моделями

### 4. Шедулинг на уровне токенов
- Система реализует шедулинг на уровне отдельных токенов
- Хотя авторы выражают сомнения в значимости этой гранулярности для ускорения
- SLA на TPOT (Time Per Output Token) измеряется в десятках миллисекунд
- Загрузка весов из CPU в GPU занимает сотни миллисекунд для больших моделей

## Производственные результаты

- Тестирование проводилось в Alibaba Model Studio более 3 месяцев
- Уровень утилизации GPU подскочил с ~30% до ~60%
- Это указывает на значительный потенциал для дальнейших улучшений в движках мультимодального инференса

## Связи с другими темами

- [[gpu_memory_management.md]] - Ручное управление памятью в Aegaeon
- [[multimodal_inference_optimization.md]] - Оптимизация мультимодального инференса
- [[model_deployment_strategies.md]] - Стратегии деплоя моделей
- [[vllm_integration.md]] - Интеграция с vLLM
- [[token_level_scheduling.md]] - Шедулинг на уровне токенов
- [[sglang]] - Альтернативный фреймворк для обслуживания LLM с высокой производительностью
- [[../../../computer_vision/multimodal_models.md]] - Общие понятия о мультимодальных моделях

## Источники

- Публикация от Alibaba Cloud о системе Aegaeon
- Пост в сообществе @deploy_ml о 84% ускорении мультимодального инференса