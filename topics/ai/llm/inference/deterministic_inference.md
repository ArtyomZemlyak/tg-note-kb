# Детерминированный инференс в LLM

## Определение

Детерминированный инференс в больших языковых моделях (LLM) - это подход, обеспечивающий воспроизводимость выходных данных модели при одинаковых входных данных и параметрах. То есть, при запуске одной и той же задачи с одинаковыми параметрами, модель будет выдавать одинаковые результаты каждый раз.

## Почему это важно

### Для обучения с подкреплением (RL)
- Обеспечивает стабильные logprobs при разных запусках
- Уменьшает стохастический шум
- Делает тренировку RL более стабильной, воспроизводимой и поддающейся отладке

### Для тестирования и отладки
- Позволяет точно воспроизводить результаты
- Упрощает отладку и идентификацию проблем
- Обеспечивает надежную оценку изменений

### Для продуктового использования
- Повышает надежность систем
- Улучшает пользовательский опыт
- Упрощает логирование и аудит

## Причины недетерминированности в стандартном инференсе

Основной причиной недетерминированности является изменяющийся размер батча. Разные размеры батча вызывают различное разделение GPU-ядрами операций редукции, что приводит к различным порядкам сложения. Из-за неассоциативности чисел с плавающей запятой ((a + b) + c ≠ a + (b + c)) получаются разные результаты даже при одинаковых входах.

## Технические решения

### Batch-invariant операторы
Решение, реализованное в таких системах как SGLang, заключается в использовании batch-invariant операторов, которые обеспечивают одинаковый результат независимо от размера батча.

### Поддержка различных бэкендов
Для детерминированного инференса требуются специальные бэкенды внимания, такие как:
- FlashInfer
- FlashAttention 3 (FA3)
- Triton

## Применение в фреймворках

### SGLang
Один из первых фреймворков, который реализовал детерминированный инференс с поддержкой:
- Chunked prefill
- CUDA графов
- Radix cache
- Нежадной выборки (даже при температуре > 0)

## Связь с другими темами

- [[tools/sglang.md]] - SGLang, один из первых фреймворков с детерминированным инференсом
- [[tools/slime.md]] - SLiME, использует SGLang для RL-тренировки с детерминированным выводом
- [[../../reinforcement_learning/ppo_algorithm.md]] - PPO, алгоритм обучения с подкреплением, который может выиграть от детерминированного инференса
- [[../../reinforcement_learning/deep_rl/deep_rl_algorithms.md]] - Описание DDPG (Deep Deterministic Policy Gradient), где "детерминистичность" также важна, но в другом контексте
- [[../../reinforcement_learning/survey_rl_comprehensive.md]] - Обзор RL с упоминанием детерминистских подходов