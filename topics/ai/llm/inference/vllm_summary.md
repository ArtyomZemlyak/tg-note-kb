# vLLM: Обзор и ключевые особенности

## Резюме

vLLM (Very Large Language Model) - это современная высокопроизводительная библиотека для инференса и сервинга больших языковых моделей. Разработанная в лаборатории Sky Computing Lab в UC Berkeley, она предлагает решительные улучшения в эффективности инференса LLM за счёт инновационных методов управления памятью и оптимизации вычислений.

## Ключевые особенности

- **PagedAttention**: Инновационный механизм управления вниманием, вдохновлённый страничной памятью в ОС
- **Непрерывная пакетная обработка**: Обработка новых запросов без ожидания завершения предыдущих
- **Интеграции**: Поддержка FlashAttention, FlashInfer и оптимизированных CUDA-ячеек
- **Квантование**: Поддержка GPTQ, AWQ, AutoRound, INT4, INT8, FP8
- **Масштабируемость**: Поддержка тензорного, пайплайн- и эксперта-параллелизма
- **Совместимость**: Полная совместимость с экосистемой Hugging Face
- **Speculative Decoding**: Ускорение генерации за счёт предположительного декодирования
- **Chunked Prefill**: Оптимизация для обработки длинных входов
- **Multi-LoRA**: Эффективное выполнение инференса нескольких тонко настроенных моделей

## Применение

vLLM идеально подходит для:
- Облачных систем сервинга LLM
- Приложений с высокими требованиями к пропускной способности
- Систем, требующих эффективного использования GPU-ресурсов
- Производственных сред с различными архитектурами моделей

## Сравнение с альтернативами

vLLM конкурирует с TensorRT-LLM, FasterTransformer, SGLang, KTransformers и HuggingFace Transformers, предлагая баланс производительности, эффективности управления памятью и совместимости.

## Новые возможности

vLLM 0.11 представил режим сна (Sleep Mode), позволяющий эффективно переключаться между моделями "на горячую". Эта функция значительно ускоряет переключение между моделями (в 18-200 раз) и повышает производительность инференса после пробуждения (до 88%). Режим сна особенно полезен при работе с несколькими моделями, которые не помещаются одновременно в GPU-память.

## Связанные темы

- [[vllm_integration.md]] - Подробное руководство по интеграции vLLM
- [[vllm_inference_optimization.md]] - Технические детали оптимизаций инференса
- [[vllm_sleep_mode.md]] - Режим сна для эффективного переключения между моделями
- [[gpu_memory_management.md]] - Влияние PagedAttention на управление памятью
- [[flash_attention_and_grouped_mechanisms.md]] - Подробное описание FlashAttention
- [[distributed_inference.md]] - Распределённые подходы к инференсу