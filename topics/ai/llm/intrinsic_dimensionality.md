# Внутренняя размерность (Intrinsic Dimensionality) в обучении LLM

## Описание

Концепция внутренней размерности (intrinsic dimensionality) в контексте обучения больших языковых моделей описывает реальную размерность пространства, в котором фактически происходит обучение модели. Эта концепция объясняет, почему эволюционные стратегии могут успешно оптимизировать модели с миллиардами параметров даже с небольшой популяцией.

## Исследование "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning" (2020)

### Методология исследования

Исследователи брали различные языковые модели и дообучали их на задачах, но искусственно ограничивали размерность подпространства (аналогично подходу LoRA). Они перебирали размерность этого обучения и смотрели на качество, затем вычисляли так называемый d_90 - размерность "LoRA", необходимую для получения 90% от качества полной модели.

### Ключевые результаты

1. **Снижение d_90 во время претрейна**: В процессе предобучения d_90, если запускать из данного чекпоинта, уменьшается со временем
2. **Обратная зависимость от размера модели**: Чем больше модель, тем меньше d_90
3. **Эффективность в низкоразмерном пространстве**: После предобучки большой модели на большом датасете дообучать модель можно в пространстве очень маленькой размерности

### Практические выводы

- Обучение LLM на самом деле происходит в пространстве гораздо меньшей размерности, чем кажется
- Это объясняет, почему эволюционные стратегии с маленькой популяцией (например, 30) могут оптимизировать модели с миллиардами параметров
- Малое пространство параметров снижает риск переобучения
- Для эффективного обучения может потребоваться гораздо меньше данных

## Применения в оптимизации

### Эволюционные стратегии

- Позволяют оптимизировать LLM в низкоразмерном пространстве
- Объясняет успешность применения ES даже к очень большим моделям
- Позволяет использовать маленькие популяции для эффективной оптимизации

### Параметрически эффективное обучение

- Поддерживает эффективность таких методов, как LoRA, AdaLoRA, адаптеры
- Объясняет, почему небольшие дополнительные параметры могут значительно улучшить качество модели
- Позволяет оптимизировать только небольшие компоненты модели

### Претрейн-дообучение

- Помогает понять, как предобученные модели становятся более эффективными для дообучения
- Позволяет оптимизировать архитектуру претрейна для лучшей эффективности последующего дообучения
- Объясняет, почему большие модели чаще достигают хороших результатов при дообучении

## Теоретическая значимость

Концепция внутренней размерности объясняет парадокс эффективности относительно простых методов дообучения LLM и помогает понять, почему модели с миллиардами параметров можно эффективно адаптировать к конкретным задачам через оптимизацию лишь небольшого подмножества параметров.

## Связи с другими темами

- [[ai/llm/evolution_strategies_optimization.md]] - Применение эволюционных стратегий к LLM
- [[ai/llm/memory_efficient_training.md]] - Экономные методы обучения, использующие низкоразмерные пространства
- [[ai/llm/lora_optimization.md]] - Low-Rank Adaptation как конкретный пример эффективного использования низкоразмерных подпространств
- [[ai/optimization/dimensionality_reduction.md]] - Общие методы снижения размерности в машинном обучении