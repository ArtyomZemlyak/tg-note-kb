# Дистилляция знаний (Knowledge Distillation)

## Определение

Дистилляция знаний — это метод машинного обучения, при котором информация, содержащаяся в большой, часто предварительно обученной модели (учитель), передается в меньшую модель (студент). Цель этого процесса — создать компактную модель, которая сохраняет высокую производительность оригинальной модели.

## Основные компоненты

- **Учитель (Teacher Model)**: Большая, часто предварительно обученная модель, обладающая высокой производительностью.
- **Студент (Student Model)**: Меньшая модель, которая обучается копировать поведение учителя.
- **Температура (Temperature)**: Параметр, который регулирует "мягкость" вероятностного распределения, полученного через функцию softmax.

## Традиционная дистилляция знаний

В оригинальной работе Хинтона и др. [1] дистилляция знаний реализована при обучении небольшой модели (студента) воспроизводить логиты большой модели (учителя). Процесс включает:

1. Обучение учителя на исходной задаче
2. Генерацию "мягких" целей от учителя
3. Обучение студента воспроизводить как мягкие, так и "жесткие" (настоящие) цели

## Проблемы традиционной дистилляции

### Exposure Bias

В традиционной дистилляции модель-студент во время обучения видит разное распределение, чем на инференсе. На тренировке она получает токены из "истинного" распределения (или от учителя), но на инференсе генерирует токены сама, что может привести к накоплению ошибок.

### Мисалигнмент токенизаторов

Классические методы дистилляции требуют одинаковых словарей токенизатора у учителя и студента, что ограничивает гибкость при работе с разными архитектурами моделей.

### Проблема сильного учителя

Большой и сильный учитель не обязательно хорошо сдистиллируется в маленького и слабого студента из-за разницы в способности моделировать распределения.

## Решения проблем

### On-Policy дистилляция

Решает проблему exposure bias за счет обучения модели на основе собственных роллаутов, оцененных учителем. Модель видит одно и то же распределение как на обучении, так и на инференсе.

### Кросс-токенизаторная дистилляция

Методы, такие как Universal Logit Distillation (ULD) и General On-Policy Logit Distillation (GOLD), позволяют дистиллировать знания между моделями с разными токенизаторами, используя расстояние Вассерштейна и другие метрики для сравнения распределений.

## Применение

- Уменьшение размера модели для ресурсоемких приложений
- Ускорение инференса
- Создание более эффективных моделей для мобильных и встроенных устройств
- Применение в современных линейках моделей: Qwen-3, Gemma-2, llama-4

## Варианты дистилляции

- **Hard-label дистилляция**: Обучение только на предсказанных токенах (наименее информативно)
- **Soft-label дистилляция**: Обучение на полных распределениях (наиболее информативно)
- **On-policy дистилляция**: Обучение на основе роллаутов с оценкой учителя
- **Off-policy дистилляция**: Обучение на предварительно подготовленных данных

## Связи с другими темами

- [[on_policy_distillation.md]] - метод дистилляции, устраняющий exposure bias
- [[gold_method.md]] - усовершенствованный метод кросс-токенизаторной дистилляции
- [[universal_logit_distillation.md]] - метод для дистилляции с разными токенизаторами
- [[distillation_challenges.md]] - основные проблемы традиционной дистилляции
- [[optimization/techniques_for_small_models.md]] - другие методы создания маленьких моделей
- [[optimization/structured_pruning.md]] - альтернативный подход к оптимизации моделей
- [[optimization/smol_training_playbook.md]] - общий подход к обучению маленьких моделей
- [[reinforcement_learning_in_llms.md]] - концепции on-policy и off-policy, применимые к дистилляции
- [[model_quantization_techniques.md]] - альтернативный метод оптимизации моделей
- [[compression/distillation_pruning_redistillation.md]] - Комплексный трехэтапный подход к сжатию LLM, начинающийся с дистилляции

## Источники

1. [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) - оригинальная статья Хинтона о дистилляции знаний
2. [Generalized Knowledge Distillation](https://arxiv.org/abs/) - статья о комбинации on-policy и off-policy методов
3. [Universal Logit Distillation](https://arxiv.org/abs/) - метод кросс-токенизаторной дистилляции с использованием расстояния Вассерштейна
4. [Contextual Dynamical Mapping](https://arxiv.org/abs/) - метод выравнивания токенов у разных моделей
5. [GOLD: General On-Policy Logit Distillation](https://arxiv.org/abs/) - последний прорывной метод кросс-токенизаторной дистилляции
6. [Huggingface Blog Post on GOLD](https://huggingface.co/blog/gold) - объяснение метода GOLD и сравнение с другими подходами