# Выравнивание LLM (LLM Alignment)

## Краткое описание

Выравнивание LLM (Large Language Model Alignment) - это процесс обеспечения того, чтобы поведение и выводы больших языковых моделей соответствовали человеческим ценностям, инструкциям и ожиданиям. Это включает в себя обучение моделей быть полезными, честными и безвредными (Helpful, Honest, Harmless - HHH) при взаимодействии с пользователями.

## Контекст и проблема

Большие языковые модели, несмотря на свои впечатляющие способности к генерации текста и рассуждению, могут проявлять поведение, которое не соответствует человеческим ценностям или инструкциям. Это включает в себя создание вредного, неправдоподобного или предвзятого контента, а также непослушание явным инструкциям. Выравнивание направлено на смягчение этих проблем.

## Подходы к выравниванию

### 1. Обучение с учителем (Supervised Fine-Tuning, SFT)

Использование инструкций и человеческих ответов для обучения модели:
- Создание датасетов с инструкциями и эталонными ответами
- Обучение модели на этих примерах с помощью максимального правдоподобия
- Основа для последующего обучения с подкреплением

### 2. Обучение с подкреплением с человеческой обратной связью (RLHF)

Использование человеческих предпочтений для обучения вознаграждению:
- Сбор параллельных ответов от модели
- Ранжирование людьми по качеству и предпочтительности
- Обучение модели вознаграждению на основе этих предпочтений
- Использование RL (например, PPO) для оптимизации политики модели

### 3. Обучение с подкреплением с AI-обратной связью (RLAIF)

Использование предпочтений от AI-судей вместо людей:
- Использование более мощных моделей для оценки ответов
- Масштабируемая альтернатива RLHF
- Повышение консистентности оценок

### 4. Обучение с прямым предпочтением (DPO)

Напрямую обучает на предпочтениях без отдельной модели вознаграждения:
- Оптимизация по парам ответов с известными предпочтениями
- Более стабильная и эффективная альтернатива RLHF
- Прямая оптимизация разницы логитов

### 5. Обучение без эталонов (Reference-Free Learning)

Новые методы, такие как Compute as Teacher:
- Использование собственных выводов модели для создания обучающих сигналов
- Синтез эталонов из нескольких роллаутов (CaT)
- Устранение зависимости от человеческой разметки

## Меры безопасности

### Обнаружение опасных запросов
- Идентификация потенциально вредного контента в промптах
- Фильтрация запросов, направленных на обход безопасности
- Активация защитных механизмов при обнаружении риска

### Внутренние защитные механизмы
- Встраивание принципов безопасности в архитектуру модели
- Обучение на примерах отказа от вредных действий
- Развитие внутреннего понимания этических норм

### Внешние фильтры
- Постпроцессинг выводов для удаления нежелательного контента
- Проверка согласованности с этическими принципами
- Блокировка опасных генераций перед выдачей пользователю

## Методы оценки выравнивания

### Автоматические метрики
- Безопасность (предотвращение вреда)
- Помощь (полезность для пользователя)
- Честность (не предвзятость)
- Справедливость (равное обращение ко всем пользователям)

### Человеческая оценка
- Оценка реальными людьми взаимодействий с моделью
- Тестирование на различных сценариях использования
- Проверка соответствия ценностям разных сообществ

## Вызовы и ограничения

### Определение ценностей
- Сложность определения универсальных человеческих ценностей
- Культурные различия в моральных нормах
- Конфликты между разными аспектами выравнивания

### Уменьшение способностей
- Риск деградации первоначальных способностей модели
- Баланс между безопасностью и полезностью
- Снижение креативности или независимости мышления

### Обход защиты
- Возможность для пользователей обойти механизмы безопасности
- "Jailbreak" атаки, направленные на обход ограничений
- Необходимость постоянного обновления защитных мер

### Смещения и дискриминация
- Перенос предубеждений из обучающих данных
- Неосознанное усиление социальных стереотипов
- Неравное обращение с разными группами пользователей

## Современные подходы

### Compute as Teacher (CaT)
- Использует синтез вместо отбора для создания эталонов
- Позволяет модели самой улучшать свои ответы
- Обучение без эталонных данных от человека

### LASER (Last-Token Self-Rewarding)
- Метод самонаграждения для улучшения рассуждений
- Согласование внутренних оценок с желаемым поведением

### Self-Proposed Rubrics
- Модель генерирует свои собственные критерии оценки
- Подходит для субъективных задач, где нет объективных эталонов

## Будущие направления

### Автоматическое определение ценностей
- Извлечение ценностей из текстов, норм и законов
- Адаптация к культурным особенностям пользователей
- Персонализированное выравнивание для разных пользователей

### Теоретические основы
- Формализация концепций этичности и выравнивания
- Понимание взаимосвязи между способностями и выравниванием
- Математическое моделирование человеческих ценностей

### Многоагентное выравнивание
- Использование нескольких моделей для проверки друг друга
- Консенсусные методы для улучшения согласованности
- Децентрализованные подходы к определению ценностей

## Связи с другими темами

- [[compute_as_teacher.md]] - Современный метод выравнивания без эталонов
- [[reinforcement_learning_from_human_feedback.md]] - Традиционный подход к выравниванию
- [[rlaif.md]] - Использование AI-судей вместо людей
- [[dpo_direct_preference_optimization.md]] - Прямая оптимизация по предпочтениям
- [[reference_free_learning.md]] - Обучение без эталонных данных
- [[llm_safety.md]] - Безопасность LLM, часть выравнивания
- [[self_proposed_rubrics.md]] - Метод для выравнивания в субъективных задачах

## Ссылки на источники

- Обзор методов выравнивания LLM
- Compute as Teacher: https://arxiv.org/abs/2509.14234
- Руководство по безопасному выравниванию LLM
- Документы по этике ИИ и выравниванию