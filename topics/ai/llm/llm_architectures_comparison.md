# Сравнение архитектур LLM

## Обзор

Современные большие языковые модели (LLM) используют различные архитектурные подходы, каждый из которых имеет свои преимущества и ограничения. В этом обзоре рассматриваются основные архитектуры LLM, их различия и области применения.

## Основные архитектуры трансформеров

### 1. Encoder-only архитектура (BERT-подобные)

- **Структура**: Использует только компоненты энкодера (без декодера)
- **Самовнимание**: Всестороннее внимание (без каузального маскирования)
- **Обучение**: Маскированное языковое моделирование - предсказывает случайно замаскированные токены
- **Процесс**: Двунаправленная обработка контекста
- **Область применения**: Классификация текста, эмбеддинги, обучение представлениям
- **Пример**: BERT (Bidirectional Encoder Representations from Transformers)
- **Преимущества**: Лучше для задач понимания (классификация, ответы на вопросы)
- **Ограничения**: Не может генерировать текст автoregressive

### 2. Decoder-only архитектура (GPT-подобные)

- **Структура**: Использует только компоненты декодера (но без cross-attention)
- **Обучение**: Автoregressive языковое моделирование - предсказывает следующий токен на основе предыдущих токенов
- **Маскирование**: Каузальное маскирование предотвращает внимание к будущим токенам
- **Область применения**: Генерация текста, выполнение инструкций
- **Примеры**: GPT-3, GPT-3.5, GPT-4, Chinchilla
- **Преимущества**: Оптимизированы для задач генерации текста, могут автoregressive генерировать продолжения
- **Ограничения**: Однонаправленная обработка - каждый токен может обращаться только к предыдущим токенам

### 3. Encoder-decoder архитектура (T5-подобные)

- **Структура**: Содержит как компоненты энкодера, так и декодера
- **Энкодер**: Обрабатывает входную последовательность с самовниманием (все-ко-всем, без каузального маскирования)
- **Декодер**: Содержит три компонента - каузально замаскированное самовнимание, cross-attention и feedforward сеть
- **Cross-attention**: Обращается к выходам энкодера, извлекает информацию из энкодирований
- **Область применения**: Машинный перевод, генерация текста в текст
- **Примеры**: Оригинальная модель Transformer, T5, BART
- **Преимущества**: Может обрабатывать как задачи генерации, так и задачи понимания

## Mixture of Experts (MoE) архитектуры

### Основные концепции

Mixture of Experts (MoE) - это класс архитектур нейронных сетей, которые заменяют плотные слои полносвязной сети (FFN) разреженными слоями MoE, содержащими несколько "экспертов" (нейронных сетей) и сеть-шлюз (gate network), которая определяет, какие токены направляются каким экспертам.

### Компоненты

- **Разреженные слои MoE**: Вместо плотных FFN слоев содержат несколько "экспертов" (обычно 8), где каждый эксперт - это нейронная сеть (обычно FFN)
- **Сеть-шлюз/маршрутизатор**: Определяет, какие токены направляются каким экспертам, с обучаемыми параметрами, обучаемыми одновременно с остальной сетью

### История развития

- **1991**: "Adaptive Mixture of Local Experts" - основополагающая статья, вводящая концепцию
- **2017**: Shazeer и др. (с Geoffrey Hinton и Jeff Dean) масштабировали MoE до 137B LSTM для перевода, вводя разреженность
- **GShard (2020)**: Работа Google по масштабированию трансформеров за пределы 600 миллиардов параметров с использованием MoE
- **Switch Transformers (2022)**: Упрощение и стабилизация обучения MoE с существенными улучшениями

### Ключевые варианты

#### GShard
- Заменяет каждый второй FFN слой слоем MoE с top-2 гейтингом как в энкодере, так и в декодере
- Использует случайную маршрутизацию и ограничения на емкость экспертов
- Позволяет масштабирование на несколько устройств, при этом слой MoE разделяется между устройствами, а другие слои реплицируются
- Вводит "емкость эксперта" - порог того, сколько токенов может обрабатывать один эксперт

#### Switch Transformers
- Обеспечили 4x ускорение предварительного обучения по сравнению с T5-XXL
- Используют упрощенную стратегию одного эксперта (в отличие от начального требования как минимум двух экспертов)
- Ввели концепции:
  - **Расчет емкости эксперта**: `(токены на батч / количество экспертов) × коэффициент емкости`
  - **Селективная точность**: Обучение экспертов с bfloat16, при этом использование полной точности для маршрутизации для стабилизации обучения

#### Другие варианты MoE
- **GLaM**: Декодерные MoE модели, которые соответствовали качеству GPT-3, используя 1/3 энергии
- **Mixtral 8x7B**: Открытая MoE модель, превосходящая Llama 2 70B
- **NLLB MoE**: MoE вариант перевода NLLB от Meta

### Преимущества MoE
- **Предварительное обучение**: Значительно более быстрое предварительное обучение с тем же вычислительным бюджетом
- **Инференс**: Более быстрый инференс по сравнению с плотными моделями с тем же количеством параметров
- **Эффективность**: Только некоторые параметры активируются во время инференса (разреженные вычисления)

## Специализированные механизмы внимания

### Multi-Query Attention (MQA) и Grouped-Query Attention (GQA)
- **MQA**: Использует один общий W^K, W^V для всех голов внимания вместо отдельных матриц для каждой головы
- **GQA**: Разбивает головы внимания на группы, каждая из которых делит ключевые-значение
- **Эффект**: Нейтральный для качества модели и скорости обучения, но увеличивает скорость инференса
- **Снижает требования к KV кэшу** для автoregressive генерации

### Multihead Latent Attention (MLA)
- Использует низкоранговое приближение стандартного Multihead Attention (MHA)
- Проектирует скрытые векторы в низкоразмерные "латентные пространства" перед вниманием
- Минимизирует KV кэш, кешируя только низкоразмерные KV векторы

### DeepSeek Sparse Attention (DSA)
- Новая технология разреженного внимания, реализованная в модели DeepSeek-V3.2-Exp
- Позволяет достичь тонко настраиваемого разреженного внимания
- Обеспечивает значительные улучшения в эффективности обучения и вывода в сценариях длинных контекстов при сохранении практически идентичного качества вывода модели
- Сфокусирована на разреженности в вычислениях внимания, что позволяет достичь дополнительной вычислительной эффективности

### Log-Linear Attention
- Новая архитектура механизма внимания, разработанная для повышения эффективности вычислений в моделях машинного обучения
- Промежуточный вариант между стандартным Attention и линейными по длине альтернативами
- Токены разбиваются на корзинки с экспоненциально растущим числом токенов, где свежие токены получают больший вес
- Сложность: O(n log n), что позволяет достичь лучшего баланса между эффективностью и способностью модели обрабатывать длинные контексты
- Может применяться как надстройка над Linear Attention, Mamba-2 и DeltaNet
- Использует структурированные матрицы HODLR и деревья Фенвика для эффективной реализации
- Стремится улучшить вычислительную эффективность, уменьшая сложность с O(n²) до O(n log n), в отличие от стандартного механизма внимания, который требует квадратичного времени для вычисления всех парных взаимодействий между токенами
- Предположительно обеспечивает качество, сравнимое с традиционными механизмами внимания

## Технические различия между архитектурами

### Основные различия

1. **Шаблоны внимания**: Каузальное (GPT), двунаправленное (BERT), или кросс-внимание (энкодер-декодер)
2. **Обучающие цели**: Маскированное предсказание, автoregressive генерация или префиксная генерация
3. **Архитектурные выборы**: Использование только энкодера, только декодера или обоих
4. **Эффективность параметров**: Техники, такие как MQA, GQA и MoE, для вычислительной эффективности

## Выбор архитектуры

Выбор конкретной архитектуры зависит от:
- **Типа задачи** (генерация, понимание, преобразование текста в текст)
- **Вычислительных ограничений** (параметры, вычислительные ресурсы)
- **Требований к эффективности** (пропускная способность, задержка)
- **Доступных данных для обучения** и целей развертывания

## Связи с другими темами

- [[models/nanoGPT.md]] - Пример реализации GPT-подобной архитектуры
- [[models/deepseek_sparse_attention.md]] - Пример специализированного механизма внимания
- [[log_linear_attention.md]] - Подробное описание Log-Linear Attention
- [[llm_memory_systems/llm_memory_overview.md]] - Основы механизмов внимания в нейронных сетях
- [[../../nlp/transformers/transformer_architecture.md]] - Подробное описание архитектуры трансформеров
- [[../../nlp/transformers/evolution_of_nlp_methods.md]] - Эволюция архитектур NLP

## Источники

- "Attention is All You Need" - основополагающая статья о трансформерах
- Исследования по MoE архитектурам от Google, OpenAI и других
- Статья "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention" (arXiv:2502.11089)