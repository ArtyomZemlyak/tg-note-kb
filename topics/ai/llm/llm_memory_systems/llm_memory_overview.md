# Системы памяти для LLM: обзор

## Введение

Системы памяти для Large Language Models (LLM) представляют собой критическую область исследований и разработок в сфере искусственного интеллекта. Они призваны решить ограничения традиционных языковых моделей, которые не сохраняют информацию между сессиями и имеют ограниченные контекстные окна.

## Типы систем памяти

### 1. Внутренняя память (встроенная в модель)
- Основана на механизмах внимания (attention mechanisms)
- Использует контекстное окно модели для хранения информации
- Примеры: GPT-2 с окном 1k токенов, Gemini 1.5 с окном до 1 миллиона токенов

### 2. Внешняя память
- Использует внешние системы хранения (векторные базы данных, графы знаний)
- Расширенная память за пределами контекстного окна модели
- Реализация через RAG (Retrieval-Augmented Generation)

### 3. Гибридная память
- Комбинирует внутреннюю и внешнюю память
- Оптимизация между производительностью и объемом хранимой информации

## Подходы к реализации памяти

### 1. Retrievaл-Augmented Generation (RAG)
- Интеграция LLM с системами поиска документов
- Кодирование запросов и документов в векторы
- Поиск документов с векторами, наиболее похожими на вектор запроса
- Генерация вывода на основе запроса и контекста из извлеченных документов

### 2. Использование инструментов и внешняя память
- Возможность LLM взаимодействовать с внешними системами или источниками данных
- Отдельная программа отслеживает поток вывода LLM для специального синтаксиса вызова инструментов
- Обратная интеграция вывода инструментов в поток ввода LLM

### 3. Интеграция памяти в AI-агентах
- Память может быть интегрирована как инструмент или как дополнительный ввод при преобразовании LLM в агентов
- ReAct: комбинирует рассуждение и действие с LLM в качестве планировщика, поддерживая записи действий и наблюдений
- Reflexion: создает агентов, которые учатся на протяжении нескольких эпизодов, сохраняя "извлеченные уроки" как долгосрочную память

### 4. Обучение в контексте (In-Context Learning)
- LLM приобретают предиктивную силу во время обучения
- Могут учиться на примерах, предоставленных в подсказке, без дополнительного обучения
- Производительность улучшается при включении примеров желаемого поведения в подсказку

## Вызовы и ограничения

### 1. Ограничения контекстного окна
- Традиционные модели имеют фиксированные контекстные окна, ограничивающие объем информации, который может быть обработан одновременно
- Обработка более длинных последовательностей требует больше вычислительных ресурсов

### 2. Проблема запоминания vs понимания
- LLM могут запоминать обучающие данные, что вызывает опасения относительно авторских прав и конфиденциальности
- Модели могут генерировать текст, который выглядит достоверно, но не соответствует обучающим данным

### 3. Фактическая согласованность
- Галлюцинации: модели генерируют беглый текст, который выглядит достоверно, но внутренне несогласован или фактически неверен
- Сложность обеспечения точности и надежности генерируемого контента

### 4. Вычислительные требования
- Поддержка и доступ к большим контекстам или внешним системам памяти требует значительных вычислительных ресурсов
- Масштабирование систем памяти увеличивает затраты на обучение и инференс

## Архитектуры и реализации

### 1. Transformer Architecture
- Большинство LLM используют архитектуру transformer с механизмами внимания для обработки текстовых последовательностей
- Несколько голов внимания обрабатывают различные типы релевантности для каждого токена

### 2. Mixture of Experts (MoE)
- Использует несколько специализированных нейронных сетей, работающих вместе, с механизмом переключения для маршрутизации входов
- Уменьшает затраты на инференс за счет использования только доли параметров для каждого ввода

### 3. Техники квантования
- Снижение требований к пространству за счет снижения точности параметров модели при сохранении производительности
- Статическое (фаза калибровки) и динамическое (применяется во время инференса) квантование

## Современные разработки

Одним из самых актуальных подходов является GigaMemory - архитектура долгосрочной персональной памяти для LLM, представленная в конкурсе AI Journey Contest 2025. Эта архитектура направлена на создание системы, которая хранит, обновляет и надежно извлекает знания о конкретном пользователе (привычки, предпочтения, ограничения, факты).

## Связи с другими темами

- [[../memory_architectures/gigamemory_architecture.md]] - архитектура GigaMemory
- [[../ai_contests/a_ij_contest/aij_contest_2025.md]] - конкурс AIJ 2025, в котором представлен GigaMemory
- [[llm_long_term_memory.md]] - долгосрочная память для LLM
- [[../../nlp/models/userlm_8b.md]] - модель симуляции диалога от Microsoft