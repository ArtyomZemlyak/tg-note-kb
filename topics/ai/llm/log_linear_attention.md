# Log-Linear Attention

## Общее описание

Log-Linear Attention - это новый механизм внимания, предлагающий промежуточный вариант между стандартным Attention (квадратичная сложность) и линейными по длине альтернативами (такими как Linear Attention, Mamba-1/2, DeltaNet). Основная идея заключается в логарифмической по памяти и времени операции, являющейся надстройкой над одним из линейных механизмов attention.

## Методология

### Проблема традиционного внимания

С момента появления механизма внимания (attention) было предложено множество альтернатив с субквадратичной сложностью. Убрав softmax из Attention, операцию можно посчитать за линейное по длине последовательности число операций. Аналогично, структурированные модели пространства состояний (SSM - S4, Mamba-1/2) и DeltaNet линейно масштабируются с ростом числа токенов.

Несмотря на успехи на отдельных задачах, ни одна из этих архитектур не смогла вытеснить трансформеры с пьедестала. Основная проблема заключается в попытке запихнуть весь контекст в скрытое состояние фиксированного размера, что фундаментально ограничивает модель в возможности знать все в длинном контексте.

### Решение Log-Linear Attention

Log-Linear Attention предлагает промежуточный вариант: токены разбиваются на корзинки с экспоненциально растущим числом токенов. Поскольку самые свежие токены обычно важнее для предсказания следующего, в одной корзине меньше токенов, и, соответственно, их вес больше. С отдалением от текущей позиции размер корзинок растет, а вклад индивидуальных токенов убывает.

### Алгоритм работы

1. Токены разбиваются на корзинки с экспоненциально растущим размером
2. Вычисляется линейный attention по корзинкам
3. Результаты каждой корзинки суммируются с некоторыми обучаемыми коэффициентами
4. Коэффициенты предсказывает отдельная MLP (многослойный перцептрон)
5. Число корзинок растет логарифмически с длиной последовательности

В результате получается сложность O(L log L), где L - длина последовательности. Для эффективной реализации используются деревья Фенвика.

## Математическая основа

Log-Linear Attention можно представить в виде структурированной матрицы HODLR (Hierarchically Off-Diagonal Low-Rank), где:
- Диагональные блоки нижнетреугольные
- Внедиагональная часть состоит из блоков ранга-1
- Размер блока растет с удалением от диагонали

## Применение

Log-Linear Attention можно применить как поверх Linear Attention, так и Mamba-2 и DeltaNet. Для всех этих архитектур написаны соответствующие CUDA кернелы, что делает реализацию эффективной.

## Экспериментальные результаты

### Синтетические задачи

На синтетических задачах Log-Linear модификация значительно улучшает качество DeltaNet на MQAR (извлечение нескольких элементов из контекста).

### Реальные задачи

Авторы обучили модели сравнимого размера (700-800M параметров, 50B токенов из Long-Data-Collections с длиной последовательности 16k):
- Transformer (базовая модель)
- DeltaNet
- Mamba-2
- Эти же модели с Log-Linear надстройкой

Log-Linear дал небольшой прирост поверх DeltaNet и Mamba-2.

### Сравнение по скорости

По скорости инференса на длинных контекстах:
- Log-Linear Mamba-2 медленнее Mamba-2 (в ~2 раза на 64k/128k токенах)
- Log-Linear Mamba-2 быстрее стандартного Attention

### Тестирование на различных задачах

- На тесте Needle-in-Haystack (поиск одного токена в длинном контексте) Log-Linear показал хорошие результаты
- В multi-key/multi-value задачах Log-Linear лучше линейных бейзлайнов, но хуже Attention
- На LongBench результаты варьируются - в одних случаях даёт прирост, в других - нет

## Оценка подхода

Авторы честно оценивают результаты и не утверждают, что предложенная модификация превосходит все существующие. В математическом плане подход красив и элегантен. В целом, Log-Linear Attention выглядит как неплохой промежуточный вариант между Attention и линейными по длине альтернативами, но требует валидации на бюджетах и размерах моделей, ближе к production-grade.

## Сравнение с другими архитектурами

| Архитектура | Сложность | Память | Особенности | 
|-------------|-----------|--------|-------------|
| Стандартное Attention | O(n²) | O(n²) | Полное взаимодействие между всеми токенами |
| Linear Attention | O(n) | O(n) | Приближенные вычисления |
| Mamba-1/2 | O(n) | O(n) | Модель пространства состояний |
| DeltaNet | O(n) | O(n) | Линейная рекуррентная архитектура |
| Log-Linear Attention | O(n log n) | O(n log n) | Надстройка над линейными механизмами, корзинки токенов |

## Использование в моделях

- Log-Linear Attention может использоваться как улучшение для существующих линейных архитектур
- Позволяет достичь лучшего баланса между эффективностью и способностью модели обрабатывать длинные контексты

## Преимущества

1. **Промежуточная сложность**: O(L log L) позволяет лучше масштабироваться, чем квадратичные архитектуры, но при этом сохраняет больше информации, чем линейные
2. **Гибкость**: Может применяться поверх различных линейных архитектур (Linear Attention, Mamba-2, DeltaNet)
3. **Математическая основа**: Использует структурированные матрицы HODLR, что делает реализацию эффективной
4. **Эмпирическое улучшение**: Показывает улучшение качества на некоторых задачах

## Недостатки

1. **Снижение скорости**: По сравнению с базовыми линейными архитектурами, Log-Linear Attention медленнее (в ~2 раза на 64k/128k токенах)
2. **Не является универсальным решением**: Не превосходит Attention во всех задачах
3. **Требует больше ресурсов**: Сложнее, чем линейные архитектуры, но эффективнее, чем квадратичные

## Связи с другими темами

- [[specialized_attention_mechanisms.md]] - Другие специализированные механизмы внимания, включая Linear Attention
- [[architectures/models/qwen3_next.md]] - Модель с DeltaNet архитектурой
- [[llm_architectures_comparison.md]] - Сравнение различных архитектур LLM
- [[../../nlp/transformers/transformer_architecture.md]] - Базовая архитектура трансформеров, на которой основано внимание

## Источники

- Оригинальная статья/работа, описывающая Log-Linear Attention
- Экспериментальные данные и сравнительный анализ с другими архитектурами