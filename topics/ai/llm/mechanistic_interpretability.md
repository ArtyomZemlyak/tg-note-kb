# Механистическая интерпретируемость

## Общее описание

Механистическая интерпретируемость (mechanistic interpretability) - это подход в области искусственного интеллекта, направленный на понимание внутренней работы нейронных сетей через анализ их механистических свойств. В отличие от традиционной интерпретируемости, которая фокусируется на объяснении вывода модели, механистическая интерпретируемость стремится понять, как именно модель производит вычисления.

## Основные принципы

Механистическая интерпретируемость основывается на идее, что чтобы по-настоящему понять поведение нейронной сети, нужно понять внутренние вычисления, которые происходят в модели. Это включает:

- Идентификацию конкретных нейронов или направлений в пространстве активаций, которые кодируют определенные понятия
- Картирование влияния этих нейронов на конечный вывод модели
- Понимание вычислительных цепочек (circuits), по которым информация проходит через сеть

## Основные методы

### 1. Дезагрегация нейронов (Neuron Dissection)

Метод, при котором отдельные нейроны или активации в модели связываются с определенными понятиями или признаками.

### 2. Анализ резидуальных связей (Residual Stream Analysis)

Изучение информации, проходящей через резидуальные соединения в трансформерах, для понимания как информация трансформируется на каждом слое.

### 3. Интервенции (Interventions)

Прямое вмешательство в активации модели, чтобы понять причинно-следственные связи между внутренними состояниями и выводами.

### 4. Обнаружение схем (Circuit Discovery)

Идентификация вычислительных подсетей (схем) внутри модели, которые отвечают за определенные аспекты поведения.

### 5. Линейные зонды (Linear Probes)

Использование линейных классификаторов, обученных на внутренних представлениях модели, для понимания, какая информация содержится на разных уровнях.

## Применение к большим языковым моделям

В контексте LLM механистическая интерпретируемость включает:

- Анализ внимания и как определенные механизмы внимания влияют на генерацию
- Понимание как и где модель хранит и извлекает знания
- Изучение рассуждающих цепочек внутри модели
- Обнаружение "счетчиков" (factored representations) информации

## Связь с гипотезой линейности

Механистическая интерпретируемость тесно связана с [гипотезой линейности](../theory/linearity_hypothesis.md), поскольку линейные представления облегчают анализ внутренних состояний модели. Понимание того, как концепты представлены как направления в пространстве активаций, позволяет более точно картировать вычислительные схемы.

## Примеры исследований

- Исследования Transformer Circuits
- Интерпретация внимания в GPT моделях
- Проекты по изучению конкретных навыков (capabilities) и знаний в LLM
- Анализ обучения через градиенты и изменение параметров

## Ограничения и вызовы

- **Высокая размерность**: Современные модели имеют миллиарды параметров, что затрудняет анализ
- **Эмерджентность**: Многие свойства моделей возникают на макроуровне из взаимодействия множества компонентов
- **Масштаб вычислений**: Анализ внутренней работы требует значительных вычислительных ресурсов

## Связи с другими темами

- [[../theory/linearity_hypothesis.md|Гипотеза линейности]] - Обе темы исследуют внутренние представления в нейронных сетях
- [[activation_engineering|Инжиниринг активаций]] - Практическое применение знаний об активациях
- [[introspection_research|Исследование интроспекции моделей]] - Соседняя область, изучающая внутреннее "мышление" моделей

## Источники и ссылки

- Исследования от Anthropic, OpenAI и других организаций в области интерпретируемости
- Работы по Transformer Circuits
- Современные исследования в области понимания внутреннего представления знаний в LLM