# Системы памяти для LLM: обзор

## Введение

Системы памяти для Large Language Models (LLM) представляют собой критическую область исследований и разработок в сфере искусственного интеллекта. Они призваны решить ограничения традиционных языковых моделей, которые не сохраняют информацию между сессиями и имеют ограниченные контекстные окна.

## Типы систем памяти

### 1. Внутренняя память (встроенная в модель)
- Основана на механизмах внимания (attention mechanisms)
- Использует контекстное окно модели для хранения информации
- Примеры: GPT-2 с окном 1k токенов, Gemini 1.5 с окном до 1 миллиона токенов

### 2. Внешняя память
- Использует внешние системы хранения (векторные базы данных, графы знаний)
- Расширенная память за пределами контекстного окна модели
- Реализация через RAG (Retrieval-Augmented Generation)

### 3. Гибридная память
- Комбинирует внутреннюю и внешнюю память
- Оптимизация между производительностью и объемом хранимой информации

## Подходы к реализации памяти

### 1. Retrievaл-Augmented Generation (RAG)
- Интеграция LLM с системами поиска документов
- Кодирование запросов и документов в векторы
- Поиск документов с векторами, наиболее похожими на вектор запроса
- Генерация вывода на основе запроса и контекста из извлеченных документов

### 2. Использование инструментов и внешняя память
- Возможность LLM взаимодействовать с внешними системами или источниками данных
- Отдельная программа отслеживает поток вывода LLM для специального синтаксиса вызова инструментов
- Обратная интеграция вывода инструментов в поток ввода LLM

### 3. Интеграция памяти в AI-агентах
- Память может быть интегрирована как инструмент или как дополнительный ввод при преобразовании LLM в агентов
- ReAct: комбинирует рассуждение и действие с LLM в качестве планировщика, поддерживая записи действий и наблюдений
- Reflexion: создает агентов, которые учатся на протяжении нескольких эпизодов, сохраняя "извлеченные уроки" как долгосрочную память

### 4. Обучение в контексте (In-Context Learning)
- LLM приобретают предиктивную силу во время обучения
- Могут учиться на примерах, предоставленных в подсказке, без дополнительного обучения
- Производительность улучшается при включении примеров желаемого поведения в подсказку

## Вызовы и ограничения

### 1. Ограничения контекстного окна
- Традиционные модели имеют фиксированные контекстные окна, ограничивающие объем информации, который может быть обработан одновременно
- Обработка более длинных последовательностей требует больше вычислительных ресурсов

### 2. Проблема запоминания vs понимания
- LLM могут запоминать обучающие данные, что вызывает опасения относительно авторских прав и конфиденциальности
- Модели могут генерировать текст, который выглядит достоверно, но не соответствует обучающим данным

### 3. Фактическая согласованность
- Галлюцинации: модели генерируют беглый текст, который выглядит достоверно, но внутренне несогласован или фактически неверен
- Сложность обеспечения точности и надежности генерируемого контента

### 4. Вычислительные требования
- Поддержка и доступ к большим контекстам или внешним системам памяти требует значительных вычислительных ресурсов
- Масштабирование систем памяти увеличивает затраты на обучение и инференс

## Архитектуры и реализации

### 1. Transformer Architecture
- Большинство LLM используют архитектуру transformer с механизмами внимания для обработки текстовых последовательностей
- Несколько голов внимания обрабатывают различные типы релевантности для каждого токена
- [[../../nlp/transformers/transformer_architecture.md]] - подробное описание архитектуры трансформеров
- [[../llm_architectures_comparison.md]] - сравнение различных архитектур LLM (encoder-only, decoder-only, encoder-decoder, MoE и др.)

### 2. Mixture of Experts (MoE)
- Использует несколько специализированных нейронных сетей, работающих вместе, с механизмом переключения для маршрутизации входов
- Уменьшает затраты на инференс за счет использования только доли параметров для каждого ввода
- [[../mixture_of_experts_architecture.md]] - подробное описание MoE архитектур, их компонентов и реализаций

### 3. Техники квантования
- Снижение требований к пространству за счет снижения точности параметров модели при сохранении производительности
- Статическое (фаза калибровки) и динамическое (применяется во время инференса) квантование

## Современные разработки

Одним из самых актуальных подходов является GigaMemory - архитектура долгосрочной персональной памяти для LLM, представленная в конкурсе AI Journey Contest 2025. Эта архитектура направлена на создание системы, которая хранит, обновляет и надежно извлекает знания о конкретном пользователе (привычки, предпочтения, ограничения, факты).

Другой важной областью являются дополненные памятью трансформеры (Memory-Augmented Transformers, MATs), которые интегрируют различные формы памяти в архитектуру трансформеров, вдохновленные нейронаучными принципами. Эти системы переходят от статических моделей к динамическим когнитивным системам, способным к пожизненному обучению. [[../../nlp/transformers/memory_augmented_transformers.md]] - подробное описание MATs.

Еще более продвинутый подход - вложенное обучение (Nested Learning) с его системой непрерывной памяти (CMS), которое реализует иерархию блоков памяти, обновляющихся в разных временных масштабах. Это позволяет модели одновременно хранить и обрабатывать информацию на нескольких уровнях временной абстракции, обеспечивая более надёжный механизм для непрерывного обучения и консолидации памяти.

Относительно новым направлением является архитектура Titans и её модуль долгосрочной памяти (LMM), который обучается адаптивно запоминать и забывать информацию, оптимизируя собственные параметры прямо во время инференса. В отличие от традиционных подходов, LMM функционирует как метамодель, способная к самообновлению. Архитектура представлена в статье "Titans: Learning to Memorize at Test Time" и включает три варианта интеграции памяти: MAC (Memory as Context), MAG (Memory as Gate), и MAL (Memory as Layer).

## Связи с другими темами

- [[../memory_architectures/gigamemory_architecture.md]] - архитектура GigaMemory
- [[../ai_contests/a_ij_contest/aij_contest_2025.md]] - конкурс AIJ 2025, в котором представлен GigaMemory
- [[../ai_contests/a_ij_contest/tracks/gigamemory_track.md]] - трек конкурса, посвященный GigaMemory
- [[llm_long_term_memory.md]] - долгосрочная память для LLM
- [[context_injection.md]] - автоматическая инъекция контекста в промпт LLM
- [[mcp_model_context_protocol.md]] - протокол контекста модели, альтернативный подход к управлению контекстом через клиент-серверную архитектуру
- [[../../nlp/models/userlm_8b.md]] - модель симуляции диалога от Microsoft
- [[../../nlp/transformers/memory_augmented_transformers.md]] - дополненные памятью трансформеры: систематический обзор от нейронаучных принципов до расширенных архитектур моделей
- [[../../nlp/transformers/mat_taxonomy.md]] - таксономия MAT, объединяющая функциональные цели, типы памяти и техники интеграции
- [[../../machine_learning/catastrophic_forgetting/catastrophic_forgetting.md]] - катаstroфическое забывание: проблема, связанная с сохранением знаний в нейронных сетях
- [[../../continual_learning/continuum_memory_system.md]] - Система непрерывной памяти из парадигмы вложенного обучения
- [[../../continual_learning/nested_learning.md]] - Парадигма вложенного обучения для непрерывного обучения
- [[../../rag/best_practices/overview.md]] - Лучшие практики RAG: Обзор подходов к расширенной генерации через извлечение