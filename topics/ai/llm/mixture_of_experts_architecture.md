# Mixture of Experts (MoE) архитектуры

## Общее описание

Mixture of Experts (MoE) - это архитектурная парадигма в глубоком обучении, при которой традиционные плотные нейронные сети заменяются разреженными сетями, содержащими несколько "экспертов" (специализированных подсетей). Вместо активации всех параметров модели при каждом вычислении только подмножество параметров активируется, что делает вычисления более эффективными.

## Основные компоненты

### Эксперты (Experts)
- Модульные нейронные сети (обычно полносвязные слои), которые специализируются на различных аспектах данных
- Каждый эксперт обучается обрабатывать определенные типы входных данных или задачи
- Обычно используется 8-64 эксперта в одной слое MoE

### Маршрутизатор (Router/Gate Network)
- Небольшая нейронная сеть, которая определяет, какие эксперты должны обрабатывать каждый конкретный вход
- Принимает решение на основе входных данных, определяя наиболее подходящих экспертов
- В процессе обучения маршрутизатор обучается направлять правильные входы к наиболее подходящим экспертам

## История развития

- **1991**: "Adaptive Mixture of Local Experts" - первоначальное введение концепции
- **2017**: Shazeer и др. масштабировали MoE до 137-миллиардной LSTM для задачи машинного перевода
- **GShard (2020)**: Google представила масштабируемый подход к MoE для трансформеров с более чем 600 миллиардами параметров
- **Switch Transformers (2022)**: Упрощение и стабилизация MoE, достижение 4x ускорения предварительного обучения

## Ключевые архитектуры MoE

### GShard
- Заменяет каждый второй FFN слой в трансформере слоем MoE с top-2 гейтингом
- Использует рандомизированную маршрутизацию и ограничения емкости эксперта
- Позволяет масштабировать модели до триллионов параметров

### Switch Transformers
- Использует упрощенный подход с одним выбранным экспертом (top-1) вместо нескольких (top-k)
- Применяет селективную точность: эксперты в bfloat16, маршрутизация в полной точности
- Демонстрирует значительное ускорение обучения при сохранении качества

### ST-MoE (Soft MoE)
- Использует softer routing, позволяя нескольким экспертам получать взвешенные вклады
- Обеспечивает лучшее распределение нагрузки между экспертами

## Технические детали

### Top-k Gating
- Вместо активации всех экспертов используется top-k выбор, где только k экспертов активируются для каждого входа
- Обычно используется top-1 или top-2 гейтинг
- Включает шум в маршрутизацию для стабильности обучения

### Load Balancing
- Важно для предотвращения ситуации, когда все токены направляются к одному и тому же эксперту
- Использует вспомогательные потери для поощрения равномерного использования всех экспертов
- Помогает избежать перегрузки отдельных экспертов

### Capacity Factor
- Определяет максимальную емкость каждого эксперта: `(токены на батч / количество экспертов) × коэффициент емкости`
- Обычно коэффициент емкости равен 1.0-2.0
- Если эксперт достигает своей емкости, дополнительные токены могут быть направлены через резидуальные соединения или отброшены

## Преимущества MoE

### Вычислительная эффективность
- **Только часть параметров активируется** во время инференса, значительно снижая вычислительные требования
- **Более быстрое обучение** по сравнению с плотными моделями с аналогичной вычислительной нагрузкой
- **Масштабируемость** - возможность создания моделей с триллионами параметров без пропорционального увеличения вычислительных затрат

### Качество модели
- **Специализация экспертов** позволяет лучше обрабатывать различные типы данных и задач
- **Эффективное разделение труда** между различными частями модели

## Проблемы и ограничения

### Memory требования
- Все эксперты должны быть загружены в память, несмотря на разреженность вычислений
- Высокие требования к VRAM

### Тонкая настройка (fine-tuning)
- Исторически были проблемы с обобщением и переобучением при тонкой настройке
- Требует специальных методов для эффективной адаптации

### Неравномерная маршрутизация
- Возможность ситуации, когда все токены направляются к нескольким экспертам
- Требует сложных методов балансировки нагрузки

## Современные примеры MoE моделей

- **GLaM (Google)**: Декодерная MoE модель, которая соответствовала качеству GPT-3 при использовании 1/3 энергии
- **Switch Transformers (Google)**: До 1.6 триллиона параметров с 2048 экспертами
- **Mixtral 8x7B (Mistral AI)**: Открытая MoE модель, превосходящая Llama 2 70B
- **DeepSeek-MoE**: Масштабная MoE модель с различными размерами экспертов

## Сравнение с плотными моделями

| Аспект | Плотные модели | MoE модели |
|--------|----------------|------------|
| Количество параметров | Все параметры активны | Только подмножество активно |
| Скорость обучения | Стандартная | Быстрее при одинаковых вычислениях |
| Скорость инференса | Одинаковая для всех входов | Зависит от маршрутизации |
| Память | Пропорционально параметрам | Высокая для хранения всех экспертов |
| Специализация | Общая для всех задач | Возможна специализация |

## Будущее направление развития

- **Улучшенные методы маршрутизации** для лучшей балансировки и специализации
- **Эффективные методы тонкой настройки** для MoE моделей
- **Адаптивные архитектуры**, динамически подстраивающие структуру экспертов
- **Сжатие и квантизация** для уменьшения требования к памяти

## Связи с другими темами

- [[llm_architectures_comparison.md]] - Общее сравнение архитектур LLM
- [[models/deepseek_v3_2_exp.md]] - Пример современной MoE модели

## Источники

- "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity" (2022)
- "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
- Исследования от Google, OpenAI, и других организаций по MoE архитектурам