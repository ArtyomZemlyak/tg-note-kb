# Mixture of Experts (MoE) архитектуры

## Общее описание

Mixture of Experts (MoE) - это архитектурная парадигма в глубоком обучении, при которой традиционные плотные нейронные сети заменяются разреженными сетями, содержащими несколько "экспертов" (специализированных подсетей). Вместо активации всех параметров модели при каждом вычислении только подмножество параметров активируется, что делает вычисления более эффективными.

### Место в обзоре "Speed Always Wins"

MoE является одним из семи ключевых направлений эффективных архитектур для LLM, описанных в обзоре "Speed Always Wins". В этом контексте MoE рассматривается как методика разреженности, применяемая не в механизме внимания, а в FFN-слоях (Feed-Forward Network), где для каждого токена активируется лишь небольшая часть экспертов, что позволяет наращивать число параметров без пропорционального роста вычислений.

## Основные компоненты

### Эксперты (Experts)
- Модульные нейронные сети (обычно полносвязные слои), которые специализируются на различных аспектах данных
- Каждый эксперт обучается обрабатывать определенные типы входных данных или задачи
- Обычно используется 8-64 эксперта в одной слое MoE

### Маршрутизатор (Router/Gate Network)
- Небольшая нейронная сеть, которая определяет, какие эксперты должны обрабатывать каждый конкретный вход
- Принимает решение на основе входных данных, определяя наиболее подходящих экспертов
- В процессе обучения маршрутизатор обучается направлять правильные входы к наиболее подходящим экспертам

## История развития

- **1991**: "Adaptive Mixture of Local Experts" - первоначальное введение концепции
- **2017**: Shazeer и др. масштабировали MoE до 137-миллиардной LSTM для задачи машинного перевода
- **GShard (2020)**: Google представила масштабируемый подход к MoE для трансформеров с более чем 600 миллиардами параметров
- **Switch Transformers (2022)**: Упрощение и стабилизация MoE, достижение 4x ускорения предварительного обучения

## Ключевые архитектуры MoE

### GShard
- Заменяет каждый второй FFN слой в трансформере слоем MoE с top-2 гейтингом
- Использует рандомизированную маршрутизацию и ограничения емкости эксперта
- Позволяет масштабировать модели до триллионов параметров

### Switch Transformers
- Использует упрощенный подход с одним выбранным экспертом (top-1) вместо нескольких (top-k)
- Применяет селективную точность: эксперты в bfloat16, маршрутизация в полной точности
- Демонстрирует значительное ускорение обучения при сохранении качества

### ST-MoE (Soft MoE)
- Использует softer routing, позволяя нескольким экспертам получать взвешенные вклады
- Обеспечивает лучшее распределение нагрузки между экспертами

## Технические детали

### Top-k Gating
- Вместо активации всех экспертов используется top-k выбор, где только k экспертов активируются для каждого входа
- Обычно используется top-1 или top-2 гейтинг
- Включает шум в маршрутизацию для стабильности обучения

### Load Balancing
- Важно для предотвращения ситуации, когда все токены направляются к одному и тому же эксперту
- Использует вспомогательные потери для поощрения равномерного использования всех экспертов
- Помогает избежать перегрузки отдельных экспертов

### Capacity Factor
- Определяет максимальную емкость каждого эксперта: `(токены на батч / количество экспертов) × коэффициент емкости`
- Обычно коэффициент емкости равен 1.0-2.0
- Если эксперт достигает своей емкости, дополнительные токены могут быть направлены через резидуальные соединения или отброшены

## Преимущества MoE

### Вычислительная эффективность
- **Только часть параметров активируется** во время инференса, значительно снижая вычислительные требования
- **Более быстрое обучение** по сравнению с плотными моделями с аналогичной вычислительной нагрузкой
- **Масштабируемость** - возможность создания моделей с триллионами параметров без пропорционального увеличения вычислительных затрат

### Качество модели
- **Специализация экспертов** позволяет лучше обрабатывать различные типы данных и задач
- **Эффективное разделение труда** между различными частями модели

## Проблемы и ограничения

### Memory требования
- Все эксперты должны быть загружены в память, несмотря на разреженность вычислений
- Высокие требования к VRAM

### Тонкая настройка (fine-tuning)
- Исторически были проблемы с обобщением и переобучением при тонкой настройке
- Требует специальных методов для эффективной адаптации

### Неравномерная маршрутизация
- Возможность ситуации, когда все токены направляются к нескольким экспертам
- Требует сложных методов балансировки нагрузки

### Сетевые ограничения при распределённом инференсе
- Классические архитектуры сталкиваются с проблемами масштабирования на кластерах GPU в облаке
- AWS EFA (Elastic Fabric Adapter) не поддерживает GPUDirect Async, что создает задержки в маршрутизации MoE более 1 мс
- Ранние решения с NVSHMEM-proxy не обеспечивали достаточной производительности для эффективного распределённого инференса MoE

## Современные примеры MoE моделей

- **GLaM (Google)**: Декодерная MoE модель, которая соответствовала качеству GPT-3 при использовании 1/3 энергии
- **Switch Transformers (Google)**: До 1.6 триллиона параметров с 2048 экспертами
- **Mixtral 8x7B (Mistral AI)**: Открытая MoE модель, превосходящая Llama 2 70B
- **DeepSeek-MoE**: Масштабная MoE модель с различными размерами экспертов
- **Cursor Composer**: Специализированная MoE модель для задач программирования с архитектурой MXFP8 и обучением с подкреплением
- **Kimi K2**: Триллионная MoE модель, инференс которой стал возможен на обычных AWS-кластерах благодаря новым методам оптимизации EFA

## Современные достижения в инференсе MoE

### Оптимизация для облачной инфраструктуры
Недавние исследования Perplexity показали, как можно эффективно запускать MoE модели с триллионами параметров на стандартных кластерах AWS, преодолевая ограничения EFA (Elastic Fabric Adapter):

- **Гибридное CPU-GPU взаимодействие**: Использование CPU для координации позволяет GPU синхронизироваться почти напрямую, обходя ограничения отсутствия GPUDirect Async
- **Специализированные ядра**: Созданы expert-parallel ядра для быстрого MoE-инференса на AWS EFA
- **RDMA-оптимизации**: Токены упаковываются в единичные RDMA-записи прямо с GPU, а специальные CPU-потоки запускают передачу и перекрывают её с вычислениями GEMM
- **Производительность**: MoE с 1T параметрами работает практически без деградации, многонодовый режим сопоставим или быстрее однонодового на 671B DeepSeek V3

Эти достижения открывают путь к сервингу триллионных моделей (например, Kimi K2) на обычных облачных инфраструктурах, а не только на специализированных суперкомпьютерах.

## Сравнение с плотными моделями

| Аспект | Плотные модели | MoE модели |
|--------|----------------|------------|
| Количество параметров | Все параметры активны | Только подмножество активно |
| Скорость обучения | Стандартная | Быстрее при одинаковых вычислениях |
| Скорость инференса | Одинаковая для всех входов | Зависит от маршрутизации |
| Память | Пропорционально параметрам | Высокая для хранения всех экспертов |
| Специализация | Общая для всех задач | Возможна специализация |

## Будущее направление развития

- **Улучшенные методы маршрутизации** для лучшей балансировки и специализации
- **Эффективные методы тонкой настройки** для MoE моделей
- **Адаптивные архитектуры**, динамически подстраивающие структуру экспертов
- **Сжатие и квантизация** для уменьшения требования к памяти

## Связи с другими темами

- [[llm_architectures_comparison.md]] - Общее сравнение архитектур LLM
- [[models/deepseek_v3_2_exp.md]] - Пример современной MoE модели
- [[models/multimodal/qwen3-omni.md]] - Пример использования MoE в мультимодальной модели Qwen3-Omni
- [[../../hardware/aws_efa_networking_for_ai.md]] - Оптимизация распределённого инференса MoE моделей с использованием AWS EFA
- [[distributed_inference.md]] - Распределённые подходы к инференсу MoE моделей
- [[tools/lplb_linear_programming_load_balancer.md]] - Инновационный балансировщик нагрузки для MoE моделей, использующий линейное программирование
- [[dynamic_moe_routing_hymba.md]] - Пример применения MoE для маршрутизации между SSM и MHA блоками в архитектуре Hymba

## Источники

- "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity" (2022)
- "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
- Исследования от Google, OpenAI, и других организаций по MoE архитектурам