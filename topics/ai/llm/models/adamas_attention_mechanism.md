# Adamas: Оптимизированный механизм внимания для LLM

## Общее описание

Adamas - новая технология, которая ускоряет механизм self-attention до 4.4 раз, сохраняя качество обработки длинных контекстов. Основная идея заключается в том, чтобы сделать внимание «разреженным» без потери смысла. В отличие от классического attention, где каждый токен сравнивается со всеми остальными, модель Adamas использует только 128 релевантных токенов для каждого запроса.

## Технические детали

### Основные принципы

1. **Преобразование Адамара**: К векторам запросов и ключей применяется преобразование Адамара, которое сглаживает экстремальные значения и позволяет их сжать.

2. **Компрессия**: Значения разбиваются на четыре уровня и кодируются всего в 2 бита. Эти компактные коды хранятся в кэше.

3. **Быстрое вычисление сходства**: С помощью лёгкой метрики (Manhattan distance) быстро вычисляется сходство между токенами.

4. **Выбор релевантных токенов**: Модель выбирает наиболее важные токены и выполняет обычное внимание только над ними.

### Преимущества

- **Значительное ускорение**: До 4.4× ускорения self-attention и около 1.5× ускорения инференса в целом
- **Сохранение качества**: Точность остаётся почти такой же, как у полного внимания
- **Эффективность памяти**: Практически не требует дополнительной памяти, лишь небольшой 2-битный код на токен
- **Простота интеграции**: Может встраиваться в существующие LLM без переобучения

## Сравнение с другими механизмами внимания

| Механизм | Сложность | Память | Скорость | Качество | Особенности |
|----------|-----------|--------|----------|----------|-------------|
| Стандартное Self-Attention | O(n²) | O(n²) | Низкая | Высокое | Традиционный подход |
| Sparse Attention | O(n) или O(n log n) | O(n) | Высокая | Зависит от шаблона | Ограниченные соединения |
| Linear Attention | O(n) | O(n) | Высокая | Ниже для сложных зависимостей | Приближенные вычисления |
| Log-Linear Attention | O(n log n) | O(n log n) | Высокая | Высокое | Баланс между точностью и эффективностью |
| Adamas | O(n) | O(n) | Очень высокая (4.4×) | Высокое | Разреженное внимание с компрессией и быстрым вычислением сходства |

## Применение

Технология Adamas может быть интегрирована в существующие Large Language Models без необходимости переобучения, что делает её привлекательной для улучшения производительности уже разработанных моделей.

## Преимущества по сравнению с другими подходами

1. **Эффективнее, чем стандартные sparse attention** - использует адаптивный отбор токенов вместо фиксированных шаблонов
2. **Лучшее качество, чем linear attention** - сохраняет больше информации за счет точного отбора релевантных токенов
3. **Минимальные требования к памяти** - всего 2 бита на токен для хранения компактных кодов
4. **Совместимость с существующими архитектурами** - не требует изменения архитектуры модели или переобучения

## Алгоритм работы

1. Применение преобразования Адамара к векторам запросов и ключей
2. Компрессия векторов в 2-битные коды
3. Хранение компактных кодов в кэше
4. Быстрое вычисление сходства с использованием Manhattan distance
5. Отбор 128 наиболее релевантных токенов для каждого запроса
6. Выполнение стандартного attention только над отобранными токенами

## Источники

- Исследовательская статья: arxiv.org/abs/2510.18413

## Связи с другими темами

- [[../specialized_attention_mechanisms.md]] - Другие специализированные механизмы внимания
- [[../log_linear_attention.md]] - Альтернативный подход к эффективному вниманию
- [[../inference_optimization/index.md]] - Оптимизация инференса LLM
- [[sparse_attention_patterns.md]] - Паттерны разреженного внимания