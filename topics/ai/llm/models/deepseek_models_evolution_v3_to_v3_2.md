# Эволюция моделей DeepSeek: от V3 до V3.2

## Общее описание

Этот документ описывает эволюцию архитектур моделей DeepSeek от V3 до V3.2, включая ключевые релизы R1, V3.1, V3.2-Exp и V3.2. Эти релизы представляют собой важные этапы в развитии открытых больших языковых моделей, достигающих уровня флагманских моделей при значительно меньших вычислительных затратах.

## Временная шкала релизов

### DeepSeek V3 (Декабрь 2024)
- **Архитектура**: Mixture-of-Experts (MoE) с Multi-Head Latent Attention (MLA)
- **Параметры**: 671 миллиард общих параметров, 37 миллиардов активных параметров во время инференса
- **Ключевые особенности**:
  - MLA (Multi-Head Latent Attention) - подход к экономии памяти, при котором тензоры ключей и значений сжимаются в пространство меньшей размерности
  - MoE архитектура с 256 экспертами, из которых 9 активны на токен (1 общий эксперт + 8 выбранных маршрутизатором)
  - Общий эксперт, который всегда активен для каждого токена, повышает общую производительность моделирования

### DeepSeek R1 (Январь 2025)
- **Цель**: Специализированная модель рассуждений
- **Архитектура**: Основана на V3 с улучшениями для задач рассуждения
- **Особенности**: 
  - Высокие результаты в задачах математики и программирования
  - Повышенная способность к цепочечному мышлению
  - Использование цикла генератор-верификатор для самопроверки и самосовершенствования

### DeepSeek V3.1 (Промежуточный этап)
- **Цель**: Гибридная модель, способная переключаться между инструкциями и режимами рассуждения
- **Архитектура**: Улучшенная MoE архитектура на основе V3
- **Особенности**:
  - Комбинация возможностей инструкций и рассуждений
  - Подготовка к более универсальным агентным системам
  - Плавный переход от специализированных (R1) к универсальным моделям

### DeepSeek V3.2-Exp (Сентябрь 2025)
- **Цель**: Экспериментальная модель для проверки DeepSeek Sparse Attention (DSA)
- **Архитектура**: Основана на V3.1-Terminus с интеграцией DSA
- **Ключевые особенности**:
  - Впервые реализована технология DeepSeek Sparse Attention (DSA)
  - Значительные улучшения в вычислительной эффективности для длинных контекстов
  - Сохранение практически идентичного качества вывода по сравнению с предшественником
  - Параметры: 685 миллиардов

### DeepSeek V3.2 (2025)
- **Уровень производительности**: Сопоставим с GPT-5 и Gemini 3.0 Pro
- **Архитектура**: Гибрид MLA и DSA для вычислительной эффективности
- **Ключевые инновации**:
  - Комбинированный подход обучения: верифицируемые награды (для математики/программирования) и LLM-модели вознаграждения (для общих задач)
  - Reinforcement Learning with Verifiable Rewards (RLVR) с Group Relative Policy Optimization (GRPO)
  - Self-verification и self-refinement

## Ключевые архитектурные инновации

### Multi-Head Latent Attention (MLA)
- **Описание**: Подход к экономии памяти, при котором тензоры ключей и значений сжимаются в пространство меньшей размерности перед сохранением в KV-кеш
- **Преимущества**: Лучшая производительность моделирования по сравнению как с GQA, так и с многоголовым вниманием
- **Использование**: Внедрена в V3 и усовершенствована в V3.2

### DeepSeek Sparse Attention (DSA)
- **Описание**: Технология разреженного внимания, уменьшающая вычислительную сложность с O(L²) до O(Lk)
- **Компоненты**:
  - Lightning Indexer: вычисляет индексные оценки для определения токенов, которые должны быть выбраны
  - Fine-grained token selection: механизм тонкого выбора, извлекающий только соответствующие пары ключ-значение
- **Преимущества**: 
  - Значительное снижение вычислительных затрат для длинных контекстов
  - Сохранение качества модели
  - Оборудование-ориентированная разработка с требованием специализированных ядер

### Гибридная архитектура V3.2 (MLA + DSA)
- **Особенность**: Интеграция MLA и DSA в латентных представлениях
- **Преимущество**: Дополнительное ускорение за счет интеграции DSA с top-K селектором прямо в латентные представления
- **Результат**: Еще более эффективная обработка длинных последовательностей

## Инновации в обучении с подкреплением

### Reinforcement Learning with Verifiable Rewards (RLVR)
- **Описание**: Подход, сочетающий обучение с верифицируемыми наградами (для математики/программирования) и LLM-моделями вознаграждения (для общих задач)
- **Методы**: 
  - Group Relative Policy Optimization (GRPO)
  - Self-verification и self-refinement

### Улучшения в GRPO
- **Unbiased KL Estimate**: Исправление систематической ошибки в оценке дивергенции KL
- **Off-Policy Sequence Masking**: Маскировка последовательностей, отклоняющихся от исходной модели
- **Keep Routing и Keep Sampling Mask**: Методы стабилизации для MoE моделей

## Модель DeepSeek V3.2-Speciale

### Особенности
- **Цель**: Флагманская модель с расширенными возможностями рассуждений
- **Архитектура**: MoE из семейства V3.1 Terminus с контекстом 128k
- **Контекст**: 128k токенов благодаря эффективной DSA
- **Достижения**: Золотые медали на IMO и IOI 2025

### Уникальные возможности
- Снижение штрафа за длину рассуждений
- Акцент на задачах глубокого мышления
- Высокая эффективность при значительно более низкой стоимости по сравнению с GPT-5 и Gemini 3 Pro

## Выдающиеся достижения

### Математические и программистские олимпиады
- **International Mathematical Olympiad (IMO) 2025**: Золотая медаль
- **International Olympiad in Informatics (IOI) 2025**: Золотая медаль  
- **World Finals of ICPC 2025**: Золотая медаль
- **Chinese Mathematical Olympiad (CMO) 2025**: Золотая медаль

### Производительность
- Сопоставима с GPT-5 и превосходит Gemini 3.0 Pro в математике и программировании
- Значительно дешевле в обучении и генерации по сравнению с ведущими проприетарными моделями
- Высокая эффективность при обработке длинных контекстов благодаря DSA

## Значение для отрасли

Эволюция от V3 до V3.2 представляет собой важный сдвиг в разработке LLM, демонстрирующий:

1. **Вычислительную эффективность**: Значительное сокращение ресурсов при сохранении качества
2. **Гибридный подход**: Баланс между специализированными и универсальными моделями
3. **Доступность**: Флагманские способности при значительно более низкой стоимости
4. **Масштабируемость**: Возможность обработки очень длинных контекстов с минимальными затратами

## Связи с другими темами

- [[deepseek_v3.md]] - Подробное описание архитектуры V3/R1
- [[deepseek_sparse_attention.md]] - Технология DeepSeek Sparse Attention
- [[multi_head_latent_attention.md]] - Многоголовое латентное внимание
- [[deepseek_v3_2_speciale.md]] - Флагманская модель V3.2-Speciale
- [[deepseek_v3_2_reinforcement_learning_agent_training.md]] - Подробности RL и подготовки агентов
- [[deepseek_v3_2_technical_report.md]] - Технические детали архитектуры
- [[reinforcement_learning_with_verifiable_rewards.md]] - Подробное описание RLVR
- [[group_relative_policy_optimization.md]] - Алгоритм GRPO с улучшениями
- [[mixture_of_experts_architecture.md]] - Архитектура MoE, используемая в моделях

## Источники

- Технический отчет: "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models"
- Статья: "Технический обзор моделей DeepSeek от V3 до V3.2"
- Официальные релизы и документация DeepSeek AI
- Публикации на Hugging Face и ModelScope