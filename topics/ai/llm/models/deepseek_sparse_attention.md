# DeepSeek Sparse Attention (DSA)

## Общее описание

DeepSeek Sparse Attention (DSA) — это новая технология разреженного внимания, впервые реализованная в модели DeepSeek-V3.2-Exp. Это позволяет достичь тонко настраиваемого разреженного внимания, обеспечивая значительные улучшения в эффективности обучения и вывода в сценариях длинных контекстов при сохранении практически идентичного качества вывода модели.

## Особенности технологии

- **Тонко настраиваемое разреженное внимание**: DSA позволяет достичь разреженности с тонкой настройкой впервые
- **Повышенная эффективность**: Значительные улучшения в эффективности обучения и вывода, особенно для длинных контекстов
- **Сохранение качества**: Практически идентичное качество вывода по сравнению с традиционными механизмами внимания
- **Оборудование-ориентированность**: Технология спроектирована с учетом особенностей оборудования для максимальной эффективности

## Технические детали

DSA реализует нативно обучаемый механизм разреженного внимания, который:
- Уменьшает вычислительную нагрузку при длинных входных последовательностях
- Сохраняет качество модели за счет адаптивного выбора важных позиций для вычисления внимания
- Использует специализированные ядра для реализации, включая FlashMLA и индексаторы логитов
- Включает lightning indexer и fine-grained token selection mechanism с top-K селектором для динамического определения, на какие токены обращать внимание
- Снижает вычислительную сложность с O(n²) до O(n*<w>), где <w> - средний размер адаптивного окна после тонкого отбора

## Применение в моделях

- Впервые использована в DeepSeek-V3.2-Exp как экспериментальная технология
- Служит промежуточным шагом к внедрению полноценных разреженных архитектур в будущих моделях DeepSeek
- Представляет собой исследование в области оптимизации вычислений для трансформеров

## Сравнение с предыдущими технологиями

В отличие от Multi-Head Latent Attention (MLA), которая была использована в сериях V2 и V3 и вводила сжатые латентные векторы для уменьшения размера кэша ключевых-значений (KV), DSA фокусируется на разреженности в расчетах внимания, что позволяет достичь дополнительной вычислительной эффективности.

## Преимущества

1. **Вычислительная эффективность**: Значительное снижение вычислительной нагрузки при длинных контекстах
2. **Сохранение качества**: Поддержание сопоставимого уровня точности и производительности
3. **Масштабируемость**: Лучшая производительность при обработке длинных текстов
4. **Оптимизация оборудования**: Более эффективное использование вычислительных ресурсов

## Источник

Технология описана в статье "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention" (arXiv:2502.11089).

## Связи с другими темами

- [[log_linear_attention.md]] - Альтернативный подход к эффективному вниманию с O(n log n) сложностью
- [[deepseek_v3_2_exp.md]] - первая модель, использующая эту технологию
- [[deepseek_v3_2_speciale.md]] - флагманская модель, улучшающая технологию DSA
- [[ai/llm/llm_memory_systems/llm_memory_overview.md]] - основы механизма внимания в нейронных сетях и архитектур трансформеров
- [[deepseek_v3_2_reinforcement_learning_agent_training.md]] - применение DSA в RL системах
- [[agent_training_with_synthetic_data.md]] - использование DSA в агентных системах
- [[dsa_with_top_k_selector.md]] - усовершенствованная версия DSA с top-K селектором
- [[enhanced_mla_with_top_k_selector.md]] - MLA, усиленная за счет интеграции с DSA в латентном пространстве