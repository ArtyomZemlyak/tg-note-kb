# DeepSeek Sparse Attention (DSA)

## Общее описание

DeepSeek Sparse Attention (DSA) — это новая технология разреженного внимания, впервые реализованная в модели DeepSeek-V3.2-Exp. Это позволяет достичь тонко настраиваемого разреженного внимания, обеспечивая значительные улучшения в эффективности обучения и вывода в сценариях длинных контекстов при сохранении практически идентичного качества вывода модели.

## Особенности технологии

- **Тонко настраиваемое разреженное внимание**: DSA позволяет достичь разреженности с тонкой настройкой впервые
- **Повышенная эффективность**: Значительные улучшения в эффективности обучения и вывода, особенно для длинных контекстов
- **Сохранение качества**: Практически идентичное качество вывода по сравнению с традиционными механизмами внимания
- **Оборудование-ориентированность**: Технология спроектирована с учетом особенностей оборудования для максимальной эффективности

## Технические детали

DSA реализует нативно обучаемый механизм разреженного внимания, который:
- Уменьшает вычислительную нагрузку при длинных входных последовательностях
- Сохраняет качество модели за счет адаптивного выбора важных позиций для вычисления внимания
- Использует специализированные ядра для реализации, включая FlashMLA и индексаторы логитов

## Применение в моделях

- Впервые использована в DeepSeek-V3.2-Exp как экспериментальная технология
- Служит промежуточным шагом к внедрению полноценных разреженных архитектур в будущих моделях DeepSeek
- Представляет собой исследование в области оптимизации вычислений для трансформеров

## Сравнение с предыдущими технологиями

В отличие от Multi-Head Latent Attention (MLA), которая была использована в сериях V2 и V3 и вводила сжатые латентные векторы для уменьшения размера кэша ключевых-значений (KV), DSA фокусируется на разреженности в расчетах внимания, что позволяет достичь дополнительной вычислительной эффективности.

## Преимущества

1. **Вычислительная эффективность**: Значительное снижение вычислительной нагрузки при длинных контекстах
2. **Сохранение качества**: Поддержание сопоставимого уровня точности и производительности
3. **Масштабируемость**: Лучшая производительность при обработке длинных текстов
4. **Оптимизация оборудования**: Более эффективное использование вычислительных ресурсов

## Источник

Технология описана в статье "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention" (arXiv:2502.11089).

## Связи с другими темами

- [[log_linear_attention.md]] - Альтернативный подход к эффективному вниманию с O(n log n) сложностью
- [[deepseek_v3_2_exp.md]] - первая модель, использующая эту технологию
- [[ai/llm/llm_memory_systems/llm_memory_overview.md]] - основы механизма внимания в нейронных сетях и архитектур трансформеров