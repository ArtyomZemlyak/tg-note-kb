# DeepSeek-V3.2: Методы обучения с подкреплением и агентной подготовки

## Общее описание

DeepSeek-V3.2 представляет собой значительный прогресс в области обучения с подкреплением (RL) и подготовке агентных систем. В отличие от предыдущих версий, в которых на RL тратилось менее 1% вычислительных ресурсов, в V3.2 было инвестировано более 10% мощностей от затраченных на базовую модель, что сравнимо с ведущими лабораториями. Модель использует специализированные подмодели и синтетические данные для подготовки универсального агента.

## Архитектура и ключевые технологии

### Специализированные модели
В пост-тренировке V3.2 используется дистилляция специализированных моделей — когда для каждой задачи сначала обучают отдельную модель, предназначенную исключительно для конкретной области. После подготовки специализированных моделей они используются для генерации домен-специфичных синтетических данных для дообучения финального чекпоинта.

### Области специализации
1. Математика
2. Программирование
3. Общие логические рассуждения
4. Общие агентские задачи
5. Агентское программирование
6. Агентский поиск

## Методы обучения с подкреплением

### Общий подход к RL
Авторы отмечают, что на RL потратили более 10% мощностей от затраченных на базовую модель. У R1 было меньше одного процента, у ведущих лабораторий эта цифра составляет около 100% или даже больше.

### Особенности RL для разных доменов
- Для определенных областей, таких как математика, применение относительно слабого штрафа за отклонение в "естественности" языка (по отношению к базовой модели, которая просто хорошо предсказывает текст) или даже полный отказ от него может привести к повышению производительности
- Помимо сравнения ответов и прогона тестов для математики/программирования, для остальных задач используется GRM (Generative Reward Model) и специфичные рубрики, описывающие критерии оценки

### GRM (Generative Reward Model)
GRM используется для оценки ответов в задачах, где не применяются простые тесты. Это генеративная модель вознаграждения, которая оценивает качество ответов посредством специфичных рубрик и критериев.

## Техники стабилизации и оптимизации RL

### Unbiased KL Estimate
Одним из ключевых улучшений в GRPO, используемом в DeepSeek-V3.2, является внедрение Unbiased KL Estimate для решения проблемы градиентного взрыва в оригинальном алгоритме. В оригинальном GRPO KL-регуляризация оценивалась с систематической ошибкой. Когда токены имели значительно более низкую вероятность под текущей политикой πθ, по сравнению со старой политикой πold, градиент оригинального лосса назначал непропорционально большие веса для максимизации правдоподобия этих токенов - отсюда и взрыв.

Это приводило к:
1. Шумным градиентным обновлениям
2. Нестабильной динамике обучения
3. Деградации качества сэмплов на последующих итерациях

Решением стало "Unbiased KL Estimate". Исправление заключается в перевзвешивании KL-члена с тем же самым коэффициентом важности (importance ratio), что и используется для основной функции потерь. Это делает градиент KL-ошибки несмещенным.

### Off-Policy Sequence Masking
Одна из больших проблем масштабного RL — это эффективность работы GPU. Многие фреймворки разделяют серверы на тренировочные и rollout-серверы, делающие генерации в параллель. DeepSeek используют иной подход: они генерируют сразу много вариантов, а затем разбивают их на маленькие батчи и делают несколько шагов оптимизации. Эта практика неизбежно привносит поведение off-policy, когда обновления считаются не для текущей модели, а для неё в прошлом. Чтобы стабилизировать обучение, применяют Off-Policy Sequence Masking — маскируют последовательности, которые отклоняются от исходной модели больше некоторого порога, если они хуже средней в группе генераций по оценке.

### Сохранение масок токенов
Также для стабилизации сохраняют маски по токенам, полученные из-за применения top-p / top-k семплирования (которое ограничивает, сколько самых вероятных токенов мы рассматриваем из модели для генерации). Это нужно для того, чтобы выровнять модели в режиме "как мы генерируем для оценки" и "что мы обучаем".

## Агентская подготовка и синтетические данные

### Масштабирование датасетов для агентского RL
Разнообразный набор задач имеет решающее значение для повышения надежности и генерализуемости модели. DeepSeek собрали и синтезировали 85 тысяч агентных задач в 4 категориях.

### Поисковый агент (Search Agent)
- Сделали мультиагентную систему на базе DeepSeek-V3.2 для генерации разнообразных и высококачественных обучающих данных
- Сначала вычленяют сущности из "длинного хвоста" в различных доменах из опубликованных веб-корпусов, хранящих сотни тысяч интернет-страниц
- Агент по конструированию вопросов исследует каждую сущность, используя инструмент поиска, консолидируя найденную информацию в пары вопрос-ответ
- Используется несколько разных агентов — это разные чекпоинты моделей DeepSeek, с разными системными промптами, разными конфигурациями поиска (вглубь/вширь) и т.д.
- Агент-верификатор проверяет все ответы в несколько проходов, оставляя только те образцы, где ответ достоверен
- Эти данные охватывают множество языков, доменов и уровней сложности
- К вопросам генерируются рубрики, используемые для оценки ответов

### Кодинг-агент (Code Agent)
- Воспроизводят SWE-Bench/ReBench, извлекая миллионы пар "issue — Pull Request (PR)" из GitHub
- Этот набор данных тщательно фильтруется с использованием эвристик и запромпченных LLM для обеспечения высокого качества
- Требовалось, чтобы каждая запись содержала разумное описание проблемы, "золотой патч" (код функциональности/фикса) и набор тестов для валидации
- DeepSeek использовали автоматизированного агента на базе DeepSeek-V3.2 для настройки среды: установки пакетов, разрешения зависимостей и прогон тестов
- Среды фильтруются, оставляют только то, где применение "золотого" патча с исправлением приводит к увеличению количества проходящих тестов, без падения любых тестов, которые проходили до применения патча
- Используя этот конвейер, DeepSeek создали десятки тысяч воспроизводимых сред, охватывающих несколько языков программирования, включая Python, Java, JavaScript, TypeScript, C, C++, Go и PHP

### Общий агент (General Agent)
- Используют агента, чтобы генерировать синтетику, чтобы учить агента
- Собирают большой набор категорий, в которых агент должен работать
- Получив категорию задачи (например, планирование маршрута путешествия) и виртуальную машину, оснащенную инструментом командной строки и инструментом поиска, агент сначала использует эти инструменты для генерации или извлечения релевантных данных из Интернета и сохранения их в базе данных
- Затем агент пишет код для набора специфичных для задачи инструментов, каждый из которых реализован как функция
- Чтобы создать задачи, которые одновременно сложны и поддаются автоматической проверке, агент сначала предлагает простую задачу на основе текущей базы данных вместе с ее решением и функциями проверки, реализованными на Python. Если решение не проходит валидацию, агент модифицирует решение или функции проверки, пока вывод решения не пройдет проверку
- Затем агент итеративно увеличивает сложность задачи (выраженное как количество вызовов инструмента) и обновляет соответствующие решение и функции проверки. В ходе этого итеративного процесса, если текущего набора инструментов недостаточно для решения задачи, агент сам дописывает инструментарий

### Пример задачи для General Agent
1. Есть детальнейшее описание того, что хочет человек при планировании путешествия
2. Есть набор функций, предоставляющий большое количество информации по городу, отелям, транспорту и т.д.
3. Задача агента придумать, какие методы и с какими аргументами вызывать инструменты, чтобы получить правильный ответ

### Code Interpreter Agent
Категория без детального описания в исходном источнике.

## Thinking Retention Mechanism (Механизм сохранения мышления)

Во время обучения на задачах с инструментами начали применять подход как у OpenAI, когда рассуждения не выкидываются, и видны модели при генерации следующего вызова инструмента. Представьте что для ответа на первый запрос пользователя нужно вызвать 2 инструмента, и после этого дать ответ. При вызове второго модель будет видеть, о чём она уже подумала, а обычно это выкидывают, что приводит к значительной неэффективности использования токенов. Этот подход заставляет модель заново обдумывать всю проблему при каждом последующем вызове инструмента.

После того как модель написала ответ и он вернулся пользователю, цепочки рассуждений удаляются, история вызовов инструментов и их результатов остается сохраненной в контексте, так что модели видно, что она делала, но не о чём думала.

## DeepSeek-V3.2-Speciale

Также обучили DeepSeek-V3.2-Speciale — всё то же самое, только снизили штраф за длину размышлений, плюс выкинули все данные без рассуждений. Как следствие, модель может думать дольше, что, со слов авторов, приводит к метрикам чуть получше-повыше.

## Результаты и достижения

### Математические достижения
- Решает 5/6 задач с международной олимпиады по математике, завоёвывая золото и повторяя успехи OpenAI / Google
- Берёт золото на ICPC/IOI, солидных соревнованиях по программированию

![Таблица 3: Производительность в олимпиадах и эффективность моделей рассуждений](../../../../media/img_1764669521_aqadtg9rgx7cul_table_3_benchmark_performance_and.jpg) <!-- TODO: Broken image path -->

**Таблица 3 показывает:** Сравнение производительности рассуждающих моделей на различных бенчмарках, включая математические олимпиады и задачи программирования. Для каждого бенчмарка ячейки показывают точность и количество выходных токенов (в тысячах). Наивысшая точность на бенчмарке выделена жирным шрифтом; вторая по величине — подчеркнута.

![Таблица 4: Производительность DeepSeek-V3.2-Speciale в топовых математических и программистских соревнованиях](../../../../media/img_1764669521_aqadxg9rgx7cul_table_4_performance_of_deepseek_v3_2_spe.jpg) <!-- TODO: Broken image path -->

**Таблица 4 показывает:** Производительность DeepSeek-V3.2-Speciale в престижных математических и программистских соревнованиях. Для ICPC WE 2025 отображается количество отправок для каждого успешно решенной проблемы. DeepSeek-V3.2-Speciale занял 2-е место на ICPC WE 2025 и 10-е место на IOI 2025.

![Сравнение с Claude 4.5 и другими моделями](../../../../media/img_1764669521_aqadtq9rgx7cul_claude_4_5.jpg) <!-- TODO: Broken image path -->

**Изображение показывает:** Сравнительная таблица производительности Claude 4.5, GPT-5, Gemini-3.0, Kimi-K2, MiniMax и DeepSeek-V3.2 на различных бенчмарках, включая MMILU-Pro, GPQA Diamond, HLE, LiveCodeBench, Codeforces, математические олимпиады (AIME, HMMT, IMO) и задачи агентов.

### Инженерные решения
- Для программирования генерируют 32 или 500 кандидатов, фильтруют по тестам, и делают несколько отправок решений, к которым привели самые длинные цепочки рассуждений
- В математике используют цикл с генератором и верификатором, как в DeepSeekMath-v2, чтобы выбиться в топ

### Ограничения
- Широта знаний о мире у DeepSeek-V3.2 всё еще отстает от ведущих проприетарных моделей
- Для Speciale-версии модели (см. в следующем посте) нужно в 2-3 раза больше токенов (при более низкой скорости генерации, по крайней мере в официальном API), чем другим моделям — это из-за более длинных цепочек рассуждений

## Проверка качества синтетических данных

Для категории General Agent делают дополнительную фильтрацию, чтобы выкинуть некачественную синтетику. Для этого запускают RL на этом наборе данных, и прогоняют промежуточную модель по 100 раз на каждом вопросе. Оставляют только те, где модель хотя бы раз смогла дать правильный ответ — и на них будут переучиваться как на "чистых" данных.

Чтобы ответить на вопрос о качестве, авторы случайным образом выбирают 50 примеров и оценивают разные модели, включая проприетарные. DeepSeek-V3.2-Exp достигает точности всего 12% с первой попытки, в то время как передовые закрытые модели достигают 62%.

## Связи с другими темами

- [[deepseek_v3_2_exp.md]] - экспериментальная версия с аналогичной технологией
- [[deepseek_v3_2_speciale.md]] - флагманская версия на основе экспериментальной модели с улучшенной архитектурой
- [[ai/llm/reinforcement_learning/rler_reinforcement_learning_with_evolving_rubrics.md]] - современные методы RL с изменяющимися рубриками
- [[ai/llm/synthetic_training_data.md]] - синтетические обучающие данные для LLM
- [[ai/agents/advanced_tool_calling_and_planning.md]] - продвинутое вызывание инструментов и планирование
- [[ai/llm/models/deepseek_sparse_attention.md]] - технология разреженного внимания, используемая в этой модели

## Связи с другими темами

- [[deepseek_v3_2_exp.md]] - экспериментальная версия с аналогичной технологией
- [[deepseek_v3_2_speciale.md]] - флагманская версия на основе экспериментальной модели с улучшенной архитектурой
- [[ai/llm/reinforcement_learning/rler_reinforcement_learning_with_evolving_rubrics.md]] - современные методы RL с изменяющимися рубриками
- [[ai/llm/synthetic_training_data.md]] - синтетические обучающие данные для LLM
- [[ai/agents/advanced_tool_calling_and_planning.md]] - продвинутое вызывание инструментов и планирование
- [[ai/llm/models/deepseek_sparse_attention.md]] - технология разреженного внимания, используемая в этой модели
- [[thinking_retention_mechanism.md]] - механизм сохранения мышления, используемый в агентных задачах
- [[off_policy_sequence_masking.md]] - метод стабилизации обучения с подкреплением
- [[generative_reward_model_grm.md]] - генеративная модель вознаграждения для оценки ответов
- [[agent_training_with_synthetic_data.md]] - методы подготовки агентных систем с использованием синтетических данных
- [[unbiased_kl_estimate_in_grpo.md]] - улучшение GRPO с несмещенной оценкой KL-дивергенции, используемое в DeepSeek-V3.2
- [[dsa_with_top_k_selector.md]] - усовершенствованный механизм DSA с top-K селектором, используемый в архитектуре

## Источники

- Технический отчет: https://huggingface.co/deepseek-ai/DeepSeek-V3.2
- Статья о DeepSeek V3.2: "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models"
- Данные о бенчмарках и сравнениях с другими моделями