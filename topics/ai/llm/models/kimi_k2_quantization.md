# Квантизация Kimi K2

## Описание

Kimi K2 - это крупномасштабная языковая модель с архитектурой Mixture of Experts (MoE), разработанная компанией MoonShot AI. По бенчмаркам находится наравне с лучшими проприетарными моделями, такими как Gemini, Claude и GPT. Модель имеет 1 триллион параметров, что делает ее одной из самых крупных LLM 2025 года, с 32 миллиардами активных параметров во время вывода.

## Архитектурные особенности

### Основа архитектуры
- Использует архитектуру DeepSeek V3 как основу, но масштабирует её
- Использует больше экспертов в модулях смеси экспертов (384 эксперта против 256 у DeepSeek-V3)
- Использует меньше голов в модуле многоголового латентного внимания (64 головы против 128 у DeepSeek-V3)

### Особенности Kimi K2 Thinking
- Улучшенная версия модели с выдающимися возможностями вызова инструментов
- Широко признана за свои продвинутые возможности автономного планирования и выполнения сложных многошаговых задач
- Может выполнять до 200-300 последовательных вызовов инструментов

## Квантизация и оптимизация

### Использование MoE-Quant
Согласно информации, MoonShot AI использовала кодовую базу MoE-Quant для квантизации своей модели Kimi-K2-Thinking. Это подчеркивает важность специализированных инструментов для квантизации MoE-моделей, которые учитывают уникальные особенности архитектуры MoE.

### Формат Compressed Tensors
- MoonShot AI использовала скрипты конвертации в формат compressed-tensors для Kimi-K2-Thinking
- Это позволяет эффективно хранить и развертывать квантованную модель
- Формат поддерживает различные схемы квантования и разреженности, что особенно важно для MoE-архитектур

## Значение для экосистемы ИИ

### "DeepSeek moment 2.0"
- Kimi K2 Thinking вызвала так называемый "DeepSeek moment 2.0" в AI-сообществе
- Демонстрирует, что открытые веса могут конкурировать с проприетарными моделями
- Показала революционные возможности вызова инструментов

### Масштабирование проверенной архитектуры
- Вместо разработки новой архитектуры, команда сосредоточилась на масштабировании существующей (DeepSeek V3) до экстремальных размеров
- Использовала оптимизатор Muon как ключевое улучшение

## Технические характеристики

- Общее количество параметров: 1 триллион
- Активные параметры во время вывода: 32 миллиарда
- Обучена на 15.5 триллионах токенов данных
- Архитектура: Mixture of Experts (MoE)
- Лицензия: модифицированная MIT

## Связи с другими темами

- [[../architectures/kimi_k2.md]] - Подробное описание архитектуры Kimi K2
- [[moe_quant.md]] - Фреймворк MoE-Quant, использованный для квантизации
- [[compressed_tensors.md]] - Формат сжатых тензоров, использованный для конвертации
- [[../architectures/mixture_of_experts.md]] - Архитектура Mixture of Experts
- [[../research_advances/deepseek_moment_2.md]] - Контекст "DeepSeek moment 2.0"