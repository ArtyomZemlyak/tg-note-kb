# Kimi-Linear

## Описание

Kimi-Linear - это гибридная архитектура линейного внимания, разработанная для повышения эффективности обработки длинных контекстов в больших языковых моделях. Модель сочетает в себе два механизма внимания: Kimi Delta Attention (KDA) и Multi-Head Linear Attention (MLA), что позволяет достичь высокой производительности при значительно сниженном потреблении памяти.

## Архитектура

### Гибридное внимание

Kimi-Linear использует гибридную архитектуру с соотношением 3:1 между Kimi Delta Attention (KDA) и глобальным MLA (Multi-Head Linear Attention), что:

- Снижает использование памяти, сохраняя или превосходя качество полного внимания
- Уменьшает потребность в больших KV-кешах до 75%
- Увеличивает пропускную способность декодирования до 6× для контекстов длиной до 1 млн токенов

### Спецификации

- **Общее количество параметров**: 48B
- **Активированные параметры**: 3B (в любой момент времени)
- **Длина контекста**: до 1 млн токенов
- **Тип архитектуры**: Гибридное линейное внимание
- **Обучено на**: 5.7 трлн токенов
- **Лицензия**: MIT

### Объяснение терминов

- **Kimi Delta Attention (KDA)**: Улучшенная версия Gated DeltaNet, которая вводит более эффективный механизм гейтирования и оптимизирует использование памяти конечных автоматов. Это линейный механизм внимания, который уточняет правило gated delta с тонкой настройкой гейтирования.
- **Multi-Head Linear Attention (MLA)**: Архитектурная техника, сжимающая тензоры ключей и значений в пространство меньшей размерности перед их сохранением в KV-кеше, а затем проецирующая их обратно к исходному размеру во время инференса.

## Технические детали

### Механизм работы

Во время обучения запросы также сжимаются, но во время инференса (вывода) - нет. Это позволяет экономить память во время инференса, где KV-кеширование наиболее критично.

### Производительность

- **MMLU-Pro (4k контекст)**: 51.0 с производительностью, схожей с полным вниманием
- **RULER (128k контекст)**: Показывает оптимальные характеристики (84.3) с 3.98× ускорением
- **TPOT**: 6.3× более быстрое время на выходной токен (TPOT) по сравнению с MLA
- **Декодирование**: Значительное ускорение при длинных последовательностях (1M токенов)

## Преимущества

- **Превосходная эффективность оборудования**, особенно для задач с длинным контекстом
- **Высокая пропускная способность** с ускорением декодирования до 6×
- **Превосходит полное внимание** в различных задачах, включая задачи с длинным контекстом и бенчмарки с усилением обучения
- **Экономия памяти**: Снижение использования KV-кеша до 75%
- **Повышенная скорость**: Ускорение до 6.3× для длинных контекстов

## Сравнение с другими подходами

### KDA vs MLA
- Основная часть слоёв использует KDA для эффективности и рассуждений
- MLA используется для точности и стабильности
- Сочетание позволяет достичь почти уровня крупных LLM на длинных контекстах при значительно меньших затратах памяти и времени

## Применение

Модель Kimi-Linear особенно эффективна для задач:
- Обработки длинных текстов
- Рассуждения с длинным контекстом
- Генерации с длинной цепочкой рассуждений
- Сценариев с ограниченными ресурсами памяти

## Связи с другими темами

- [[../architectures/kimi_delta_attention.md|Kimi Delta Attention]] - основной механизм внимания в архитектуре
- [[../architectures/multi_head_latent_attention.md|Multi-Head Latent Attention]] - второй компонент гибридной архитектуры
- [[../log_linear_attention.md|Log-linear attention]] - связанные линейные архитектуры внимания
- [[../specialized_attention_mechanisms.md|Специализированные механизмы внимания]] - альтернативные подходы к эффективному вниманию