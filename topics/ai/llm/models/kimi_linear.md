# Kimi-Linear

## Описание

Kimi-Linear - это гибридная архитектура внимания, в которой новый модуль линейного внимания, Kimi Delta Attention (KDA), чередуется со стандартным полным вниманием (MLA) в соотношении 3:1. В своей основе KDA развивает правило Gated Delta Rule из работы Gated DeltaNet, добавляя мелкозернистый, поканальный механизм гейтинга для более точного управления рекуррентной памятью. Для аппаратной эффективности используется специально разработанный параллельный алгоритм, работающий по частям (chunkwise) и основанный на ограниченном варианте структуры Diagonal-Plus-Low-Rank (DPLR). Это позволяет достичь почти вдвое большей скорости работы ядра по сравнению с общими формулировками DPLR.

## Архитектура

### Гибридное внимание

Kimi-Linear использует гибридную архитектуру с соотношением 3:1 между Kimi Delta Attention (KDA) и глобальным MLA (Multi-Head Linear Attention), что:

- Снижает использование памяти, сохраняя или превосходя качество полного внимания
- Уменьшает потребность в больших KV-кешах до 75%
- Увеличивает пропускную способность декодирования до 6× для контекстов длиной до 1 млн токенов

### Ключевые архитектурные решения

- **Передача всей информации о позициях слоям KDA**: Слои MLA используют No Position Encoding (NoPE), что даёт два преимущества:
  - Упрощает обучение на длинных контекстах, избавляя от необходимости настраивать гиперпараметры RoPE
  - Позволяет преобразовывать слои MLA в высокоэффективное Multi-Query Attention (MQA) во время инференса

### Спецификации

- **Общее количество параметров**: 48B
- **Активированные параметры**: 3B (в любой момент времени)
- **Длина контекста**: до 1 млн токенов
- **Тип архитектуры**: Гибридное линейное внимание
- **Обучено на**: 1.4 триллиона токенов (оригинальный источник: 1.4 трлн, не 5.7 как возможно в старой версии)
- **Лицензия**: MIT

### Объяснение терминов

- **Kimi Delta Attention (KDA)**: Улучшенная версия Gated DeltaNet, которая вводит мелкозернистый диагонализированный гейт `Diag(αₜ)`, что является прямым улучшением по сравнению с предыдущими работами, такими как Gated DeltaNet (GDN), где использовался более грубый гейт забывания для всей головы внимания. Поканальный механизм позволяет каждой размерности признаков иметь свою независимую скорость забывания, обеспечивая гораздо более точное управление RNN-памятью модели фиксированного размера. Основная рекуррентная формула: `Sₜ = (I - βₜ kₜ kₜᵀ) Diag(αₜ) Sₜ₋₁ + βₜ kₜ vₜᵀ`
- **Multi-Head Linear Attention (MLA)**: Архитектурная техника, сжимающая тензоры ключей и значений в пространство меньшей размерности перед их сохранением в KV-кеше, а затем проецирующая их обратно к исходному размеру во время инференса.
- **Diagonal-Plus-Low-Rank (DPLR)**: Структура, используемая в KDA, где переменные скорости затухания и обучения привязаны к вектору ключа kₜ, позволяющая использовать специально разработанный и высокооптимизированный параллельный алгоритм, работающий по частям (chunkwise).
- **No Position Encoding (NoPE)**: Отказ от позиционных эмбеддингов в слоях MLA, что упрощает обучение на длинных контекстах.

## Технические детали

### Механизм работы

Во время обучения запросы также сжимаются, но во время инференса (вывода) - нет. Это позволяет экономить память во время инференса, где KV-кеширование наиболее критично. Также используется стратегическое разделение обязанностей: слои KDA получают всю информацию о позициях, в то время как MLA использует NoPE.

### Производительность

- **MMLU-Pro (4k контекст)**: 51.0 (опережая MLA 47.2 и GDN-H 47.9)
- **RULER (128k контекст)**: 84.3
- **RepoQA (128k контекст)**: 68.5
- **Средний балл на длинных контекстах**: 54.5
- **TPOT (Time Per Output Token)**: 6.3× более быстрое время на выходной токен (TPOT) по сравнению с MLA при контексте 1 млн токенов
- **Prefill**: 2.9× более быстрое время начальной обработки промпта по сравнению с MLA
- **Использование KV-кэша**: На 75% меньше по сравнению с традиционными архитектурами
- **Декодирование**: Значительное ускорение при длинных последовательностях (1M токенов)

## Преимущества

- **Превосходная эффективность оборудования**, особенно для задач с длинным контекстом
- **Высокая пропускная способность** с ускорением декодирования до 6×
- **Превосходит полное внимание** в различных задачах, включая задачи с длинным контекстом и бенчмарки с усилением обучения
- **Экономия памяти**: Снижение использования KV-кеша до 75%
- **Повышенная скорость**: Ускорение до 6.3× для длинных контекстов
- **Конец компромисса между качеством и эффективностью**: Впервые в строгих сравнениях при одинаковом масштабе показано, что гибридная линейная архитектура может стабильно превосходить сильный бейзлайн на основе полного внимания в сценариях с коротким и длинным контекстом, а также в обучении с подкреплением
- **Парето-оптимальность**: Предлагает масштабируемую, Парето-оптимальную "drop-in replacement" замену для стандартных трансформеров
- **Эффективность в обучении с подкреплением**: Показывает значительно более быструю сходимость и более высокую итоговую производительность на сложных задачах, требующих рассуждений

## Сравнение с другими подходами

### KDA vs MLA
- Основная часть слоёв использует KDA для эффективности и рассуждений
- MLA используется для точности и стабильности
- Сочетание позволяет достичь почти уровня крупных LLM на длинных контекстах при значительно меньших затратах памяти и времени

### KDA vs Gated DeltaNet (GDN)
- KDA улучшает Gated DeltaNet, вводя мелкозернистый диагонализированный гейт
- Обеспечивает гораздо более точное управление RNN-памятью модели фиксированного размера за счёт поканального механизма
- На синтетических задачах извлечения информации и отслеживания состояния KDA сходится значительно быстрее, чем GDN и Mamba2
- Включает аппаратно-эффективную формулировку DPLR для ускорения вычислений

## Применение

Модель Kimi-Linear особенно эффективна для задач:
- Обработки длинных текстов
- Рассуждения с длинным контекстом
- Генерации с длинной цепочкой рассуждений
- Сценариев с ограниченными ресурсами памяти
- Агентного ИИ и приложений с экстремально длинным контекстом
- Сценариев, где требуется высокая пропускная способность инференса

## Ограничения

- Вычислительное преимущество в эксперименте по закону масштабирования обучения относительно скромное (~1.16x), что говорит о схожих свойствах масштабирования с полным вниманием
- Основные выгоды сосредоточены в эффективности инференса
- Производительность модели чувствительна к соотношению гибридных слоёв и стратегии NoPE
- Вариант с использованием RoPE в слоях MLA показал худшие результаты на задачах с длинным контекстом

## Будущие направления

- Создание гибридных моделей, которые бы сочетали сжатие и обобщающую способность линейного внимания с возможностями извлечения мелкозернистой информации, присущими разреженному вниманию (sparse attention).

## Связи с другими темами

- [[../architectures/kimi_delta_attention.md|Kimi Delta Attention]] - основной механизм внимания в архитектуре
- [[../architectures/gated_deltanet.md|Gated DeltaNet]] - базовая архитектура, улучшенная в KDA
- [[../architectures/multi_head_latent_attention.md|Multi-Head Latent Attention]] - второй компонент гибридной архитектуры
- [[../log_linear_attention.md|Log-linear attention]] - связанные линейные архитектуры внимания
- [[../specialized_attention_mechanisms.md|Специализированные механизмы внимания]] - альтернативные подходы к эффективному вниманию
- [[../reinforcement_learning/reinforcement_learning_for_llm_alignment.md|Обучение с подкреплением]] - улучшенная эффективность в RL-сеттингах