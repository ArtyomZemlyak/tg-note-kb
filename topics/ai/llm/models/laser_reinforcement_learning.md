# LASER: Обучение с подкреплением с использованием самонаграждения последнего токена

## Краткое описание

LASER (Reinforcement Learning with Last-Token Self-Rewarding) - это алгоритм обучения с подкреплением, который улучшает крупные языковые модели (LLM) путем согласования оценок самонаграждения последнего токена с наградами за рассуждение на основе верификатора, тем самым улучшая производительность при рассуждениях и масштабировании во время вывода.

## Авторы

- Вэнькай Ян (Wenkai Yang)
- Вэйцзе Лю (Weijie Liu)
- Руобин Се (Ruobing Xie)
- Ицзю Гуо (Yiju Guo)
- Лулу У (Lulu Wu)
- Сайён Ян (Saiyong Yang)
- Янькай Линь (Yankai Lin)

## Основные концепции

### Reinforcement Learning with Verifiable Rewards (RLVR)

RLVR (Обучение с подкреплением с проверяемыми наградами) - это метод, используемый для улучшения возможностей рассуждения крупных языковых моделей. Он позволяет моделям генерировать решения, а затем проверять их корректность с помощью внутреннего механизма проверки.

### Last-Token Self-Rewarding (Самонаграждение последнего токена)

Ключевая идея LASER заключается в том, что истинная награда за рассуждение решения равна оценке самонаграждения его последнего токена. Эта оценка вычисляется как разница между лог-вероятностью следующего токена, присвоенной моделью политики любому заранее заданному токену в последнем токене решения, и заранее вычисленной константой, масштабированной коэффициентом KL.

### Теоретическое понимание

В статье делается вывод, что замкнутое решение для цели RL при самопроверке можно свести к простой форме: истинная награда за рассуждение решения равна оценке его самонаграждения последнего токена. Это позволяет вычислить оценку награды сразу после генерации последнего токена, используя вероятностное распределение следующего токена.

### Метод LASER

На основе этого понимания авторы предлагают LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), который дополняет исходную потерю RLVR потерей MSE (Mean Squared Error), выравнивающей оценки самонаграждения последнего токена с наградами за рассуждение на основе верификатора. Метод совместно оптимизирует возможности рассуждения и самонаграждения LLM с минимальными дополнительными затратами всего в один дополнительный вывод токена.

## Основные идеи

1. **Улучшение эффективности**: Предыдущие методы требовали от LLM генерации решений и самопроверок с использованием двух отдельных шаблонов запросов, что снижало эффективность. LASER решает эту проблему, используя унифицированный подход.

2. **Теоретическое понимание**: Истинная награда за рассуждение решения равна оценке его самонаграждения последнего токена, которую можно вычислить из распределения вероятности следующего токена последнего токена сразу после генерации.

3. **Простая реализация**: LASER добавляет простую потерю MSE к стандартной цели RLVR, чтобы выровнять оценки самонаграждения последнего токена с наградами за рассуждение на основе верификатора.

4. **Двойная оптимизация**: Алгоритм одновременно оптимизирует как рассуждение, так и возможности самонаграждения LLM.

5. **Минимальные вычислительные затраты**: Подход требует только одного дополнительного вывода токена, что делает его вычислительно эффективным.

6. **Универсальное применение**: Оптимизированные оценки самонаграждения могут использоваться как на этапах обучения, так и на этапах тестирования для повышения производительности модели.

## Применение

- Улучшение способностей LLM к рассуждению
- Оптимизация эффективности во время вывода
- Совместная оптимизация рассуждений и самонаграждения
- Использование как во время обучения, так и во время тестирования

## Преимущества перед другими методами

- Повышенная эффективность по сравнению с методами, требующими отдельных шаблонов запросов
- Минимальные дополнительные вычислительные затраты
- Простота реализации
- Улучшенная масштабируемость при выводе

## Связи с другими темами

[[ai/llm/models/anthropic_haiku_4_5.md]] - сравнение методов оптимизации LLM