# LLaDA2.0: Диффузионные языковые модели с архитектурой Mixture-of-Experts

## Общее описание

LLaDA2.0 - это серия передовых диффузионных языковых моделей (Diffusion Language Models, DLM), разработанных компанией inclusionAI. Эти модели представляют собой улучшенную, настроенную на выполнение инструкций версию оригинальной серии LLaDA, оптимизированную для практических приложений. В отличие от традиционных автoregressive моделей (ARMs), LLaDA2.0 использует диффузионный подход к генерации текста, основанный на маскировке данных и обратном процессе восстановления.

## Архитектурные особенности

### Модель диффузии
LLaDA модели распределяют вероятности через прямой процесс маскировки данных и обратный процесс восстановления, параметризованный обычным трансформером для предсказания зашумленных токенов. Это альтернативный подход к традиционным автoregressive моделям, где текст генерируется последовательно токен за токеном.

### Mixture-of-Experts (MoE)
Все модели LLaDA2.0 используют архитектуру Mixture-of-Experts (MoE), которая позволяет активировать только часть параметров во время вывода, сохраняя вычислительную эффективность при масштабировании количества параметров. Это позволяет достичь высокой производительности при значительной экономии вычислительных ресурсов.

## Варианты моделей

### LLaDA2.0-mini
- **Общее количество параметров**: 16 миллиардов
- **Количество активированных параметров во время вывода**: 1.4 миллиарда
- **Количество слоев**: 20
- **Количество внимательных головок**: 16
- **Длина контекста**: 32,768 токенов
- **Позиционное встраивание**: Rotary (RoPE)
- **Размер словаря**: 157,184
- **Назначение**: Более легковесная версия для задач, где важна эффективность

### LLaDA2.0-flash
- **Общее количество параметров**: 100 миллиардов
- **Количество активированных параметров во время вывода**: 6.1 миллиарда
- **Количество слоев**: 32
- **Количество attention голов**: 32
- **Длина контекста**: 32,768 токенов
- **Позиционное встраивание**: Rotary (RoPE)
- **Размер словаря**: 157,184
- **Назначение**: Более мощная версия для сложных задач, требующих высокой производительности

## Технические характеристики

### Общие параметры
- **Формат файла**: Safetensors
- **Тип тензора**: BF16
- **Лицензия**: Apache License 2.0
- **Языковая модель**: CausalLM (генеративная языковая модель)

### Рекомендуемые параметры работы
- **Параметры сэмплирования**: Температура=0.0, block_length=32, steps=32
- **Достаточная длина вывода**: Рекомендуется использовать длину вывода 32,768 токенов для большинства запросов
- **Важное замечание**: Использование более высокого значения температуры может привести к смешиванию языков и небольшому снижению производительности модели

### Пример использования (Python)
```python
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer

model_path = "/path/to/LLaDA2.0-mini"  # или LLaDA2.0-flash
device = "cuda:0"
model = AutoModelForCausalLM.from_pretrained(
    model_path, trust_remote_code=True, device_map=device
)
model = model.to(torch.bfloat16)
model.eval()
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

prompt = "Why does Camus think that Sisyphus is happy?"
input_ids = tokenizer.apply_chat_template(
    [{"role": "user", "content": prompt}],
    add_generation_prompt=True,
    tokenize=True,
    return_tensors="pt",
)
generated_tokens = model.generate(
    inputs=input_ids,
    eos_early_stop=True,
    gen_length=512,
    block_length=32,
    steps=32,
    temperature=0.0,
)
generated_answer = tokenizer.decode(
    generated_tokens[0],
    skip_special_tokens=True,
)
print(generated_answer)
```

## Производительность и бенчмарки

### LLaDA2.0-mini
| Категория | Среднее | MMLU | MMLU-Pro | GPQA | arc-c | CMMLU | C-EVAL | GAOKAO-Bench |
|-----------|---------|------|----------|------|-------|-------|--------|--------------|
| Оценка | 71.67 | 80.53 | 63.22 | 47.98 | 93.56 | 79.50 | 81.38 | 84.30 |

**Кодирование**:
- CRUXEval-O: 71.62
- MBPP: 81.50
- HumanEval: 86.59
- Spider: 76.76

**Математика**:
- GSM8K: 94.24
- MATH: 93.22
- OlympiadBench: 67.70

### LLaDA2.0-flash
| Категория | Среднее | MMLU | MMLU-Pro | GPQA | arc-c | CMMLU | C-EVAL | GAOKAO-Bench |
|-----------|---------|------|----------|------|-------|-------|--------|--------------|
| Оценка | 79.32 | 87.69 | 73.36 | 61.98 | 95.93 | 85.13 | 86.75 | 93.90 |

**Кодирование**:
- CRUXEval-O: 85.12
- MBPP: 88.29
- HumanEval: 94.51
- Spider: 82.49

**Математика**:
- GSM8K: 96.06
- MATH: 95.44
- OlympiadBench: 74.07

## Применение

LLaDA2.0 оптимизированы для следующих задач:
- **Генерация текста**: Подходят для различных задач по генерации текста, включая творческое письмо и создание контента
- **Выполнение инструкций**: Хорошо подходят для задач, требующих выполнения специфических инструкций пользователя
- **Сложные рассуждения**: Эффективны для задач, требующих аналитического мышления, логических выводов и решения проблем
- **Генерация кода**: Показывают отличные результаты в задачах по написанию и завершению кода
- **Математические вычисления**: Сильные способности к решению сложных математических задач
- **Использование инструментов**: Поддержка вызова инструментов делает их подходящими для сложных задач на основе агентов и автоматизации

## Основные преимущества

1. **Высокая эффективность**: Архитектура MoE позволяет значительно снизить вычислительные затраты при превосходстве над открытыми плотными моделями аналогичного масштаба
2. **Превосходящие способности к рассуждению**: Показывают впечатляющие результаты в задачах кодирования, сложных рассуждениях и математических вычислениях
3. **Поддержка инструментов**: Возможность вызова инструментов и отличные результаты в задачах на основе агентов
4. **Открытость и расширяемость**: Полностью открытые модели с приверженностью прозрачности
5. **Длинный контекст**: Поддержка длины контекста в 32,768 токенов позволяет работать с очень длинными документами

## Связи с другими темами

- [[../architectures/diffusion/diffusion_llm_architectures.md]] - Архитектуры диффузионных LLM, на которых основана LLaDA
- [[../diffusion_models.md]] - Общее описание диффузионных моделей
- [[../architectures/diffusion/text_diffusion_models.md]] - Текстовые диффузионные модели
- [[../architectures/mixture_of_experts_architecture.md]] - Архитектура Mixture-of-Experts, используемая в LLaDA2.0
- [[../post_training_methods.md]] - Методы дообучения, включая обучение с подкреплением
- [[../tools/dllm_library.md]] - Библиотека DLLM, связанная с диффузионными языковыми моделями
- [[reasoning.md]] - Способности к рассуждению, в которых LLaDA2.0 показывает отличные результаты
- [[text_generation_methods.md]] - Методы генерации текста, в которых LLaDA2.0 представляет альтернативный подход

## Источники

1. [LLaDA2.0-mini Model on Hugging Face](https://huggingface.co/inclusionAI/LLaDA2.0-mini) - Официальная страница модели LLaDA2.0-mini с подробной информацией о характеристиках, использовании и производительности
2. [LLaDA2.0-flash Model on Hugging Face](https://huggingface.co/inclusionAI/LLaDA2.0-flash) - Официальная страница модели LLaDA2.0-flash с подробной информацией о характеристиках, использовании и производительности
3. [Large Language Diffusion Models Overview](https://javiersolisgarcia.com/posts/llada/) - Описание оригинальной концепции LLaDA и маскировочного диффузионного подхода
4. [Paper Review: Large Language Diffusion Models](https://andlukyane.com/blog/paper-review-llada) - Технический обзор оригинальной диффузионной модели LLaDA