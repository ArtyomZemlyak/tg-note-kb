# MiniMax-M2

## Общее описание

**MiniMax-M2** — это инновационная модель искусственного интеллекта с архитектурой Mixture of Experts (MoE), обладающая 230 миллиардами параметров при активных только 10 миллиардах параметров. Это делает её мощной, быстрой и экономичной моделью, сочетающей интеллект на уровне топовых LLM с оптимизацией под агентные применения и программирование.

## Архитектура

### Основные характеристики
- **Общее количество параметров**: 230 миллиардов
- **Активные параметры**: 10 миллиардов (MoE-архитектура)
- **Архитектура**: GPT-OSS с комбинацией Full Attention и Sliding Window Attention (SWA)
- **Нормализация**: Каждая голова внимания имеет собственный RMSNorm
- **RoPE-параметры**: Разные параметры для блоков Full Attention и SWA

### Особенности архитектуры
MiniMax-M2 построена по принципу GPT-OSS и использует сочетание Full Attention и Sliding Window Attention (SWA), что помогает эффективно работать с длинным контекстом. Часть модели анализирует всё сразу, другая концентрируется на ближайших фрагментах текста.

Каждая голова внимания имеет собственный RMSNorm, а блоки Full Attention и SWA используют разные RoPE-параметры, что повышает гибкость и устойчивость модели.

## Ключевые особенности

### Интеллект мирового уровня
- Демонстрирует отличные результаты в математике, науке, программировании, следовании инструкциям и использовании инструментов
- Занимает #1 место среди всех open-source моделей по суммарному индексу интеллекта (по данным Artificial Analysis)

### Кодирование (Программирование)
- Разработана для полного цикла разработки - от файловых правок до тестирования кода и его автокоррекции
- Показывает отличные результаты на Terminal-Bench и (Multi-)SWE-Bench
- Эффективна в IDE, терминалах и CI-системах

### Агентные возможности
- Умеет планировать и выполнять сложные цепочки действий через shell, браузер, retrieval и code runners
- В тестах BrowseComp уверенно находит труднодоступные источники и восстанавливается после сбоев, не теряя цепочку рассуждений
- Может использовать инструменты и планировать сложные цепочки действий

## Производительность и бенчмарки

MiniMax-M2 достигает впечатляющих результатов в различных бенчмарках:

### Кодирование и агентские задачи:
- **SWE-bench Verified**: 69.4 
- **Multi-SWE-Bench**: 36.2
- **Terminal-Bench**: 46.3
- **ArtifactsBench**: 66.8
- **BrowseComp**: 44
- **BrowseComp-zh**: 48.5

### Общие интеллектуальные способности:
- **AIME25**: 78
- **MMLU-Pro**: 82
- **GPQA-Diamond**: 78
- **LiveCodeBench (LCB)**: 83
- **AA Intelligence**: 61

## Применение

### Для разработчиков
- Поддержка полного цикла разработки
- Многофайловые правки
- Циклы кодинг-запуск-исправление
- Тестирование и проверка исправлений

### Для агентных систем
- Планирование сложных задач
- Веб-браузинг и поиск информации
- Восстановление после сбоев
- Интеграция с различными инструментами

## Преимущества

- **Эффективность**: Быстрее и дешевле, чем аналогичные по возможностям модели
- **Производительность**: Высокий уровень интеллекта при меньшем потреблении ресурсов
- **Масштабируемость**: Более эффективное планирование пропускной способности
- **Обратная связь**: Более быстрые циклы обратной связи благодаря меньшему размеру активных параметров

## Сравнение с другими моделями

MiniMax-M2 представляет новый стандарт эффективности для AGI-агентов и кодирования: умнее, быстрее и дешевле, чем аналоги. Благодаря архитектуре MoE и оптимизации для агентных рабочих процессов, она обеспечивает превосходное соотношение производительности и стоимости.

## Связи с другими темами

- [[ai/llm/architectures/mixture_of_experts_architecture.md]] - Подробнее об архитектуре Mixture of Experts, используемой в MiniMax-M2
- [[ai/llm/models/llm_architectures_comparison.md]] - Сравнение различных архитектур крупных языковых моделей
- [[ai/agents/ai_agents.md]] - Общая информация об ИИ-агентах, для которых оптимизирована модель
- [[programming/code_generation.md]] - Кодогенерация, один из ключевых сценариев использования MiniMax-M2
- [[ai/agents/terminal_bench_framework.md]] - Фреймворк Terminal-Bench, на котором тестируется модель

## Ссылки

- [Официальная страница модели на Hugging Face](https://huggingface.co/MiniMaxAI/MiniMax-M2)
- [Академические работы](https://arxiv.org/search/?query=MiniMax+M2)