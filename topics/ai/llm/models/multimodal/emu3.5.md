# Emu3.5 - Масштабная мультимодальная world-модель

## Общее описание

**Emu3.5** - это новая масштабная мультимодальная world-модель, разработанная BAAI (Beijing Academy of Artificial Intelligence), которая работает одновременно с двумя потоками данных: текстом и пикселями, предсказывая их совместное состояние на каждом шаге.

## Архитектура

### Native Multimodal Model
- **Unified World Modeling**: Предсказывает следующее состояние совместно для визуальных и языковых потоков, обеспечивая когерентное моделирование и генерацию мира
- **End-to-End Pretraining**: Обучена с унифицированной next-token предсказательной целью над чередующимися последовательностями "видео-язык"
- **Native Multimodal I/O**: Обрабатывает и генерирует чередующиеся визуально-текстовые последовательности без адаптеров модальностей или специфичных для задач головных модулей

### Обучение
- Обучена на более чем 10 триллионах чередующихся vision-language токенов из видеокадров и транскриптов
- Захватывает пространственно-временную структуру
- Дообучена с помощью методов подкрепления (Reinforcement Learning) для улучшения рассуждения, композициональности и качества генерации

## Ключевые особенности

### DiDA (Discrete Diffusion Adaptation) подход
- **Принцип работы**: Преобразует последовательное декодирование в параллельное двустороннее "денойзинг"-предсказание в дискретном пространстве токенов
- **Преимущество**: Обеспечивает примерно 20× более быстрый инференс без потери качества
- **Механизм**: Использует дискретные диффузионные адаптации для ускорения генерации

### Производительность
- Превосходит Nano Banana (Gemini 2.5 Flash Image) в задачах генерации, редактирования и интерливинга
- Высокое качество мультимодального рассуждения и генерации благодаря RL-дообучению
- Оптимизирована для задач длинного горизонта визуально-языковой генерации

## Применения

### Основные задачи
- Генерация изображений по текстовому описанию
- Редактирование изображений
- Интерливинговые задачи (чередование текста и изображений в одном потоке)
- Долгосрочное прогнозирование визуальных и текстовых последовательностей

### Возможности генерации
- X2I синтез (любое-в-изображение)
- Создание текстово-насыщенных изображений
- Пространственно-временно согласованное моделирование мира
- Открытая эмбодиментная манипуляция в виртуальном мире

## Сравнение с другими моделями

| Модель | Производительность | Особенности |
|--------|-------------------|-------------|
| Emu3.5 | Современный уровень | Native multimodal, DiDA, World modeling |
| Nano Banana (Gemini 2.5 Flash) | Высокий уровень | Быстрая мультимодальная обработка |
| Emu3 | Предыдущая версия | Менее оптимизированный инференс |

## Варианты модели

- **Emu3.5**: Полнофункциональная версия
- **Emu3.5-Image**: Специализированная версия для обработки изображений
- **Emu3.5-VisionTokenizer**: Специализированная версия для токенизации визуальных данных

## Технические требования

- Рекомендуется ≥2 GPU для лучшей пропускной способности
- Требует flash_attn==2.8.3 для оптимальной производительности
- Конфигурация через configs/config.py

## Связи с другими темами

- [[../../computer_vision/multimodal_models.md|Мультимодальные модели]] - Общие понятия о мультимодальных архитектурах
- [[../../computer_vision/world_models.md|World модели]] - Концепция world моделей, на которых основана Emu3.5
- [[../diffusion_models.md|Диффузионные модели]] - Теоретическая основа для подхода DiDA
- [[../../inference/multimodal_inference_optimization.md|Оптимизация мультимодального инференса]] - Техники ускорения инференса, включая DiDA
- [[../architectures/diffusion/diffusion_llm_architectures.md|Диффузионные LLM архитектуры]] - Сравнение с другими диффузионными подходами
- [[adamas_attention_mechanism.md|Механизмы внимания]] - Альтернативные архитектурные решения для обработки мультимодальных данных