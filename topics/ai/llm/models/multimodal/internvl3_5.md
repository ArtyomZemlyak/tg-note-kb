# InternVL3.5: Продвинутые опенсорс мультимодальные модели в универсальности, рассуждении и эффективности

## Общее описание

**InternVL3.5** - это новое поколение опенсорс-семейства мультимодальных моделей от разработчиков InternVL, которое устанавливает новые рекорды по результатам бенчмарков. Модель превосходит современные State-of-the-art (SoTA) результаты благодаря трем ключевым инновациям, которые значительно повышают универсальность, эффективность рассуждений и общую вычислительную эффективность.

## Три ключевых нововведения

### 1. Каскадное обучение с подкреплением (Cascade Reinforcement Learning)

#### Предыстория
- Ранее модели InternVL использовали MPO (Maximum A Posteriori Policy Optimization) в качестве offline RL (Offline Reinforcement Learning)
- В новой версии 3.5 авторы добавили online RL, который считается более эффективным для LLM/VLM, чем offline RL
- Однако offline RL значительно вычислительно легче (в основном потому, что во время обучения не нужно генерировать ответы на инструкции)

#### Результаты
- Авторы показали, что offline RL не так уж сильно отстает от online RL, но при этом обучается в 20 раз быстрее
- Лучшее качество модели достигается при совместном каскадном обучении: результаты лучше, чем у online RL, даже на двух эпохах
- Таким образом, offline RL превратился в warmup для online RL

#### GSPO (Group Sequence Policy Optimization)
- В качестве online RL используется GSPO — модификация GRPO (Group Relative Policy Optimization)
- GSPO решает проблему нестабильности обучения и "коллапса модели", особенно при тренировке Mixture-of-Experts-моделей
- GRPO работает на уровне отдельных токенов, создавая шумные градиенты
- GSPO применяет оптимизацию на уровне всей последовательности целиком, что важно для длинных цепочек рассуждений
- Это особенно критично для мультимодальных задач, требующих сложного пространственно-временного понимания

### 2. Visual Resolution Router (ViR)

#### Цель
- Основная цель этого нововведения — снизить вычислительную нагрузку на модель во время инференса
- Это достигается за счёт уменьшения количества визуальных токенов в представлении каждого кропа картинки
- Сколько токенов нужно выделить на кроп, решает роутер
- Среднее количество визуальных токенов, поступающих в LLM, при таком подходе сокращается на 50%

#### Процесс кодирования изображения
Стандартный процесс кодирования картинки выглядит так:
- Изображение делится на кропы
- Каждый патч преобразуется в 1024 токена для ViT (Vision Transformer)
- После обработки ViT количество токенов уменьшается адаптером до 256 и передаются в LLM
- Роутер может направить токены в более агрессивный адаптер и сжать до 64 токенов

#### Процесс обучения
Обучение происходит в два этапа:
1. **Первый этап**: Модель тренируется решать задачу с меньшим количеством токенов за счёт минимизации KL-дивергенции между распределениями выходных данных изначального сжатия и более агрессивного сжатия
2. **Второй этап**: Обучение самого роутера ViR принимать правильные решения о степени сжатия для каждого кропа. ViR обучается как стандартный бинарный классификатор, где label кропа определяется по значению loss из первого этапа

#### Результат
- Flash-модель практически без потери качества с ускорением до 4 раз (точная цифра зависит от разрешения картинки и размера модели)
- Это позволяет значительно снизить вычислительные затраты при сохранении высокой точности

### 3. DvD (Decoupled Vision-Language Deployment)

#### Принцип работы
- В этой системе модель для обработки изображений (ViT) и языковая модель (LLM) разворачиваются на отдельных серверах или GPU
- Они работают не последовательно (сначала картинка, потом текст), а параллельно
- Пока языковая модель генерирует ответ на предыдущий запрос, визуальный энкодер уже обрабатывает следующее изображение

#### Эффективность
- Это даёт ускорение до 2 раз для базовых моделей
- В комбинации с ViR — до 4 раз на высоких разрешениях
- По словам авторов, новая InternVL3.5 рассуждает на +16,0% эффективнее и в 4,05 раз быстрее, чем её предшественники
- Такой подход особенно эффективен при обработке потоков мультимодальных данных

## Архитектурные особенности

### Mixture-of-Experts (MoE)
- InternVL3.5 использует архитектуру Mixture-of-Experts для более эффективного использования вычислительных ресурсов
- Эта архитектура позволяет активировать только часть параметров модели для каждого конкретного запроса
- Облегчает масштабирование моделей без пропорционального увеличения вычислительных затрат

### Vision Transformer (ViT) компонент
- Обновленный ViT энкодер с улучшенной способностью обрабатывать детали на различных разрешениях
- Интеграция с ViR позволяет оптимизировать количество обрабатываемых визуальных токенов
- Сохраняет пространственную информацию для последующей обработки LLM

### Языковая модель (LLM) компонент
- Современная трансформерная архитектура с улучшенными возможностями рассуждения
- Интегрирована с визуальным энкодером через эффективные мультимодальные соединения
- Поддерживает длинные контексты и сложные рассуждения

## Производительность и эффективность

### Вычислительная эффективность
- Значительное снижение вычислительных затрат за счёт ViR и DvD
- Ускорение инференса до 4 раз при сохранении качества
- Более эффективное распределение нагрузки между GPU при параллельной обработке

### Качество рассуждений
- Повышенная эффективность рассуждений на 16% по сравнению с предыдущими версиями
- Улучшенная способность к визуально-языковым рассуждениям
- Лучшая обработка сложных сценариев с множественными объектами и отношениями

### Масштабируемость
- Модель более эффективно масштабируется с увеличением данных и вычислительных ресурсов
- Каскадное обучение с подкреплением улучшает сходимость при масштабировании
- Архитектура MoE позволяет эффективно масштабировать параметры модели

## Источники

- Разбор статьи о модели InternVL3.5 подготовлен Антоном Астаховым (CV Time) - информация, представленная в этой статье, основана на техническом разборе новой версии модели семейства InternVL

## Связи с другими темами

- [[../../computer_vision/multimodal_models.md|Мультимодальные модели]] - Общее понимание мультимодальных архитектур, включая CLIP, SigLIP и другие модели
- [[../../reinforcement_learning/reinforcement_learning_in_llms.md|Обучение с подкреплением в LLM]] - Теоретическая база для применения RL в обучении языковых и визуально-языковых моделей
- [[mixture_of_experts_architecture.md|Архитектура Mixture-of-Experts]] - Подробное описание MoE архитектуры, используемой в InternVL3.5
- [[../../computer_vision/vision_transformer.md|Vision Transformer]] - Архитектура ViT, лежащая в основе визуального энкодера модели
- [[deepseek_v3_2_reinforcement_learning_agent_training.md|Обучение агентов с подкреплением DeepSeek]] - Сравнение подходов к RL в мультимодальных моделях
- [[emu3.5.md|Emu3.5]] - Другая мультимодальная модель, использующая методы подкрепления для улучшения рассуждений
- [[../../efficiency/multimodal_inference_optimization.md|Оптимизация мультимодального инференса]] - Техники оптимизации, включая те, что используются в InternVL3.5 для ускорения инференса