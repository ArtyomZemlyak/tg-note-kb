# Эволюция семейства моделей Ming

## Обзор

Проект Ming начался как единая мультимодальная модель и развился в семейство моделей с различными характеристиками производительности и эффективности. Этот документ описывает эволюцию моделей Ming от оригинальной версии до текущего флагмана.

## Хронология развития

### 1. Ming-Omni (оригинальная версия)
- **Описание**: Первая универсальная мультимодальная модель от inclusionAI
- **Технический отчет**: Опубликован 12 июня 2025 года
- **Особенности**: Основала принципы унифицированного мультимодального восприятия и генерации

### 2. Ming-lite-omni Preview
- **Дата релиза**: 4 мая 2025 года
- **Описание**: Тестовая версия облегченной архитектуры
- **Особенности**: Первый шаг к созданию более эффективных версий модели

### 3. Ming-lite-omni v1
- **Дата релиза**: 28 мая 2025 года
- **Описание**: Официальная версия облегченной модели
- **Особенности**: Улучшенная производительность и поддержка генерации изображений

### 4. Ming-lite-omni v1.5
- **Дата релиза**: 15 июля 2025 года
- **Описание**: Обновленная версия с существенными улучшениями
- **Особенности**: Значительные улучшения по всем модальностям

### 5. Ming-flash-omni Preview (текущий флагман)
- **Дата релиза**: 27 октября 2025 года
- **Описание**: Флагманская модель, построенная на разреженном варианте Mixture-of-Experts (MoE) архитектуры Ling-Flash-2.0
- **Параметры**: 100 млрд общих параметров, 6 млрд активных параметров на токен
- **Особенности**: Наиболее продвинутая архитектура с двухбалансной маршрутизацией

## Архитектурные изменения

### От Lite к Flash
- **Ming-lite-omni** версии сосредоточены на повышении эффективности и доступности
- **Ming-flash-omni Preview** представляет собой переход к более крупным масштабам с использованием Sparse MoE архитектуры
- Каждое поколение внедряет улучшения в распознавании речи, генерации изображений и мультимодальном понимании

### Ключевые улучшения по версиям
- **v1**: Введение поддержки генерации изображений
- **v1.5**: Улучшения во всех модальностях
- **Flash Preview**: Революционные улучшения с использованием 100B-A6B MoE архитектуры

## Технические аспекты эволюции

### Масштабирование
- От простой архитектуры к Sparse Mixture-of-Experts
- Повышение эффективности путем активации только необходимых параметров (6B активных из 100B общих)

### Оптимизация
- Баланс между производительностью и вычислительными затратами
- Улучшенная маршрутизация экспертов для равномерной активации по всем модальностям

## Сравнение моделей

| Модель | Дата релиза | Архитектура | Особенности |
|--------|-------------|-------------|-------------|
| Ming-Omni | 2025.06.12 (по тех.отчету) | Оригинальная | Основа для семейства |
| Ming-lite-omni Preview | 2025.05.04 | Облегченная | Тестовая версия |
| Ming-lite-omni v1 | 2025.05.28 | Облегченная | Улучшенная производительность |
| Ming-lite-omni v1.5 | 2025.07.15 | Облегченная | Значительные улучшения |
| Ming-flash-omni Preview | 2025.10.27 | 100B-A6B MoE | Флагман, двухбалансная маршрутизация |

## Значение эволюции

Эволюция семейства моделей Ming демонстрирует стратегию company inclusionAI по постепенному совершенствованию мультимодальных возможностей:
- Сначала создание универсальной основы (Ming-Omni)
- Затем оптимизация для эффективности (Lite-версии)
- Наконец, масштабирование до передовых возможностей (Flash-версии)

Эта стратегия позволила inclusionAI разработать одну из самых передовых мультимодальных моделей на рынке, сочетающую высокую производительность с вычислительной эффективностью.

## Связи с другими темами

- [[ming.md]] - Общая информация о проекте Ming
- [[ming_flash_omni_preview.md]] - Детальная информация о флагманской модели
- [[speech_recognition_dialects.md]] - Технологии распознавания речи, улучшенные в последних версиях
- [[generative_models.md]] - Класс генеративных моделей, к которому относится Ming