# Методология обучения Qwen3-Omni

## Общий обзор

Обучение Qwen3-Omni проходит в несколько этапов, включающих предобучение (pretraining) и постобучение (post-training). Процесс включает три стадии предобучения и отдельные стадии для Thinker и Talker во время постобучения.

## Этапы предобучения

### 1. Стадия выравнивания энкодеров (Encoder Alignment Stage)

- Замораживается LLM (языковая модель)
- Обучаются только энкодеры и адаптеры для них
- Начинают именно с адаптеров
- Используется инициализация:
  - Для LLM: Qwen3
  - Для энкодера изображений: Qwen3-VL
  - Для энкодера аудио: новый аудиоэнкодер, обученный ранее

### 2. Стадия мультимодального обучения

- Все параметры размораживаются
- Добавляются более разнообразные мультимодальные данные и задачи

### 3. Стадия расширения контекста

- Увеличивается контекстное окно с 8192 до 32768 токенов
- Модель может обрабатывать длинные входы
- В данные добавляются более длинные аудио/видео

## Постобучение (Post-training)

Постобучение разделено для Thinker и Talker.

### Постобучение для Thinker

В отличие от Qwen2.5-Omni, теперь добавлены дополнительные стадии:

1. **SFT (supervised fine-tuning)** - Основная стадия тонкой настройки
2. **Дистилляция** - Используется для получения более компактных LLM
   - Применяется подход Strong-to-Weak Distillation из Qwen3
3. **RL (GSPPO)** - Обучение с подкреплением
   - Оценивается качество отклика модели
   - Для задач с чёткими критериями (mathematics, coding) применяются награды, которые вычисляются по заранее заданным правилам
   - Для остальных задач, где сложно сформулировать чёткую награду, используется подход LLM-as-a-judge, где для оценки ответа модели используются Qwen3 и Qwen2.5-VL

### Постобучение для Talker

Ранее было три стадии, теперь - четыре:

1. **Предварительное обучение** - На большом объёме данных с мультимодальным контекстом
2. **Добавление качественных данных** - Для борьбы с галлюцинациями после первой стадии
3. **DPO (Direct Preference Optimization)** - Оптимизация на основе предпочтений
4. **Speaker Fine-Tuning** - Настройка, чтобы Talker научился копировать тембр и интонации во время генерации аудио

## Особенности методологии

### Асинхронный prefilling

- Как только Thinker заканчивает prefilling для текущего чанка, его выходы отдаются в Talker, чтобы он тоже мог начать prefilling
- При этом Thinker уже начинает обрабатывать следующий чанк

### Left-context only для генерации аудио

- Используется только левый контекст для генерации аудио
- В отличие от Qwen2.5-Omni, где создавалась задержка из-за необходимости накопить немного правого контекста

## Открытый источник

- Исследователи выпустили в опенсорс Qwen3-Omni-30B-A3B-Captioner - модель для решения задачи audio captioning на основе Qwen3-Omni-30B-A3B

## Связь с другими компонентами

[[qwen3-omni.md|Qwen3-Omni]] - Общая информация о модели
[[qwen3-omni-audio-processing.md|Обработка аудио в Qwen3-Omni]] - Подробная информация о возможностях аудиообработки
[[qwen3-omni-video-processing.md|Обработка видео в Qwen3-Omni]] - Подробная информация о видеообработке