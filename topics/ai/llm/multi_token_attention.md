# Multi-Token Attention (MTA)

## Общее описание

Multi-Token Attention (MTA) - это новая архитектурная техника для улучшения механизмов внимания в трансформерах, разработанная для решения проблемы "бутылочного горлышка" в стандартных механизмах внимания. В отличие от классического подхода, где веса внимания определяются одним вектором запроса (query) и одним вектором ключа (key), MTA позволяет определять веса внимания на основе нескольких векторов query и key одновременно.

## Проблема, которую решает MTA

В стандартном механизме внимания веса определяются формулой softmax(QK/√d), где для каждого токена есть вектор эмбеддинга, который проецируется в три отдельных вектора: Q (запрос), K (ключ) и V (значение). Это создает "бутылочное горлышко", ограничивающее способность модели эффективно комбинировать информацию из нескольких токенов.

Например, если модель ищет предложение, содержащее несколько элементов (например, "Алиса" и "кролик" в запросе "Where did Alice see the rabbit?"), стандартный механизм внимания не может эффективно комбинировать веса внимания для этих элементов в пределах одного слоя, так как нет взаимодействий между отдельными attention maps.

## Архитектурные компоненты

MTA включает три ключевых компонента:

### 1. Key-Query Convolution (Конволюция ключ-запрос)

Комбинирует несколько векторов key и query внутри головы внимания. Применяет двумерную обучаемую свёртку к логитам внимания перед softmax (QK/√d), с измерениями q и k. Каждая голова внимания учит свою свёртку. Используется маска, запрещающая "заглядывать" в будущее.

- **Pre-softmax convolution** (по умолчанию): мультипликативные взаимодействия
- **Post-softmax convolution**: аддитивные взаимодействия

### 2. Head Mixing Convolution (Конволюция смешивания голов)

Позволяет перемешивать информацию между разными головами внимания в пределах одного временного шага. Все головы разбиваются на группы заданного размера, и перемешивание происходит внутри группы. Может быть использован как до, так и после softmax (по умолчанию после).

### 3. Group Normalization with Depth Scaling (Групповая нормализация с масштабированием по глубине)

Использует GroupNorm и независимое масштабирование для каждой головы, следуя рецепту от Differential Transformer. Улучшает поток градиентов.

## Варианты реализации

Возможны четыре варианта блока MTA с разными комбинациями pre/post сверток:
- Pre-softmax + Pre-head mixing
- Pre-softmax + Post-head mixing  
- Post-softmax + Pre-head mixing
- Post-softmax + Post-head mixing

Если оба типа сверток являются pre или post, их можно объединить в одну трехмерную свертку.

## Эксперименты и результаты

### Игрушечная задача

Модели дают последовательность блоков из N случайных букв, затем L<N букв вопроса. Задача - найти блок, содержащий все буквы из вопроса в любом порядке.

Пример: `hjnvt.qfjgt.whftb.bjtpq. ...(many blocks)... .pxjvf.ulhik.qoiax#pb` (нужно найти блок с pb)

Для N=5 и 8, L=2: MTA показывает почти нулевую ошибку, в то время как обычный трансформер показывает двузначные проценты.

### LLM эксперименты

- Модель: 880M параметров, архитектура LLaMa
- Данные: SlimPajama, 105B токенов
- В MTA: key-query convolution использовалась в каждом четвертом слое, head convolution в каждом
- Размеры сверток: c_q=6, c_k=11, размер группы 2

MTA показывает лучшую перплексию (при этом GroupNorm важен). На бенчмарках в среднем лучше, чем остальные, хотя разница часто в последней цифре.

### Задачи с долгосрочными зависимостями

- LAMBADA: MTA однозначно превосходит
- NeedleIn-A-Haystack: точность MTA обычно выше
- BabiLong: хорошие результаты на QA1-5

## Визуализации и паттерны

- Множество сверточных ядер близки к identity
- Некоторые имеют диагональную структуру для поиска точного совпадения с паттерном
- Другие аналогичны edge detection, усиливают первый или последний из последовательных ключей с высоким вниманием
- В свертках по головам частый паттерн - контраст, вычитание одной головы из другой

## Абляции

- Даже нескольких слоев MTA достаточно для превосходства над бейзлайнами
- Все предложенные компоненты улучшают перплексию

## Преимущества и недостатки

### Преимущества

- Лучшая способность комбинировать информацию из нескольких токенов
- Улучшенные результаты на задачах с долгосрочными зависимостями
- Более точное моделирование взаимодействий между разными аспектами входных данных

### Недостатки

- Требует в 3 раза больше памяти
- В 5 раз больше FLOPS (в текущей неоптимизированной реализации)
- Это результат отсутствия оптимизированного ядра для CUDA

## Сравнение с другими подходами

| Механизм | Особенности | Сложность | Преимущества |
|----------|-------------|-----------|--------------|
| Стандартное MHA | Абсолютные значения Q/K | O(n²) | Высокое качество |
| Multi-Query Attention | Общие K/V для всех голов | O(n²) | Экономия памяти |
| Multi-Token Attention | Конволюции между Q/K, головами | O(n²) | Комбинирование информации из токенов |
| Linear Attention | Приближения для O(n) | O(n) | Эффективность |
| Sliding Window | Ограниченное окно | O(n*w) | Локальный контекст |

## Применение

- Задачи, требующие сопоставления нескольких элементов в последовательности
- Долгосрочные зависимости в тексте
- Модели, где важно комбинировать информацию из различных частей контекста

## Связи с другими темами

- [[specialized_attention_mechanisms.md]] - Другие модифицированные механизмы внимания
- [[../nlp/transformers/differential_transformer.md]] - Другой подход к модификации механизма внимания, улучшающий понимание относительных отношений между токенами; содержит информацию о связи с Multi-Token Attention
- [[adamas_attention_mechanism.md]] - Еще один механизм, оптимизирующий вычисления внимания за счет разреженности
- [[log_linear_attention.md]] - Альтернативный подход к эффективному вниманию с логарифмически-линейной сложностью
- [[multi_head_latent_attention.md]] - Механизм, уменьшающий размер KV-кеширования за счет низкоранговых приближений
- [[sliding_window_attention.md]] - Ограниченное окно внимания, подход к эффективности для длинных последовательностей

## Источники

- Оригинальная статья: "Multi-Token Attention" (https://arxiv.org/abs/2504.00927), авторы: Olga Golovneva, Tianlu Wang, Jason Weston, Sainbayar Sukhbaatar