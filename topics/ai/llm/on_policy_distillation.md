# On-Policy Distillation (Дистилляция по политике)

## Определение

On-Policy Distillation — это метод дистилляции знаний, при котором обучение модели-ученика происходит в процессе генерации примеров, которые затем проверяются моделью-учителем. Обучение происходит только в тех случаях, когда учитель не согласен с выводами ученика.

## Основные особенности

- **Активное участие учителя**: Примеры, сгенерированные учеником, проходят проверку учителем
- **Селективное обучение**: Модель-ученик обучается только в местах, где есть расхождение с учителем
- **Проблема словарей**: Классические методы дистилляции требуют одинаковые словари токенизатора у учителя и ученика

## Отличие от традиционной дистилляции

В отличие от традиционной дистилляции знаний (как в оригинальной статье Хинтона), где используются логиты учителя напрямую, on-policy distillation:

- Преобразует задачу прямой дистилляции в задачу критика
- Использует динамическую проверку согласия между учителем и учеником
- Позволяет использовать модели с разными словарями токенизатора

## Сравнение с другими методами

- **Speculative decoding**: Сходство есть, но on-policy distillation работает в "обратном" направлении
- **Reinforcement Learning**: Использует аналогичные концепции on-policy и off-policy алгоритмов
- **KL-дивергенция**: Можно использовать для измерения несогласованности между моделями

## Применение

On-policy distillation особенно полезна:
- При обучении маленьких моделей с помощью больших учителей
- При работе с моделями, имеющими разные архитектуры и словари
- В сценариях, где важна точность конкретных выводов

## Связи с другими темами

- [[knowledge_distillation.md]] - основная концепция дистилляции знаний
- [[speculative_decoding.md]] - метод, с которым сравнивают on-policy distillation
- [[reinforcement_learning.md]] - концепции on-policy и off-policy алгоритмов