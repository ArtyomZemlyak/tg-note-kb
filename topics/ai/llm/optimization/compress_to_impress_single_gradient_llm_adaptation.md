# Compress to Impress: Адаптация LLM за один шаг градиентного спуска

## Краткое описание

Compress to Impress - это новая методика эффективной адаптации больших языковых моделей (LLM) к новым доменам, которая не требует традиционного обучения. Метод основан на технике LAyer-SElective-Rank reduction (LASER), но решает её главное узкое место: медленный полный перебор весовых матриц, которые нужно сжать. Вместо этого авторы заменяют этот перебор одним-единственным обратным проходом всего по 100 размеченным примерам. Градиент сингулярных значений каждой матрицы используется для надёжной оценки и определения компонентов, которые вредят модели и которые следует подвергнуть прунингу. Кроме того, авторы улучшают качество сжатия, вводя многоподпространственную факторизацию, аппроксимированную простой эвристикой «разбиения на блоки».

## Основная информация

Compress to Impress (BLOCK-FIRST GRADIENT LOW-RANK ADAPTATION) представляет собой методику адаптации LLM, которая основана на идее понижения ранга (low-rank adaptation) определённых весовых матриц модели без необходимости повторного обучения. Ключевое нововведение заключается в использовании градиентов сингулярных значений для определения компонентов, которые вредят модели на новой задаче, и последующего удаления этих компонентов.

Метод значительно ускоряет процесс адаптации по сравнению с оригинальным LASER, достигая ускорения до 52 раз на GPT-J и 22.2 раза на Roberta, при этом сохраняя или даже улучшая точность. Это делает адаптацию LLM значительно более доступной и практичной, сводя процесс к операции, занимающей минуты на одной GPU.

### Основные проблемы, которые решает метод

- **Высокие вычислительные затраты традиционных методов адаптации**: Полный файнтюнинг и даже параметрически-эффективные методы (LoRA) требуют нескольких шагов градиентного спуска.
- **Медленный перебор в методе LASER**: Оригинальный LASER требовал полного послойного перебора, включающего прямые проходы по всему датасету для каждой матрицы-кандидата, что делало его непрактичным для быстрого развёртывания.
- **Необходимость большого количества данных**: Традиционные методы адаптации требуют больших датасетов для эффективной настройки.
- **Отсутствие направленности в прунинге**: Стандартные методы прунинга могут не учитывать специфику новой задачи.

### Решения и методы

1. **Выбор матриц на основе градиента**
   Вместо полного перебора метод определяет кандидатов на понижение ранга с помощью одного обратного прохода. Ключевая идея в том, что градиент функции потерь по сингулярным значениям матрицы даёт мощный сигнал о том, какие её компоненты вредны для новой задачи.

   Для весовой матрицы W с её сингулярным разложением (SVD) W = UΣVᵀ и обычным матричным градиентом G = ∂L/∂W, градиент лосса по i-му сингулярному значению σᵢ элегантно вычисляется как:
   
   ∂L/∂σᵢ = uᵢᵀGvᵢ

   Эта формула имеет ясную интуитивную трактовку: она напрямую измеряет, насколько градиент лосса G сонаправлен с компонентом ранга один (uᵢvᵢᵀ), связанным с этим сингулярным значением. Отрицательное значение указывает, что лосс-функция "хочет" уменьшить этот компонент, что даёт точный, целенаправленный сигнал о том, какие части структуры модели вредны и должны быть подвергнуты прунингу. Суммируя отрицательные градиенты, связанные с наименьшими сингулярными значениями (в частности, последними двадцатью), авторы создают надёжную метрику для ранжирования всех матриц. Это позволяет им сосредоточить усилия по сжатию только на нескольких лучших кандидатах, достигая огромного ускорения.

2. **Радикальная эффективность по числу примеров**
   Весь процесс — и вычисление градиентов для выбора матриц, и оценка итоговой сжатой модели — можно надёжно выполнить, используя всего 100 размеченных примеров. В чём же секрет? Оказывается, адаптация LLM часто связана не столько с изучением новых статистических распределений из больших датасетов, сколько с подстройкой под стиль промптинга, форматирование и структуру ответов целевого домена. Эти стилистические паттерны часто повторяются, а значит, необходимые градиентные сигналы насыщаются очень быстро. Именно этот принцип и позволяет методу достичь своей поразительной эффективности, превращая адаптацию в операцию, которую можно выполнить за минуты на одной GPU.

3. **Удаление шума в нескольких подпространствах**
   Для дальнейшего повышения производительности в статье вводится более сложная техника факторизации. Стандартный подход LASER применяет одно SVD ко всей весовой матрице, предполагая единую глобальную низкоранговую структуру. Это может быть слишком грубым подходом, поскольку предобученные матрицы часто содержат неоднородные структуры (например, одни строки могут быть связаны с синтаксисом, другие — с семантикой).

   Хотя оптимальное решение, проективная кластеризация (projective clustering), вычислительно сложна, авторы применяют эвристику с почти нулевыми затратами: разбиение на блоки (block splitting). Этот удивительно эффективный метод просто разрезает строки весовой матрицы на несколько последовательных блоков и применяет к каждому из них независимое понижение ранга на основе SVD, не требуя сложного алгоритма кластеризации. Это обогащает пространство поиска и позволяет более целенаправленно удалять шум, что приводит к значительному улучшению точности.

## Эксперименты и результаты

Авторы тестируют свой подход на GPT-J и Roberta на восьми различных бенчмарках. Результаты впечатляют:

- **Производительность и скорость**: Наиболее эффективная конфигурация (CL-100G-100E) достигает среднего ускорения в 52 раза на GPT-J и в 22.2 раза на Roberta по сравнению с оригинальным методом LASER. При этом компромисс в производительности незначителен, а по точности метод часто даже превосходит LASER.

- **Влияние многоподпространственной факторизации**: Мощь этого подхода ярко проявляется в задаче BigBench-Epistemic Reasoning, где использование кластеризованной факторизации повышает точность GPT-J с 38.3% (LASER) до 62.9%, что составляет огромный прирост в 24.6 процентных пункта. Это подтверждает, что более гранулярное, локализованное сжатие — ключ к лучшей обобщающей способности.

- **Надёжные абляции**: В статье представлен подробный набор абляций, которые систематически подтверждают ценность каждого компонента метода. Эти исследования по отдельности подтверждают как выигрыш от многоподпространственной факторизации, так и жизнеспособность протокола со 100 примерами, подкрепляя обоснованность всего подхода.

## Новые концепции и термины

- **BLOCK-FIRST GRADIENT LOW-RANK ADAPTATION**: Название предложенного метода, который использует один шаг градиентного спуска для определения матриц, подлежащих понижению ранга.
- **LAyer-SElective-Rank reduction (LASER)**: Оригинальная методика, на которой основан Compress to Impress, предполагающая понижение ранга определённых весовых матриц без повторного обучения.
- **Градиент сингулярных значений**: Метрика, используемая для определения компонентов весовой матрицы, которые вредны для новой задачи; вычисляется как ∂L/∂σᵢ = uᵢᵀGvᵢ.
- **Разбиение на блоки (block splitting)**: Эвристика для многоподпространственной факторизации, при которой строки весовой матрицы разбиваются на блоки и к каждому применяется независимое понижение ранга.
- **Структурированный прунинг**: Метод, при котором удаляются не отдельные веса, а абстрактные структурные компоненты (сингулярные векторы), руководствуясь сигналом от конкретной задачи.
- **Стилистическое выравнивание**: Процесс подстройки модели под стиль промптинга, форматирование и структуру ответов целевого домена, а не изучения новых статистических закономерностей.

## Примеры применения

- **Быстрая адаптация LLM к новым доменам**: Адаптация к конкретным задачам без длительного обучения.
- **Развёртывание на устройствах с ограниченными ресурсами**: Быстрая настройка моделей для работы на мобильных устройствах или edge-устройствах.
- **Быстрое прототипирование**: Быстрая проверка применимости модели к новой задаче.
- **Использование в условиях ограниченных вычислительных ресурсов**: Применение метода на системах с ограниченной вычислительной мощностью.
- **Ограниченные данные**: Адаптация модели, когда размеченные данные ограничены, но необходимо адаптировать модель к домену.

## Связи с другими темами

- [[laser_layer_selective_rank_reduction.md]] - Оригинальный метод, на котором основан Compress to Impress, с понижением ранга без обучения
- [[../pruning/structured_pruning.md]] - Общий подход к структурированному прунингу, частью которого являются методы понижения ранга
- [[../../optimization/memory_efficient_training.md]] - Связанная концепция параметрически эффективного обучения, включающая LoRA, сравнимую по эффективности с Compress to Impress
- [[../model_quantization_techniques.md]] - Другой подход к оптимизации LLM, направленный на снижение вычислительных затрат
- [[../rlhf.md]] - Традиционный метод адаптации LLM, с которым можно сравнить эффективность Compress to Impress
- [[../../nlp/transformers/transformer_architecture.md]] - Архитектура, к которой применяются методы понижения ранга

## Ссылки на источники

- Основная статья: Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples
- Авторы: Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus
- Paper: https://arxiv.org/abs/2510.20800
- Review: https://arxiviq.substack.com/p/compress-to-impress-efficient-llm