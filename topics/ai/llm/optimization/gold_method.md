# General On-Policy Logit Distillation (GOLD)

## Определение

General On-Policy Logit Distillation (GOLD) — это усовершенствованный метод дистилляции знаний по политике, который решает фундаментальную проблему традиционных методов: необходимость использования одинаковых словарей токенизатора у моделей-учителя и ученика.

## Основные особенности

- **Решение проблемы словарей**: Метод позволяет использовать модели с разными словарями токенизатора
- **Обобщенный подход**: Метод является обобщением on-policy distillation
- **Логит-ориентированность**: Основывается на сравнении логитов, но с учетом различных словарей

## Принцип работы

GOLD работает по следующей схеме:

1. Модель-ученик генерирует примеры
2. Эти примеры проверяются моделью-учителем
3. Обучение происходит только в тех местах, где учитель не согласен с учеником
4. Используются специальные методы для согласования выходов с разными словарями

## Преимущества

- **Гибкость**: Возможность использования моделей разных архитектур и словарей
- **Эффективность**: Селективное обучение экономит вычислительные ресурсы
- **Совместимость**: Решает проблему несовместимости словарей в традиционной дистилляции

## Отличия от традиционных методов

- Традиционная дистилляция Хинтона использует логиты учителя напрямую
- GOLD преобразует задачу прямой дистилляции в задачу критика
- Вместо простого суммирования логпроб, используется более сложная схема оценки согласия

## Сравнение с другими методами

- **Сходство с speculative decoding**: Оба метода используют критические оценки, но в разном направлении
- **KL-дивергенция**: Может использоваться для измерения несогласованности между моделями
- **Compute as Teacher (CaT)**: Использует аналогичные концепции динамической проверки

## Применение

GOLD особенно эффективен:
- При дистилляции знаний между моделями с разными словарями
- При ограничениях на вычислительные ресурсы (селективное обучение)
- В сценариях, где важна адаптация больших моделей к специфическим задачам

## Связи с другими темами

- [[on_policy_distillation.md]] - общая концепция дистилляции по политике
- [[knowledge_distillation.md]] - основы дистилляции знаний
- [[tokenization_strategies.md]] - проблемы, связанные с различными словарями
- [[model_optimization.md]] - оптимизация моделей, включающая дистилляцию