# General On-Policy Logit Distillation (GOLD)

## Определение

General On-Policy Logit Distillation (GOLD) — это усовершенствованный метод дистилляции знаний по политике, который решает фундаментальную проблему традиционных методов: необходимость использования одинаковых словарей токенизатора у моделей-учителя и ученика.

## Основные особенности

- **Решение проблемы словарей**: Метод позволяет использовать модели с разными словарями токенизатора
- **Обобщенный подход**: Метод является обобщением on-policy distillation
- **Логит-ориентированность**: Основывается на сравнении логитов, но с учетом различных словарей
- **Комбинация методов**: Сочетает on-policy и off-policy подходы

## Принцип работы

GOLD работает по следующей схеме:

1. Модель-ученик генерирует примеры
2. Эти примеры проверяются моделью-учителем
3. Обучение происходит только в тех местах, где учитель не согласен с учеником
4. Используются специальные методы для согласования выходов с разными словарями

## Три ключевых преимущества GOLD

1. **Сложение логпроб для неалайнящихся токенов**: Вместо маскирования неалайнящихся токенов как в CDM, GOLD складывает логпробы этих токенов и приближает их. Логпроба - это логарифм вероятности, так что сумма логпроб это произведение вероятностей - общая вероятность последовательности.

2. **Смешение KLD и ULD для совпадающих и несовпадающих токенов**: Вместо дистилляции всех токенов через Вассерштейна как в ULD, GOLD дистиллирует совпадающие токены через KLD (как в обычной Soft-Label дистилляции) с фоллбеком до ULD для несовпадающих токенов. При мисматче размеров несовпадающих токенов токенизатор добивается нулями и сортируется.

3. **Комбинация on-policy и off-policy как в GKD**: В отличие от ULD, которая является off-policy, GOLD включает комбинацию on-policy и off-policy методов.

## Преимущества

- **Гибкость**: Возможность использования моделей разных архитектур и словарей
- **Эффективность**: Селективное обучение экономит вычислительные ресурсы
- **Совместимость**: Решает проблему несовместимости словарей в традиционной дистилляции
- **Высокое качество**: Показывает значительно лучшие результаты по сравнению с предыдущими методами

## Результаты экспериментов

- При дистилляции Qwen-3-4B-Instruct-2507 в Qwen-2.5-1.5B-Instruct (почти совпадающие токенизаторы) качество при on-policy дистилляции возрастает по сравнению с off-policy дистилляцией
- На The Countdown Game с полной on-policy дистилляцией достигается 92% от качества учителя
- При дистилляции Qwen-3-4B в Llama-3.2-1B (0.64 совпадающих токенов) GOLD показывает 59% качества учителя в отличие от 9% качества учителя при ULD
- Сравнение с GRPO показывает, что дистилляция из сильного учителя работает на 200% лучше, чем GRPO

## Отличия от традиционных методов

- Традиционная дистилляция Хинтона использует логиты учителя напрямую
- GOLD преобразует задачу прямой дистилляции в задачу критика
- Вместо простого суммирования логпроб, используется более сложная схема оценки согласия
- Может работать с моделями, у которых токенизаторы совпадают только на 40-50%

## Возможные улучшения

- Сочетание с идеями из Speculative Knowledge Distillation может позволить динамически управлять пропорцией лямбдой и получить еще несколько процентов качества
- Необходимо решить вычислительную сложность - логиты более ресурсоемкие, чем текст

## Сравнение с другими методами

- **Сходство с speculative decoding**: Оба метода используют критические оценки, но в разном направлении
- **KL-дивергенция**: Может использоваться для измерения несогласованности между моделями
- **Compute as Teacher (CaT)**: Использует аналогичные концепции динамической проверки
- **Universal Logit Distillation (ULD)**: Также решает проблему разных токенизаторов, но менее эффективно
- **Contextual Dynamical Mapping (CDM)**: Использует выравнивание токенов, но маскирует неалайнящиеся токены

## Применение

GOLD особенно эффективен:
- При дистилляции знаний между моделями с разными словарями
- При ограничениях на вычислительные ресурсы (селективное обучение)
- В сценариях, где важна адаптация больших моделей к специфическим задачам
- При дистилляции с восстановлением general capabilities после специализации

## Связи с другими темами

- [[on_policy_distillation.md]] - общая концепция дистилляции по политике
- [[knowledge_distillation.md]] - основы дистилляции знаний
- [[universal_logit_distillation.md]] - предшественник метода GOLD
- [[contextual_dynamical_mapping.md]] - методы выравнивания токенов
- [[tokenization_strategies.md]] - проблемы, связанные с различными словарями
- [[model_optimization.md]] - оптимизация моделей, включающая дистилляцию
- [[distillation_challenges.md]] - проблемы, которые решает GOLD
- [[optimization/structured_pruning.md]] - альтернативный подход к оптимизации моделей
- [[model_quantization_techniques.md]] - еще один метод оптимизации моделей
- [[reinforcement_learning_in_llms.md]] - концепции on-policy и off-policy алгоритмов

## Источники

1. [GOLD: General On-Policy Logit Distillation](https://arxiv.org/abs/) - оригинальная статья о методе GOLD
2. [Huggingface Blog Post on GOLD](https://huggingface.co/blog/gold) - подробное объяснение метода
3. [Code Repository for GOLD](https://github.com/huggingface/trl/tree/main/trl/trainer) - реализация GOLD метода в TRL (экспериментальная ветка)