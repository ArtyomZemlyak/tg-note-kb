# LASER: LAyer-SElective-Rank Reduction

## Краткое описание

LASER (LAyer-SElective-Rank reduction) - это методика адаптации больших языковых моделей (LLM), которая заключается в понижении ранга определённых весовых матриц модели без необходимости повторного обучения. Метод показал, что простое понижение ранга определённых весовых матриц может повысить точность на последующих задачах, но требовал дорогостоящего послойного перебора для определения, какие матрицы подвергать сжатию.

## Основная информация

LASER (LAyer-SElective-Rank reduction) представляет собой методику адаптации LLM, которая основана на идее понижения ранга (low-rank adaptation) определённых весовых матриц модели без необходимости повторного обучения. Метод предполагает, что в предобученных LLM содержатся компоненты, которые могут мешать производительности на специфических задачах, и снижение ранга этих компонентов может улучшить результаты.

Оригинальный LASER подходил к проблеме понижения ранга, но страдал от одного главного недостатка: он требовал полного послойного перебора, включающего прямые проходы по всему датасету для каждой матрицы-кандидата, что делало его непрактичным для быстрого развёртывания. Это ограничение было решено в более поздней работе "Compress to Impress", которая заменила перебор одним шагом градиентного спуска.

### Основные проблемы, которые решает метод

- **Высокие вычислительные затраты традиционных методов адаптации**: Полный файнтюнинг требует значительных вычислительных ресурсов.
- **Необходимость повторного обучения**: Большинство методов адаптации требуют обучения модели на новых данных.
- **Сложность интерпретации**: Понимание того, какие компоненты модели мешают производительности на новых задачах.

### Решения и методы

1. **Послойный перебор**
   Оригинальный LASER подход использовал полный перебор весовых матриц, включающий прямые проходы по всему датасету для каждой матрицы-кандидата. Для каждой весовой матрицы W выполнялось сингулярное разложение (SVD) и затем применялось понижение ранга, чтобы определить, какое влияние это окажет на производительность модели.

2. **Понижение ранга**
   Метод заключается в том, что весовые матрицы модели разлагаются с помощью сингулярного разложения: W = UΣVᵀ, где Σ - диагональная матрица сингулярных значений. Затем для уменьшения сложности модели и потенциального улучшения обобщающей способности, наименьшие сингулярные значения устанавливаются в ноль (или уменьшаются), что фактически понижает ранг матрицы.

3. **Оценка эффективности**
   Каждая модифицированная матрица оценивалась на задаче, и выбирались те модификации, которые приводили к улучшению производительности. Этот подход позволял определить, какие компоненты модели являются "шумовыми" и стоит от них избавиться.

## Эксперименты и результаты

Хотя оригинальная статья LASER не была подробно описана в источнике, известно, что метод показал эффективность в повышении точности на различных задачах. Однако, из-за вычислительно затратного подхода с полным перебором, его применение было ограничено.

## Новые концепции и термины

- **LAyer-SElective-Rank reduction (LASER)**: Методика понижения ранга определённых весовых матриц в LLM без повторного обучения.
- **Понижение ранга**: Процесс уменьшения эффективного числа параметров в весовой матрице путем уменьшения или исключения наименьших сингулярных значений.
- **Сингулярное разложение (SVD)**: Математическая операция, разлагающая матрицу на три компонента W = UΣVᵀ, где U и V - ортогональные матрицы, а Σ - диагональная матрица сингулярных значений.
- **Послойный перебор**: Метод определения, какие весовые матрицы подвергать понижению ранга, через полную проверку всех потенциальных кандидатов.

## Примеры применения

- **Адаптация LLM к новым доменам**: Понижение ранга для улучшения производительности на специфических задачах.
- **Оптимизация моделей без обучения**: Улучшение характеристик модели без необходимости повторного обучения.
- **Исследование внутренней структуры LLM**: Понимание, какие компоненты модели важны для различных задач.

## Связи с другими темами

- [[compress_to_impress_single_gradient_llm_adaptation.md]] - Улучшенная версия LASER, которая решает проблему вычислительной сложности
- [[../pruning/structured_pruning.md]] - Общий подход к структурированному прунингу, частью которого является LASER
- [[../../optimization/memory_efficient_training.md]] - Связанная концепция параметрически эффективного обучения
- [[../model_quantization_techniques.md]] - Другой подход к оптимизации LLM, направленный на снижение вычислительных затрат

## Ссылки на источники

- Оригинальная статья: LASER: LAyer SElective Rank Reduction (Low-Rank Adaptation Without Training)
- Paper: https://arxiv.org/abs/2312.13558