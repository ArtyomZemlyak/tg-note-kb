# Структурированный прунинг в LLM

## Описание

Структурированный прунинг (structured pruning) - это метод оптимизации больших языковых моделей, при котором удаляются целые структурные компоненты модели (например, нейроны, каналы, головы внимания), а не отдельные веса. В отличие от неструктурированного прунинга, структурированный прунинг создает более регулярную архитектуру модели и часто более совместим с аппаратными ускорителями.

## Принцип работы

### Основные подходы

1. **Прунинг по модулям**: Удаление целых компонентов архитектуры (слоёв, голов внимания, подсетей)
2. **Прунинг по каналам**: Удаление целых каналов передачи информации в модели
3. **Прунинг по фильтрам**: В случае сверточных компонентов в архитектуре - удаление целых фильтров
4. **Прунинг по критериям важности**: Использование различных метрик для определения важности структурных компонентов

### Критерии важности

- **Магнитуда весов**: Меньшие веса считаются менее важными
- **Норма активацций**: Компоненты с низкой активацией считаются менее важными
- **Гессиан-основанные метрики**: Использование второй производной для оценки важности
- **Информационные метрики**: Оценка важности на основе количества информации

## Сравнение с другими методами

- **Неструктурированный прунинг**: Удаляет отдельные веса, требует специальных библиотек для ускорения
- **Квантования**: Уменьшает битность представления весов
- **Knowledge distillation**: Создает меньшую "студенческую" модель на основе большой "учительской"
- **LoRA**: Обновляет только небольшое количество параметров, но не удаляет их

## Связь с внутренней размерностью

Структурированный прунинг тесно связан с концепцией внутренней размерности (intrinsic dimensionality):

- Обе концепции подтверждают, что LLM обладают значительным избыточным параметризованием
- Прунинг удаляет избыточные структурные компоненты, в то время как внутренняя размерность показывает, что обучение происходит в низкоразмерном подпространстве
- После прунинга модели могут сохранять способность к эффективному обучению в малых подпространствах

## Преимущества

- **Улучшенная вычислительная эффективность**: Удаление целых компонентов позволяет ускорить инференс
- **Снижение памяти**: Меньше параметров требуют меньше памяти
- **Аппаратная совместимость**: Более предсказуемая производительность на существующем оборудовании
- **Регулярная структура**: Упрощает анализ и оптимизацию архитектуры

## Вызовы и ограничения

- **Устойчивость к прунингу**: Некоторые структурные компоненты критичны для функциональности
- **Поиск оптимальной структуры**: Требует тщательного анализа и экспериментов
- **Баланс точности и эффективности**: Сложный компромисс между производительностью и качеством

## Связи с другими темами

- [[ai/llm/intrinsic_dimensionality.md]] - Концепция внутренней размерности, объясняющая возможность сокращения параметов
- [[ai/llm/lora_optimization.md]] - Альтернативный подход к параметрически эффективному обучению
- [[ai/optimization/combinatorial_optimization.md]] - Методы оптимизации структуры модели
- [[ai/llm/model_quantization_techniques.md]] - Другой подход к оптимизации LLM
- [[ai/llm/on_policy_distillation.md]] - Метод дистилляции знаний, использующий согласие между моделями-учителем и учеником