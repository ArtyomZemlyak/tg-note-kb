# Техники эффективного обучения небольших моделей

## Краткое описание

Этот файл описывает конкретные техники и методы, которые позволяют эффективно обучать небольшие языковые модели, как описано в Smol Training Playbook от HuggingFaceTB.

## Основная информация

### 1. Обучение с дистилляцией знаний (Knowledge Distillation)

Дистилляция знаний - это метод, при котором маленькая "студенческая" модель обучается на выходах большой "учительской" модели. Это позволяет передать знания большой модели в маленькую, сохраняя при этом большую часть производительности.

**Преимущества:**
- Снижение вычислительной сложности
- Уменьшение размера модели
- Сохранение качества при правильной реализации

**Процесс дистилляции:**
1. Обучение большой "учительской" модели на исходной задаче
2. Использование предобученной модели для генерации "мягких" меток
3. Обучение маленькой "студенческой" модели на этих мягких метках
4. Возможная донастройка на "жестких" метках из оригинального датасета

### 2. Параметрически эффективные методы (Parameter-Efficient Methods)

#### LoRA (Low-Rank Adaptation)
- Обновляет только небольшое подмножество параметров модели
- Вводит адаптеры с низкоранговыми матрицами
- Позволяет обучать на потребительских GPU

#### Адаптеры (Adapters)
- Вставляются в промежуточные слои модели
- Обновляются только параметры адаптеров
- Остальная часть модели заморожена

#### BitFit
- Обновляет только параметры смещения (bias)
- Значительно снижает количество обучаемых параметров
- Простота реализации

### 3. Архитектурные оптимизации

#### Модели с разреженным вниманием (Sparse Attention)
- Активны только подмножества нейронов
- Экономия вычислительных ресурсов
- Сохранение производительности

#### Модели с эффективными архитектурами
- Mixture of Experts (MoE) - активация только части параметров
- Switch Transformers - улучшенная версия MoE
- ALBERT-style параметризация - снижение числа параметров

### 4. Методы оптимизации гиперпараметров

#### Поиск гиперпараметров для маленьких моделей
- Более чувствительны к выбору гиперпараметров
- Требуется тщательная настройка скорости обучения
- Подбор оптимального размера батча для стабильности

#### Расписания обучения (Learning Rate Schedules)
- Линейное убывание
- Косинусное расписание
- Warmup-фаза для стабилизации

### 5. Эффективные методы оптимизации

#### Gradient Checkpointing
- Жертвует вычислительной скоростью ради экономии памяти
- Сохраняет промежуточные результаты, а не все активации
- Позволяет обучать на меньших GPU

#### Mixed Precision Training
- Использование float16 вместо float32
- Снижение потребления памяти вдвое
- Ускорение вычислений на современных GPU

#### Micro-batching
- Разделение больших батчей на микро-батчи
- Позволяет симулировать большие батчи на маленьких GPU
- Улучшает стабильность обучения

## Новые концепции и термины

- **Knowledge Distillation Pipeline**: Комплексный процесс обучения маленьких моделей на основе больших, включая подготовку учителя, генерацию меток и обучение студента.

- **Parameter-Efficient Fine-Tuning (PEFT)**: Методы тонкой настройки, при которых обновляется только небольшая часть параметров, что экономит память и вычислительные ресурсы.

- **Student-Teacher Architecture**: Архитектура, при которой одна модель (учитель) обучает другую модель (студент) через передачу знаний.

- **Soft Labels**: Вероятностные метки, генерируемые учителем, которые содержат больше информации, чем жесткие (один-из-всех) метки.

- **Curriculum Learning**: Обучение модели на данных, упорядоченных по сложности, начиная с простых примеров и постепенно переходя к сложным.

## Примеры применения

- **Обучение на consumer GPU**: Применение методов для обучения моделей на недорогих видеокартах
- **Квантование и дистилляция**: Создание компактных моделей для мобильных и edge-устройств
- **Многозадачное обучение**: Обучение одной модели на нескольких задачах с минимальными ресурсами
- **Федеративное обучение**: Обучение моделей на распределенных устройствах с ограниченными ресурсами
- **Персонализация**: Адаптация моделей для конкретных пользователей или задач с экономией ресурсов

## Связи с другими темами

- [[smol_training_playbook.md]] - Общее руководство по обучению небольших моделей
- [[lora_optimization.md]] - Подробности о методе LoRA
- [[llm_fine_tuning_preserving_skills.md]] - Сохранение навыков при дообучении LLM: лучшие практики предотвращения катастрофического забывания
- [[structured_pruning.md]] - Прореживание как способ уменьшения размера моделей
- [[model_quantization_techniques.md]] - Квантование как дополнительная техника сжатия
- [[memory_efficient_training.md]] - Общие методы эффективного использования памяти
- [[intrinsic_dimensionality.md]] - Теоретическая основа того, почему параметрически эффективные методы работают
- [[on_policy_distillation.md]] - Детали дистилляции знаний в LLM

## Ссылки на источники

- "Distilling the Knowledge in a Neural Network" - оригинальная статья о дистилляции знаний
- HuggingFaceTB Smol Training Playbook на Hugging Face Spaces
- "LoRA: Low-Rank Adaptation of Large Language Models" - статья о методе LoRA
- "Parameter-Efficient Transfer Learning" - обзор методов параметрически эффективного обучения