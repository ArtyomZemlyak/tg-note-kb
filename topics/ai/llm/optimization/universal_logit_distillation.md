# Universal Logit Distillation (ULD)

## Определение

Universal Logit Distillation (ULD) — это метод кросс-токенизаторной дистилляции, использующий расстояние Вассерштейна (также известное как Earth Moving Distance) для сравнения распределений логитов моделей-учителя и ученика с разными токенизаторами.

## Проблема, которую решает ULD

Классические методы дистилляции знаний требуют, чтобы модели-учитель и ученик имели одинаковые токенизаторы, что ограничивает гибкость при обучении моделей разных архитектур. ULD решает эту проблему, позволяя дистиллировать знания между моделями с разными токенизаторами.

## Основная идея

ULD использует расстояние Вассерштейна для измерения расстояния между двумя распределениями разного размера. Метод работает следующим образом:

1. Берутся логиты от учителя и логиты от ученика
2. Генерации обрезаются до самой короткой длины
3. Вероятности токенов сортируются (делается предположение, что распределение вероятностей у софтмакса будет примерно одинаковым)
4. Вычисляется расстояние Вассерштейна между распределениями

## Преимущества

- **Кросс-токенизаторная совместимость**: Может работать с моделями с разными токенизаторами
- **Лучше hard-label дистилляции**: Показывает значительно лучшее качество по сравнению с hard-label подходом
- **Использование полного распределения**: В отличие от hard-label, позволяет обучать на всем распределении, а не только на топ-1 токене

## Недостатки

- **Потеря информации**: Так как длина генерации обрезается до более короткой, теряется информация из конца генерации
- **Риск неправильного сопоставления**: Так как ID токенов разные, можно дистиллировать друг в друга несовпадающие по смыслу токены

## Примеры применения

В оригинальной работе авторам удалось сдистиллить:
- Llama и Mistral в mT0-580m
- BloomZ-560m
- OPT-350m
- Pythia-410m

С значительно лучшим качеством по сравнению с hard-label дистилляцией.

## Сравнение с другими методами

- **Hard-label дистилляция**: Дешевле (не требует хранения всех логитов), но уступает по качеству и не использует полное распределение
- **GOLD**: Более совершенный метод, который улучшает ULD, решая проблемы потери информации и неправильного сопоставления токенов
- **CDM**: Использует выравнивание токенов, но также может иметь проблемы с потерей информации

## Связи с другими темами

- [[knowledge_distillation.md]] - основы дистилляции знаний
- [[gold_method.md]] - улучшенная версия ULD
- [[contextual_dynamical_mapping.md]] - метод, улучшающий ULD
- [[wasserstein_distance.md]] - математическая основа метода
- [[distillation_challenges.md]] - проблема мисалигнмента токенизаторов
- [[on_policy_distillation.md]] - контекст проблемы и решения
- [[model_optimization.md]] - общая область применения
- [[optimization/structured_pruning.md]] - альтернативный метод оптимизации

## Источники

1. [Universal Logit Distillation](https://arxiv.org/abs/) - оригинальная статья о методе ULD
2. [Huggingface Blog Post on GOLD](https://huggingface.co/blog/gold) - объяснение контекста и сравнения с другими методами