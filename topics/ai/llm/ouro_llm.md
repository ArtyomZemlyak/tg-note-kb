# Ouro-LLM: Зацикленные Языковые Модели

## Краткое описание

Ouro-LLM (Ouro Looped Language Models) - это семейство предварительно обученных зацикленных языковых моделей, которые встраивают рассуждение в фазу предварительного обучения. Модель названа в честь рекурсивного символа Уробороса, символизирующего зацикленную природу архитектуры. Вместо явной генерации текста Ouro использует итеративные вычисления в латентном пространстве.

## Основные особенности

- **Зацикленная архитектура**: Использует параметрически разделенные блоки трансформера, применяемые рекурсивно
- **Исключительная параметрическая эффективность**: Ouro-1.4B соответствует стандартным моделям в 4B параметров, Ouro-2.6B сопоставим с моделями в 8B параметров
- **Превосходные возможности манипуляции знаниями**: Сосредоточена на манипуляции знаниями, а не на их хранении
- **Адаптивные вычисления с регуляризацией энтропии**: Динамическое распределение глубины для аллокации на основе сложности входных данных

## Архитектура и методология

### Зацикленная архитектура

Ouro-LLM использует инновационную зацикленную архитектуру, где стандартные блоки декодера Transformer применяются рекурсивно. В отличие от традиционных последовательных трансформеров, которые обрабатывают входные данные за один проход, Ouro применяет одни и те же параметрически разделенные блоки несколько раз, позволяя модели "углубляться" в рассуждение без пропорционального увеличения параметров.

Модель использует 4 рекурсивных шага (R4) для достижения параметрической эффективности, при этом простые входные данные выходят после меньшего числа шагов, а сложные задачи используют больше итераций.

### Итеративные вычисления в латентном пространстве

В отличие от методов цепочки рассуждений (chain-of-thought), которые добавляют рассуждения в явной текстовой форме, Ouro-LLM интегрирует рассуждение через итеративные вычисления в латентном пространстве. Это позволяет модели проводить более глубокие рассуждения без увеличения длины выходного текста.

### Регуляризация с адаптивной глубиной

Модель использует энтропийно-регуляризированную целевую функцию для обучения распределению глубины, где простые задачи завершаются быстрее, а сложные задачи получают больше итераций для вычислений.

## Пайплайн обучения

Процесс обучения Ouro-LLM включает в себя многоэтапный пайплайн, обрабатывающий 7.7 триллионов токенов:

1. **Разогрев**: Начальный этап прогрева модели
2. **Стабильная фаза обучения 1**: 3 триллиона токенов на стандартных данных предварительного обучения
3. **Формирование ветвления модели**: Создание вариантов 1.4B и 2.6B через "upcycling" (восстановление)
4. **Стабильная фаза обучения 2**: Дополнительные 3 триллиона токенов для обеих вариаций
5. **CT Annealing**: 1.4 триллиона токенов с отжигом цепочки рассуждений (chain-of-thought)
6. **LongCT**: 20 миллиардов токенов обучения цепочке рассуждений с длинным контекстом
7. **Промежуточное обучение**: 300 миллиардов токенов целевого промежуточного обучения
8. **SFT с уклоном в рассуждение**: Контролируемая тонкая настройка для моделей с фокусом на рассуждение (варианты Ouro-Thinking)

## Варианты моделей

- **Ouro-1.4B**: Базовая модель с 1.4 миллиардами параметров
- **Ouro-2.6B**: Увеличенная модель с 2.6 миллиардами параметров
- **Ouro-1.4B-Thinking**: Вариант, ориентированный на рассуждение
- **Ouro-2.6B-Thinking**: Увеличенный вариант, ориентированный на рассуждение

## Ключевые инновации

- **Интеграция рассуждения в предварительное обучение**: Вместо добавления рассуждений через пост-обучение, Ouro строит рассуждение в фазу предварительного обучения
- **Параметрическая эффективность через зацикленную архитектуру**: Возможность более глубоких рассуждений без пропорционального увеличения параметров
- **Фокус на манипуляцию знаниями**: Превосходные способности манипулировать знаниями, а не просто их хранить
- **Превосходящая достоверность процесса рассуждения**: Показывает лучшую верность процессу рассуждения по сравнению со стандартными LLM

## Технические детали

- **Архитектура**: Стандартный Transformer только-декодер с вращающимися позиционными вложениями (RoPE), активацией SwiGLU и нормализацией бутерброда
- **Обучение**: 7.7 триллионов токенов всего в многоступенчатом пайплайне
- **Итеративные вычисления в латентном пространстве** вместо явной генерации текста
- **Целевая функция с регуляризацией энтропии** для распределения глубины, основанного на сложности входных данных

## Результаты и преимущества

Эксперименты показывают 2-3-кратное улучшение параметрической эффективности по различным бенчмаркам. Преимущества происходят от превосходных возможностей манипуляции знаниями, а не от увеличенного хранения знаний. Модель показывает улучшенную последовательность в процессе рассуждения по сравнению с традиционными LLM.

## Сравнение с другими подходами

В отличие от традиционных методов chain-of-thought, которые генерируют промежуточные рассуждения в явной текстовой форме, Ouro-LLM выполняет итеративные рассуждения в латентном пространстве, что делает процесс более эффективным и менее зависимым от длины вывода.

## Связи с другими темами

- [[../reasoning/reasoning_in_llms.md]] - механизм рассуждения, встроенный в Ouro-LLM
- [[architectures/transformer_architecture.md]] - основа архитектуры, на которой построена Ouro-LLM
- [[models/llm_evaluations.md]] - сравнительные оценки параметрической эффективности
- [[optimization/parameter_efficient_training.md]] - подходы к параметрически эффективному обучению
- [[reasoning/chain_of_thought_methods.md]] - альтернативный подход к встроенному рассуждению в LLM

## Источники

- Официальный сайт: https://ouro-llm.github.io/
- Научная статья: "Ouro: Looped Language Models" (авторы из ByteDance, UC Santa Cruz, Princeton University, Mila - Quebec AI Institute, University of Montreal, и Carnegie Mellon University)