# Coconut: Chain of Continuous Thought

## Описание

Coconut (Chain Of CONtinUous Thought) — это подход к рассуждению в больших языковых моделях (LLM), при котором рассуждения происходят в непрерывном латентном пространстве, а не в дискретном токенном пространстве, как в традиционном Chain-of-Thought (CoT). Идея заключается в том, чтобы использовать последнее скрытое состояние модели не для декодирования в токен, а для непосредственной передачи в качестве эмбеддинга на следующий шаг авторегрессивного процесса генерации.

## Основная идея

Традиционное рассуждение через CoT (Chain-of-Thought) подразумевает генерацию текстовых пошаговых рассуждений, видимых в виде токенов. Однако языковое пространство может быть не оптимальным выбором для внутреннего ризонинга. Coconut предлагает альтернативу: рассуждение в непрерывном латентном пространстве без выхода в токенное пространство на промежуточных этапах. 

Это позволяет:
- Избежать узкого места, связанного с преобразованием эмбеддингов в токены, где богатство внутреннего представления может быть потеряно
- Избежать равного вычислительного бюджета для всех токенов, независимо от их значимости
- Соответствовать нейробиологическим данным, показывающим, что при рассуждении языковые области мозга не всегда задействованы

![Coconut архитектура](../../../../images/img_1763206945_AgACAgIA.jpg)

**Описание:** На изображении показана архитектура Coconut (Chain of Continuous Thought), демонстрирующая, как модель переключается между языковым и латентным режимами рассуждения. В латентном режиме последние скрытые состояния передаются непосредственно без декодирования в токены.

## Реализация

В процессе рассуждения модель переключается между двумя режимами:
1. **Языковой режим** - стандартный режим работы LLM с генерацией токенов
2. **Латентный режим** - новый режим, в котором эмбеддинги последнего слоя непосредственно передаются как вход на следующий шаг, без декодирования

Переключение между режимами осуществляется с помощью специальных токенов:
- `<bot>` - начало латентного режима
- `<eot>` - конец латентного режима

Для токенов между `<bot>` и `<eot>` включается новый режим, при котором выходной эмбеддинг предыдущего шага используется напрямую как входной эмбеддинг следующего шага, без участия выходной головы модели и слоя эмбеддинга.

## Обучение

Coconut использует многоэтапную учебную программу (curriculum):
1. **Начальный этап**: обучение обычному языковому CoT
2. **Последующие этапы**: для шага номер k первые k шагов языкового рассуждения убираются, а внутри тегов `<bot>`/<`eot>` появляются k позиций, в каждую из которых записываются эмбеддинги предыдущего шага

Во время обучения оптимизируется традиционный negative log-likelihood loss, но лосс маскируется и не учитывается для вопроса и латентных мыслей. Состояние оптимизатора сбрасывается между отдельными этапами.

## Инференс

Во время инференса основной задачей является определение, когда нужно входить в латентный режим и выходить из него. Для токена `<bot>` это просто - он ставится сразу после вопроса, а для `<eot>` рассматриваются две стратегии:
1. Обучение бинарного классификатора, который решает по эмбеддингу, когда нужно переключаться
2. Добивание латентных размышлений до фиксированной длины паддингом

По умолчанию используется более простой второй подход.

## Эксперименты и результаты

Coconut тестировался на трех датасетах:
- **GSM8k** - математическое рассуждение
- **ProntoQA** - логическое рассуждение
- **ProsQA** - новый датасет для логического рассуждения

Результаты показали:
- Coconut стабильно превосходит LLM без CoT
- Лучше, чем CoT на логических задачах
- На GSM8k обычный CoT показал лучшие результаты, но качество Coconut растет с увеличением числа мыслей на шаг
- Coconut использует значительно меньше токенов по сравнению с традиционным подходом

Кроме того, Coconut с k=0 (когда он вынужден генерировать обычную языковую цепочку CoT без латентных мыслей, но с парой токенов <bot>/<eot>) показывает более высокое качество, чем стандартный CoT, и меньше галлюцинирует (данные из оригинальной статьи [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769)).

## Интерпретация латентного рассуждения

Латентное рассуждение в Coconut можно интерпретировать как поиск по дереву, так как непрерывные мысли могут содержать более одного шага рассуждения. Эта интерпретация напоминает поиск в ширину (BFS), но с вероятностями или приоритетами, а не равномерный. На основе вычисленных вероятностей можно оценить степень параллелизма мыслей, которая выше у первых мыслей.

Латентное рассуждение позволяет модели отложить выбор конкретных слов и "обдумывать" варианты глубже по дереву поиска, оценивая узлы уже у самых листьев, где ошибочные пути легко выявить. Уверенность модели обратно пропорциональна высоте узла: на малых высотах она четко отделяет правильные варианты от неправильных, тогда как на больших это различие размывается.

## Сравнение с другими подходами

Coconut сравнивался с несколькими бейзлайнами:
1. Обычный CoT с файнтюнингом модели на примерах
2. No-CoT, с обучением модели сразу выдавать ответ
3. iCoT, implicit CoT, который постепенно интернализировал промежуточные шаги рассуждений
4. Pause token, когда между вопросом и ответом вставляются специальные токены <pause>

Также тестировался Coconut в разных режимах:
- Без мультиэтапного обучения (curriculum)
- Без использования непрерывных латентных мыслей
- С заменой непрерывных мыслей на токены pause

## Связь с другими подходами

Coconut перекликается с подходом LCM (Latent Chain of Thought), только в LCM сразу работали на уровне отдельных больших мыслей-предложений, а в Coconut избавляются от токенов для промежуточных вычислений. 

Также интересно сравнение с TRM (Tiny Recursive Model), где рассуждение разворачивается в другом измерении - в глубине вызовов модели, а не на уровне токенов при авторегрессивной генерации, как в Coconut.

## Связанные темы

- [[cot_variants.md]] - другие варианты Chain of Thought, включая CoT-SC и CaT
- [[latent_variables_reasoning.md]] - рассуждение с помощью латентных переменных
- [[reasoning_patterns.md]] - паттерны рассуждения в моделях
- [[trm_architecture.md]] - архитектура Tiny Recursive Model с альтернативным подходом к рассуждению
- [[computational_graphs_in_llm_reasoning.md]] - вычислительные графы в рассуждениях LLM

## Источники

1. [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769) - основная статья о Coconut, описывающая подход Chain of Continuous Thought
2. [Coconut GitHub репозиторий](https://github.com/facebookresearch/coconut) - официальный репозиторий с кодом для Coconut