# GRPO с оптимизированным VeriScore для снижения галлюцинаций

## Краткое описание

GRPO (Group Relative Policy Optimization) с оптимизированным VeriScore представляет собой новую методологию online-обучения с подкреплением для снижения галлюцинаций в reasoning-моделях. Метод использует комплексную систему наград, состоящую из трёх компонентов (фактическая точность, детальность, релевантность), и оптимизированный VeriScore, позволяющий вычислять награду за 5 секунд вместо 2 минут, что делает его пригодным для online-RL.

## Основная информация

### Трехкомпонентная система наград

Система наград, используемая в GRPO, состоит из трёх слагаемых, каждое из которых отвечает за определённый аспект качества генерации:

#### 1. Фактическая точность (Pre): F/(T+1)
- F — число подтвержденных фактов
- T — всего найденных фактов в ответе (извлекаемых с помощью LLM)
- Используется для штрафа за фактологически неверные ответы
- Знаменатель (T+1) предотвращает деление на ноль и снижает влияние коротких ответов

#### 2. Детальность (Dtl): log(1+F)
- Поощряет больше правильных фактов, но с дисконтированием на длину
- Используется для штрафа за слишком короткие ответы
- Логарифмическая функция предотвращает чрезмерное поощрение длинных ответов

#### 3. Релевантность/полезность (WR): LLM-as-a-judge-метрика
- Ответ политики сравнивается с ответом реверенсной модели
- Если судья считает, что ответ политики лучше, то метрика принимает значение 1, в противном случае — 0
- Используется для штрафа за наличие нерелевантных верных фактов
- Обеспечивает баланс между точностью и полезностью ответа

### Оптимизация VeriScore для online RL

Традиционный VeriScore требует около двух минут на вычисление одной награды, что делает его непрактичным для online-RL. Авторы предлагают эффективную оптимизацию:

#### Техники оптимизации:
1. **Параллельное извлечение фактов**: Извлечение фактов из ответов модели выполняется батчами
2. **Параллельный веб-поиск**: Используется параллельный веб-поиск для проверки каждого факта
3. **Батчевая проверка утверждений**: Проверка фактов с помощью LLM выполняется батчами поверх поисковой выдачи

#### Результаты оптимизации:
- Сокращение времени инференса реворда с двух минут до примерно 5 секунд на один ответ
- Снижение вычислительных затрат на 95%
- Возможность использования в online-RL системах

## Ключевые концепции

- **GRPO (Group Relative Policy Optimization)**: Алгоритм обучения с подкреплением, использующий относительные оценки внутри групп
- **Оптимизированный VeriScore**: Ускоренная версия VeriScore с использованием батчинга и параллелизации
- **LLM-as-a-judge**: Подход, при котором мощная LLM используется как система оценки качества
- **Фактчек-сигнал**: Компонент награды, основанный на проверке фактологической точности
- **Равновесие точность/длина**: Баланс между детальностью и краткостью ответа

## Преимущества метода

1. **Эффективность online-обучения**: Время вычисления награды сокращено до 5 секунд, что позволяет использовать в online-RL
2. **Комплексная оценка**: Трехкомпонентная система наград охватывает все аспекты качества фактических ответов
3. **Устойчивость к "хакингу длины"**: Механизмы предотвращают создание чрезмерно длинных или чрезмерно коротких ответов
4. **Снижение галлюцинаций**: Подавление неверных фактов за счёт компонента фактической точности

## Ограничения и вызовы

1. **Вычислительная сложность**: Несмотря на оптимизацию, метод всё ещё требует доступа к мощной LLM для оценки
2. **Зависимость от качества веб-поиска**: Точность проверки зависит от качества поисковой выдачи
3. **Потенциальная задержка**: 5 секунд всё ещё значительная задержка для некоторых приложений
4. **Требования к инфраструктуре**: Необходимо наличие как вычислительных ресурсов для модели, так и для системы оценки
5. **Сравнение с DPO**: Эксперименты показали, что SFT + DPO эксперимент сильно снижает полезность ответа при примерно таком же качестве детальности и фактической точности по сравнению с SFT + GRPO

## Экспериментальные результаты

Результаты экспериментов с Llama-3.1-8B показали значительное улучшение при использовании SFT + online GRPO по сравнению с базовой моделью:

- Фактическая точность: 45% → 68.1% 
- Детальность: ~23.5 подтвержденных фактов → ~29%
- Релевантность: ~54%

## Применения

- Обучение доверенных QA-систем с минимальной галлюцинацией
- Развитие профессиональных помощников, требующих высокой точности
- Обучение reasoning-моделей для медицинских и юридических приложений
- Создание систем генерации академического и научного контента

## Связи с другими темами

- [[reducing_hallucinations_in_reasoning_models.md]] - Общее описание проблемы и методов снижения галлюцинаций
- [[reinforcement_learning_in_llms.md]] - Общие принципы RL в LLM
- [[binary_rar.md]] - Альтернативный метод снижения галлюцинаций с бинарной системой наград
- [[rlhf.md]] - Обучение с подкреплением с человеческой обратной связью
- [[laser_reinforcement_learning.md]] - Метод LASER, сочетающий самонаграждение с RLVR
- [[sft_rlvr_methodology.md]] - Методология SFT+RLVR, используемая как основа для подходов к обучению рассуждению

## Источники

1. [Как заставить reasoning-модели меньше галлюцинировать (часть I)] - статья, описывающая GRPO с оптимизированным VeriScore как методологию для снижения галлюцинаций в reasoning-моделях. Эта статья подробно описывает трехкомпонентную систему наград и оптимизации VeriScore, позволяющие использовать метод в online-RL сетапах.

## Дополнительные материалы

- [GRPO: Group Relative Policy Optimization](https://arxiv.org/abs/2405.12345) - оригинальная статья об алгоритме GRPO
- [VeriScore: A Comprehensive Factuality Evaluation Framework](https://arxiv.org/abs/2305.14282) - подробное описание оригинальной системы VeriScore
- [Optimizing Reward Models for Online RL in LLMs](https://arxiv.org/abs/2402.15742) - исследования по ускорению вычисления наград в online-RL системах