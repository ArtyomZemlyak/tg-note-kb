# Learning to Reason for Factuality: часть II - Эксперименты и результаты

## Краткое описание

Продолжение анализа статьи "Learning to Reason for Factuality", посвященное результатам экспериментов и интересным выводам о снижении галлюцинаций в reasoning-моделях. В статье рассматриваются результаты на шести сложных бенчмарках фактологических ответов и сравниваются различные подходы к обучению.

## Основная информация

### Бенчмарки для оценки фактологичности

Оценка производилась на шести сложных бенчмарках фактологических ответов:
- **LongFact**: запросы, требующие длинных фактологически точных ответов
- **FAVA**: оценка верификации атрибутов 
- **AlpacaFact**: датасет для проверки фактологической точности
- **Biography**: биографические данные и факты о людях
- **FactBench-Hard**: сложные задачи проверки фактов
- **Factory-Hard**: сложные задачи для оценки фактологичности

### Результаты для Llama-3.1-8B

Результаты сравнения базовой модели и модели после полного пайплайна (SFT + online GRPO с новым ревордом):

| Метрика | Базовая Llama-3.1-8B | После пайплайна (SFT + online GRPO) |
|---------|----------------------|------------------------------------|
| Фактическая точность | 45% | 68.1% |
| Подтвержденных фактов на ответ | 23.5 | - |
| Детальность (Dtl) | - | 29% |
| Релевантность | - | ~54% |

Таким образом, в описанном сетапе reasoning-версия модели стала меньше галлюцировать без потери полезности относительно своей не-reasoning-версии.

### Сравнение подходов: SFT + online GRPO vs SFT + DPO

Интересный результат был получен при сравнении двух подходов к обучению:

- **SFT + DPO эксперимент**: сильно снижает полезность ответа, при примерно таком же качестве детальности (Dtl) и фактической точности (Pre) по сравнению с SFT + GRPO
- **SFT + online GRPO**: сохраняет полезность при значительном улучшении фактологичности

Это делает предложенный авторами подход (SFT + online GRPO с новой системой наград) довольно актуальным для практического применения.

### Мета-рассуждения в CoT-цепочках

Исследователи также попытались определить стратегии meta-reasoning в CoT-рассуждениях модели, используя Llama-3.1-70B-Instruct. Было выявлено, что стратегии ризонинга для повышения фактологичности ответов модели сильно отличаются от стратегий, используемых при решении математических и кодинг-задач:

#### Для математических задач наиболее частые стратегии:
- Self-verification (самопроверка)
- Exploration (исследование)
- Calculation and backtracking (вычисления и возврат к предыдущим шагам)
- Другие стратегии, ориентированные на решение задач с однозначным ответом

#### Для задач с фактологической точностью наиболее частые стратегии:
- Synthesis (синтез информации)
- Summarization (обобщение)
- Explanation (объяснение фактов)
- Evaluation (оценка достоверности информации)

### Причина большего галлюцинирования в RLVR-сетапе

Этот факт может быть одной из причин большего галлюцинирования reasoning-моделей, которые обучаются в RLVR-сетапе на задачах математики и кода, при обработке запросов, требующих фактологической точности. Модели, обученные на задачах с однозначными ответами, не развивают необходимые стратегии для работы с фактологически точной информацией.

## Ключевые концепции

- **Фактическая точность (Pre)**: доля подтвержденных фактов в ответе
- **Детальность (Dtl)**: мера полноты информации в ответе
- **Релевантность**: мера соответствия ответа вопросу
- **Мета-рассуждения**: стратегии, используемые моделью для организации процесса рассуждения
- **CoT-рассуждения**: Chain-of-Thought рассуждения, цепочки логических выводов
- **SFT + online GRPO**: комбинация supervised fine-tuning и online-обучения с подкреплением
- **"Хакинг награды" в DPO**: снижение полезности при сохранении точности

## Примеры применения

- Обучение доверенных QA-систем с минимальной галлюцинацией
- Понимание различий между типами рассуждений для разных задач
- Оптимизация процесса обучения для конкретных типов задач
- Разработка более эффективных систем проверки информации

## Связи с другими темами

- [[reducing_hallucinations_in_reasoning_models.md]] - Основная информация о проблеме галлюцинаций в reasoning-моделях
- [[grpo_optimized_veriscore_for_hallucination_reduction.md]] - Подробное описание GRPO с оптимизированным VeriScore
- [[sft_rlvr_methodology.md]] - Методология SFT+RLVR, используемая как база
- [[logical_reasoning_in_llms.md]] - Логические рассуждения в языковых моделях
- [[reasoning_patterns.md]] - Паттерны рассуждений в LLM

## Источники

1. [Как заставить reasoning-модели меньше галлюциновать (часть II)](https://duzhny.nlp) - статья, описывающая результаты экспериментов из статьи "Learning to Reason for Factuality". В статье представлены данные сравнения производительности различных подходов к обучению (SFT + online GRPO vs SFT + DPO), а также анализ различий в стратегиях рассуждений для математических и фактологических задач. Подготовлено Душным NLP.

## Дополнительные материалы

- [Learning to Reason for Factuality - исследование] - оригинальная статья с полным описанием методов и экспериментов
- [Сравнение RL-алгоритмов для снижения галлюцинаций] - обзор различных подходов к обучению с подкреплением для улучшения фактологичности