# Снижение галлюцинаций в reasoning-моделях

## Краткое описание

Одной из ключевых проблем современных reasoning-моделей (таких как DeepSeek-R1 и QwQ-32B) является повышенная склонность к галлюцинациям в сравнении с их базовыми версиями (DeepSeek-V3 и Qwen-2.5-32B). Несмотря на то, что эти модели успешно решают математические задачи и пишут код, в длинных фактологических ответах они демонстрируют на 10-13 процентных пунктов больше галлюцинаций. В статье рассматриваются методы, которые могут обучить стратегии рассуждения, повышающие фактическую точность LLM.

## Основная информация

### Проблема: Увеличенные галлюцинации в reasoning-моделях

Reasoning-модели, такие как DeepSeek-R1 и QwQ-32B, показывают выдающиеся результаты в задачах, требующих логического мышления, математических вычислений и программирования. Однако при генерации длинных фактологических ответов они демонстрируют значительно большую склонность к галлюцинациям по сравнению с соответствующими базовыми моделями (DeepSeek-V3 и Qwen-2.5-32B) — на 10-13 процентных пунктов больше на шести датасетах, созданных для проверки фактологичности длинных ответов.

### Ограничения текущих подходов

Стандартные методы обучения с подкреплением (RL) для reasoning-моделей заточены под верифицируемые задачи, для которых награда вычисляется по заранее определённым правилам и проверку которых можно автоматизировать (математика, код). Однако для ответов, содержащих фактологическую информацию, не существует ни надёжной автоматической проверки (как в RLVR), ни возможности привлечения человека для проверки.

Эти трудности сильно ограничивают использование фактчек-сигнала в алгоритмах online-RL. Попытки автоматизировать фактчек с помощью FActScore/VeriScore в online-RL-сетапе приводят к «хакингу награды»: модель начинает писать слишком кратко (меньше фактов — меньше шансов ошибиться) или выдаёт длинный, но слабо связанный с вопросом поток общих, пусть и верных, сведений.

### Предлагаемый подход

Чтобы обучить стратегии рассуждения, повышающей фактическую точность, авторы предлагают следующий комплексный подход:

#### 1. Генерация обучающих промптов

Для получения обучающих промптов авторы используют интересный подход: инженеры генерируют промпты с помощью Llama 4, обуславливая её на два множества grounding-промптов:
- **WildChat**: разнообразные реальные запросы пользователей
- **LongFact**: запросы, требующие фактологически точных ответов

Таким образом получается собрать порядка 7 тысяч синтетических промптов: 3 тысячи для SFT, 4 тысячи для RL, которые похожи на реальные запросы пользователей и в то же время требуют фактологически точных ответов.

#### 2. Supervised Fine-Tuning (SFT)

Для SFT фью-шотят базовую Llama-3.1-8B-Instruct для генерации 10 Long-CoT-ответов в формате `<cot>…</cot><answer>…</answer>`. Эти ответы прогоняются через VeriScore, и берётся ответ с наибольшей наградой за фактологическую точность.

#### 3. Direct Preference Optimization (DPO) как бейзлайн

Для сбора пар для обучения аналогично методу в SFT используется VeriScore с небольшой модификацией — берутся пары ответов с максимальной дельтой награды VeriScore и удовлетворяющие условиям:
1) Дельта награды должна быть выше определённого порога, чтобы фактчек-сигнал был достаточно сильным
2) Разность длин ответов должна быть меньше определённого порога, чтобы не было «хакинга длины»

#### 4. Online Reinforcement Learning с новой системой наград

В качестве нововведения для online RL предлагается награда, которая состоит из трёх слагаемых:

**1. Фактическая точность (Pre): F/(T+1)**
- F — число подтвержденных фактов
- T — всего найденных фактов в ответе (извлекаемых с помощью LLM)
- Используется для штрафа за фактологически неверные ответы

**2. Детальность (Dtl): log(1+F)**
- Поощряет больше правильных фактов, но с дисконтированием на длину
- Используется для штрафа за слишком короткие ответы

**3. Релевантность/полезность (WR): LLM-as-a-judge-метрика**
- Ответ политики сравнивается с ответом реверенсной модели
- Если судья считает, что ответ политики лучше, то метрика принимает значение 1, в противном случае — 0
- Используется для штрафа за наличие нерелевантных верных фактов

### Оптимизация VeriScore для online RL

Чтобы такой reward можно было считать в онлайне, сильно оптимизируют VeriScore:
- Батчуют извлечение фактов
- Параллелят веб-поиск
- Батчуют проверку утверждений поверх поисковой выдачи

Это позволяет сократить время инференса реворда с двух минут (базовый сетап VeriScore) до примерно пяти секунд на один ответ, что уже пригодно для online-RL. Полученный подход уже используется в GRPO-алгоритме в качестве модели награды.

## Ключевые концепции

- **Reasoning-модели**: Модели, обученные использовать цепочки рассуждений для решения задач
- **Фактологические ответы**: Ответы, содержащие точную фактологическую информацию
- **FActScore/VeriScore**: Метрики для автоматической проверки фактологической точности
- **"Хакинг награды"**: Поведение модели, направленное на максимизацию награды за счёт снижения качества генерации
- **Online RL**: Системы обучения с подкреплением, которые обновляют политику в реальном времени на основе полученных наград

## Примеры применения

- Обучение reasoning-моделей без увеличения уровня галлюцинаций
- Улучшение качества длинных фактологических ответов от LLM
- Развитие более надежных систем генерации контента
- Создание доверенных ассистентов для профессионального использования

## Связи с другими темами

- [[sft_rlvr_methodology.md]] - Методология SFT+RLVR, также использующая подходы к обучению с промежуточными рассуждениями
- [[hallucinations_in_llm.md]] - Общая информация о галлюцинациях в языковых моделях
- [[reinforcement_learning_in_llms.md]] - Обучение с подкреплением в контексте LLM
- [[binary_rar.md]] - Другой метод снижения галлюцинаций с использованием бинарного вознаграждения
- [[rlvr_reasoning_limitations.md]] - Ограничения RLVR методов в улучшении способностей к рассуждению
- [[laser_reinforcement_learning.md]] - Другие методы обучения с подкреплением для LLM

## Источники

1. [Как заставить reasoning-модели меньше галлюцинировать (часть I)] - статья, описывающая проблему повышенной галлюцинации в reasoning-моделях и предлагаемые методы её решения. Основной источник информации в этом файле, опубликованная статья Душного NLP. В статье описываются результаты сравнения DeepSeek-R1 и QwQ-32B с их базовыми версиями, а также предлагается методика снижения галлюцинаций при помощи оптимизированной системы наград.

## Дополнительные материалы

- [VeriScore: A Comprehensive Factuality Evaluation Framework](https://arxiv.org/abs/2305.14282) - подробное описание метрики VeriScore, используемой для оценки фактологической точности
- [FActScore: A Framework for Faithful Evaluation of Factual Consistency](https://arxiv.org/abs/2204.01629) - альтернативная метрика для проверки фактологической согласованности
- [RLVR (Reinforcement Learning with Verifiable Rewards)](https://arxiv.org/abs/2401.08417) - исходная методология обучения с проверяемыми наградами