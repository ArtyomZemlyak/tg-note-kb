# RLVR: Иллюзия новых способностей

## Краткое описание

Недавнее исследование, прошедшее в финал (Best Paper Runner-Up) на NeurIPS 2025, под названием "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?" ставит под сомнение эффективность методов RLVR (Reinforcement Learning with Verifiable Rewards) в расширении способностей к рассуждению LLM за пределы того, что уже присутствует в базовой модели. Авторы систематически исследовали границы возможностей рассуждающих моделей, используя несмещенную метрику pass@∞ на задачах по математике, кодингу и визуальному мышлению.

## Основная информация

### Ключевые выводы исследования

Исследование показывает, что RLVR методы (включая PPO, GRPO, Reinforce++ и ReMax) не добавляют существенных новых способностей к рассуждению. Вместо этого, они радикально улучшают *эффективность сэмплирования* (правильные ответы выпадают чаще), но не расширяют фундаментальные границы возможностей модели. На больших значениях k базовые модели часто решают *больше* уникальных задач, чем их RL-версии, что говорит об ограниченности текущих методов RL прайорами предобучения.

### Метрика способностей: pass@∞

Исследователи использовали несмещенную оценку для бюджета из n сэмплов, где c — количество правильных решений:

pass@∞ := E[ 1 - C(n-c, k) / C(n, k) ]

Эта формула критически важна, так как служит прокси для общей "площади рассуждений" модели. Если бы RLVR действительно учил модель новым стратегиям, кривая pass@∞ при больших k (например, 256 или 1024) должна была бы расти относительно базовой модели. Если же RLVR работает просто как фильтр (rejection sampler), обостряя плотность вероятности вокруг известных путей, то асимптотическая производительность на больших k останется прежней или даже деградирует из-за коллапса энтропии.

- При малых k RLVR-модели действительно чаще попадают в правильный ответ
- При увеличении k базовые модели догоняют и превосходят RLVR-модели практически на всех наборах задач (AIME24, MATH500)
- Это указывает на то, что множество задач, решаемых RL-моделью, фактически является *подмножеством* задач, решаемых базовой моделью

### Суть ограничений RLVR

RLVR методы:
- **Не расширяют границы решаемости задач** (включая математические и кодовые)
- **Повышают эффективность сэмплирования** уже существующих траекторий
- **Увеличивают вероятность сразу пойти по правильному пути** в дереве рассуждений
- **Сужают спектр потенциальных рассуждений**, фокусируясь на уже известных путях
- **Работают как фильтры**, усиливая плотность вероятности вокруг уже известных путей
- **Не создают новых логических мостов**, лишь перевешивая существующий граф переходов
- **Ограничены фундаментально рамками претрейна**, не расширяя границы возможностей

### Визуализация поиска в дереве рассуждений

![Дерево рассуждений RLVR vs базовая модель](../../../../images/img_1762934819_AgACAgIA.jpg) <!-- TODO: Broken image path -->

График на изображении показывает, что:
- Все пути рассуждений в RLVR-модели уже присутствуют в базовой модели
- Для некоторых задач (например, задача A) RLVR обучение смещает распределение в сторону вознаграждаемых путей, улучшая эффективность сэмплирования
- Однако это происходит за счет уменьшения объема рассуждений: для других задач (например, задача B) базовая модель содержит правильный путь, а RLVR-модель - нет
- По мере прогресса RLVR обучения средняя производительность (pass@k) улучшается, но покрытие решаемых задач (pass@256) уменьшается, что указывает на сокращение границ рассуждений LLM

### Разведка vs Эксплуатация

Текущий консенсус в области пост-тренировки (post-training) гласит, что обучение с подкреплением (RLVR) служит катализатором для мышления "Системы 2". Считается, что оно позволяет моделям самоисправляться, откатываться назад и находить новые пути решения, недоступные базовой модели. Это часто сравнивают с "моментом AlphaGo", когда агент превосходит человеческие эвристики через игру с самим собой (self-play).

Однако исследование ставит под сомнение, является ли прирост производительности результатом изучения *новых* поведенческих паттернов или это просто жесткое подавление вероятности ошибочных токенов в пользу правильных путей, которые уже существовали в латентном распределении.

### Алгоритмы не играют роли

Значительная часть исследования посвящена проверке того, не являются ли эти результаты артефактом конкретного алгоритма. Авторы перепроверили гипотезу на целом зоопарке методов RLVR: PPO (Proximal Policy Optimization), GRPO (Group Relative Policy Optimization), Reinforce++ и ReMax.

Обучение проводили с оптимизатором AdamW и константным learning rate (10e-6), а в некоторых прогонах (например, Oat-Zero и DAPO) даже исключали штраф за KL-дивергенцию, чтобы разрешить максимальную разведку (exploration). Несмотря на все вариации, Разрыв Эффективности Сэмплирования (Δ_SE) — разница между pass@∞ RL-модели и pass@∞ базовой — оставался стабильно высоким везде. Это говорит о том, что ограничение кроется не в PPO или GRPO, а является фундаментальным свойством применения on-policy RL к замороженному претрейн-прайору без получения новой информации от среды.

### Дистилляция работает лучше

Валидация этих утверждений подкрепляется детальным анализом покрытия. На бенчмарке AIME24 задачи разбили на категории: решаемые только базой, только RL-моделью или обеими. Оказалось, что пренебрежимо малое число задач (0.0%) решается *только* RL-моделью, тогда как значительная доля (13.3%) поддается *только* базовой модели при достаточном сэмплировании (k=1024).

Авторы противопоставили этому дистилляцию. В отличие от замкнутого в себе RLVR, дистилляция учит модель-студента на траекториях более сильного учителя (например, DeepSeek-R1). В этом сеттинге граница рассуждений студента *действительно* расширялась, превосходя исходную ёмкость базовой модели. Это служит контрольным экспериментом: архитектура *способна* выучивать новые паттерны, но self-play RLVR (в текущем виде) недостаточно для их генерации исключительно из прайоров базы.

### Влияние на обучение с подкреплением

- RLVR методы эффективны для улучшения конкретных задач при ограниченном числе попыток
- Однако они не расширяют фундаментальные способности модели к рассуждению
- Все еще остается критическая зависимость от претрейна (pretraining)
- Методы RLVR фокусируются на оптимизации уже существующих возможностей, а не на расширении новых
- Это переосмысливает нарратив о "самоулучшении": текущие методы RLVR лучше всего понимать как оптимизацию инференса, "запечённую" в веса, а не как двигатель открытия принципиально нового интеллекта
- Базовая модель служит жёстким верхним пределом возможностей рассуждения

## Ограничения исследования

Исследование ограничено величиной k. Теоретически возможно, что "новые" стратегии существуют в базовой модели с астрономически низкими вероятностями (k→∞), которые RL всё же находит. Работа также фокусировалась на одноходовых задачах (Math/Code) с бинарной наградой. Динамика может отличаться в мультиагентных средах или open-ended задачах, где ландшафт наград более плотный и насыщенный градиентами. Кроме того, анализ построен на моделях с открытыми весами (Qwen, LLaMA, Mistral); внутренние механизмы закрытых фронтир-моделей (вроде o1) остаются "чёрным ящиком".

## Новые концепции и термины

- **RLVR (Reinforcement Learning with Verifiable Rewards)** - обучение с подкреплением с проверяемыми наградами, метод для улучшения способностей к рассуждению LLM
- **pass@∞** - несмещенная метрика, показывающая вероятность найти правильное решение среди k сэмплов, используемая как прокси для общей "площади рассуждений" модели
- **Эффективность сэмплирования** - способность модели найти правильное решение за меньшее число попыток
- **Границы рассуждений** - пределы задач, которые модель в принципе может решить
- **Разрыв Эффективности Сэмплирования (Δ_SE)** - разница между pass@∞ RL-модели и pass@∞ базовой модели
- **Self-play** - обучение методом игры с самим собой для открытия новых стратегий

## Примеры применения

- Оценка эффективности RLVR методов в сравнении с базовыми моделями
- Планирование стратегий обучения: понимание ограничений RLVR помогает правильно распределить ресурсы
- Оценка реального прогресса в области рассуждений у LLM
- Сравнение RLVR с другими подходами, такими как дистилляция, для улучшения рассуждений
- Понимание того, что "скейлинг вычислений во время обучения" через RL требует лучших данных или сред, а не просто больше шагов PPO на том же распределении

## Связи с другими темами

- [[sft_rlvr_methodology.md]] - Методология SFT+RLVR, которая также используется для улучшения рассуждений
- [[supervised_reinforcement_learning_srl.md]] - Альтернативный подход к обучению с промежуточными рассуждениями
- [[mathematical_reasoning_limitations.md]] - Другое исследование ограничений математического мышления в LLM
- [[laser_reinforcement_learning.md]] - Метод обучения с подкреплением, использующий RLVR принципы
- [[reducing_hallucinations_in_reasoning_models.md]] - Методы снижения галлюцинаций в reasoning-моделях, которые стремятся улучшить ограничения RLVR
- [[grpo_optimized_veriscore_for_hallucination_reduction.md]] - GRPO как альтернативный подход к обучению с подкреплением, ориентированный на снижение галлюцинаций
- [[deepseek_v3.md]] - Модель DeepSeek-R1, упоминаемая в исследовании как пример RLVR модели
- [[reasoning_patterns.md]] - Обсуждение шаблонов рассуждений, на которых тестируется эффективность RLVR

## Источники

1. [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837) - Исследование, показывающее ограничения RLVR методов в расширении способностей к рассуждению LLM за пределы базовой модели. Авторы: Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang. Опубликовано: 2025 год. Основной вывод: RL обучение смещает распределение вывода в сторону вознаграждаемых путей, не расширяя границы рассуждений. Это исследование стало лучшим runner-up на NeurIPS 2025.
2. [NeurIPS 2025: Does Reinforcement Learning Really Incentivize Reasoning in LLMs?](https://openreview.net/forum?id=4OsgYD7em5) - Открытый обзор исследовательской работы, представленной на конференции NeurIPS 2025
3. [Официальный сайт исследования](https://limit-of-rlvr.github.io) - Проектная страница с дополнительными материалами и кодом для воспроизведения результатов

## Дополнительные материалы

- [Проектная страница исследования](https://limit-of-RLVR.github.io) - Официальная страница проекта с дополнительными материалами
- [Review статьи на ArxivIQ](https://arxiviq.substack.com/p/neurips-2025-does-reinforcement-learning) - Анализ и обсуждение ключевых выводов исследования