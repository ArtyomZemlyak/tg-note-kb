# Ограничения RLVR в улучшении способностей к рассуждению LLM

## Краткое описание

Недавнее исследование под названием "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?" ставит под сомнение эффективность методов RLVR (Reinforcement Learning with Verifiable Rewards) в расширении способностей к рассуждению LLM за пределы того, что уже присутствует в базовой модели.

## Основная информация

### Ключевые выводы исследования

Исследование показывает, что RLVR методы (включая PPO, GRPO, Reinforce++ и другие популярные алгоритмы) не добавляют существенных новых способностей к рассуждению. Вместо этого, они просто переупаковывают уже существующие в распределении базовой модели знания и пути рассуждений.

### Метрика оценки: pass@k

Исследователи использовали метрику pass@k, которая показывает, что задача считается решенной, если среди k попыток модели (сэмплов) есть хотя бы одна правильная. Эта метрика отражает потенциал модели решать задачу при разумном числе попыток.

- При малых k RLVR-модели действительно чаще попадают в правильный ответ
- При увеличении k базовые модели догоняют и превосходят RLVR-модели практически на всех наборах задач

### Суть ограничений RLVR

RLVR методы:
- **Не расширяют границы решаемости задач** (включая математические и кодовые)
- **Повышают эффективность сэмплирования** уже существующих траекторий
- **Увеличивают вероятность сразу пойти по правильному пути** в дереве рассуждений
- **Сужают спектр потенциальных рассуждений**, фокусируясь на уже известных путях

### Визуализация поиска в дереве рассуждений

![Дерево рассуждений RLVR vs базовая модель](../../../../images/img_1762934819_AgACAgIA.jpg) <!-- TODO: Broken image path -->

График на изображении показывает, что:
- Все пути рассуждений в RLVR-модели уже присутствуют в базовой модели
- Для некоторых задач (например, задача A) RLVR обучение смещает распределение в сторону вознаграждаемых путей, улучшая эффективность сэмплирования
- Однако это происходит за счет уменьшения объема рассуждений: для других задач (например, задача B) базовая модель содержит правильный путь, а RLVR-модель - нет
- По мере прогресса RLVR обучения средняя производительность (pass@k) улучшается, но покрытие решаемых задач (pass@256) уменьшается, что указывает на сокращение границ рассуждений LLM

### Влияние на обучение с подкреплением

- RLVR методы эффективны для улучшения конкретных задач при ограниченном числе попыток
- Однако они не расширяют фундаментальные способности модели к рассуждению
- Все еще остается критическая зависимость от претрейна (pretraining)
- Методы RLVR фокусируются на оптимизации уже существующих возможностей, а не на расширении новых

## Новые концепции и термины

- **RLVR (Reinforcement Learning with Verifiable Rewards)** - обучение с подкреплением с проверяемыми наградами, метод для улучшения способностей к рассуждению LLM
- **pass@k** - метрика, показывающая вероятность найти правильное решение среди k сэмплов
- **Эффективность сэмплирования** - способность модели найти правильное решение за меньшее число попыток
- **Границы рассуждений** - пределы задач, которые модель в принципе может решить

## Примеры применения

- Оценка эффективности RLVR методов в сравнении с базовыми моделями
- Планирование стратегий обучения: понимание ограничений RLVR помогает правильно распределить ресурсы
- Оценка реального прогресса в области рассуждений у LLM

## Связи с другими темами

- [[sft_rlvr_methodology.md]] - Методология SFT+RLVR, которая также используется для улучшения рассуждений
- [[supervised_reinforcement_learning_srl.md]] - Альтернативный подход к обучению с промежуточными рассуждениями
- [[mathematical_reasoning_limitations.md]] - Другое исследование ограничений математического мышления в LLM
- [[laser_reinforcement_learning.md]] - Метод обучения с подкреплением, использующий RLVR принципы
- [[reducing_hallucinations_in_reasoning_models.md]] - Методы снижения галлюцинаций в reasoning-моделях, которые стремятся улучшить ограничения RLVR
- [[grpo_optimized_veriscore_for_hallucination_reduction.md]] - GRPO как альтернативный подход к обучению с подкреплением, ориентированный на снижение галлюцинаций

## Источники

1. [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://huggingface.co/papers/2504.13837) - Исследование, показывающее ограничения RLVR методов в расширении способностей к рассуждению LLM за пределы базовой модели. Авторы: Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang. Опубликовано: 18 апреля 2025 года. Основной вывод: RL обучение смещает распределение вывода в сторону вознаграждаемых путей, не расширяя границы рассуждений.

## Дополнительные материалы

- [Проектная страница исследования](https://limit-of-RLVR.github.io) - Официальная страница проекта с дополнительными материалами