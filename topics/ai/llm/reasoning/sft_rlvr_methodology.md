# Методология SFT+RLVR

## Краткое описание

SFT+RLVR (Supervised Fine-Tuning + Reinforcement Learning with Verifiable Rewards) - это комбинированный подход к обучению моделей с промежуточными рассуждениями, разработанный для задач шаблонного рассуждения. Метод позволяет эффективно обучать модели рассуждению при ограниченных ресурсах ручной разметки.

## Основная информация

SFT+RLVR представляет собой двухэтапный процесс обучения:

1. **SFT (Supervised Fine-Tuning)**: Обучает модель генерировать явные траектории рассуждений (рационали)
2. **RLVR (Reinforcement Learning with Verifiable Rewards)**: Усиливает стратегию решения на основе проверяемых ответов без необходимости в "золотых" рационалах

### Этапы методологии

#### 1. SFT (Supervised Fine-Tuning)
- Обучает модель выдавать явные цепочки рассуждений
- Использует ограниченное количество примеров с человеческими рационалями
- Фокусируется на усвоении шаблона рассуждения, а не на количестве данных

#### 2. RLVR (Reinforcement Learning with Verifiable Rewards)
- Улучшает стратегию решения на основе проверяемых (верифицируемых) ответов
- Не требует "золотых" рационалов
- Использует большие наборы данных с парами (вопрос, ответ)

## Ключевые результаты

- **Эффективность**: SFT+RLVR обученное на 10× меньшем количестве человеческих рационалей почти не теряет в качестве
- **Устойчивость**: Даже с частично "испорченными" рационалями (с сохранением шаблона) качество почти не падает
- **Лучший результат**: На задаче NSM (Numerical Semantic Matching) SFT+RLVR достигло точности 90.3% и F1 78.4%

## Преимущества методологии

- **Экономичность**: Требует значительно меньше ручной разметки рационалей
- **Масштабируемость**: Может использовать большие наборы данных на этапе RLVR
- **Надежность**: Устойчиво к неточностям в начальных рационалах, если шаблон сохранен
- **Промышленная применимость**: Предоставляет путь к экономичному и воспроизводимому обучению рассуждению

## Практическое применение

1. **Определение задачи**: Убедитесь, что задача относится к категории шаблонного рассуждения с фиксированным шаблоном решения
2. **Описание шаблона**: Четко сформулируйте пошаговый шаблон решения задачи
3. **Короткий SFT**: Обучите модель на небольшом наборе примеров с рационалями, следующими шаблону
4. **RLVR**: Продолжите обучение с использованием большого набора пар (вопрос, ответ)

## Ограничения

- **Не подходит для адаптивных задач**: Где стратегия решения меняется от случая к случаю
- **Зависит от качества шаблона**: Неправильно описанный шаблон приведет к обучению неправильным паттернам
- **Требует верифицируемых ответов**: RLVR работает только при наличии способа проверить правильность ответа

## Связи с другими темами

- [[ai/reasoning/pattern_learning/paro.md]] - Основная методика, использующая SFT+RLVR
- [[ai/reasoning/reasoning_patterns.md]] - Классификация задач на шаблонное и адаптивное рассуждение
- [[ai/reasoning/rftd_analysis.md]] - Диагностика поведения моделей, обученных SFT+RLVR
- [[ai/applications/financial_reasoning/financial_reasoning_tasks.md]] - Практические задачи, использующие методологию
- [[ai/reinforcement_learning/laser_reinforcement_learning.md]] - Сравнение с LASER, другим методом обучения с подкреплением для улучшения рассуждений