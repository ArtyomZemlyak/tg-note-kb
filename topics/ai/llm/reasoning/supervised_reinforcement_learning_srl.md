# Supervised Reinforcement Learning (SRL): Обучение пошаговому рассуждению через экспертные траектории

## Краткое описание

Supervised Reinforcement Learning (SRL) - это инновационный подход к обучению моделей, при котором модели учатся не просто предсказывать готовый ответ, а планировать и проверять каждый шаг рассуждений. Метод сочетает в себе преимущества контролируемого обучения и классического обучения с подкреплением, обеспечивая стабильность и глубину рассуждений.

## Основная информация

SRL представляет собой новый подход к обучению моделей, который отличается от традиционных методов:

1. **Пошаговая оценка**: В отличие от финальной оценки, SRL дает награду за каждый шаг в цепочке рассуждений
2. **Планирование поэтапно**: Модель учится думать поэтапно, а не просто копировать готовое решение
3. **Сигналы для малых моделей**: Даже маленькие модели получают четкий сигнал обучения и начинают планировать эффективно

### Техническая реализация

#### Основной принцип
- Экспертные решения разрезаются на маленькие шаги
- Модель делает шаг → получает оценку близости к эксперту
- Используется текстовый matcher + небольшой формат-штраф
- Обновления в стиле GRPO с динамическим выбором батчей, чтобы избегать пустых сигналов

#### Как это работает
1. **Декомпозиция эксперта**: Экспертные решения разбиваются на последовательные логические шаги
2. **Пошаговая оценка**: Модель получает награду за каждый шаг, который приближает к правильному решению
3. **Обратная связь**: Модель учится корректировать свое поведение на основе промежуточных результатов
4. **Оптимизация**: Используются градиентные методы для оптимизации стратегии на каждом шаге

### Преимущества метода

Модель, обученная с помощью SRL, развивает:
- **Раннее планирование**: Способность предвосхищать шаги и строить стратегию заранее
- **Коррекцию по ходу дела**: Способность адаптировать свое поведение на основе промежуточных результатов
- **Самопроверку результата**: Способность оценивать качество текущего состояния и исправлять ошибки
- **Высокое качество без увеличения длины**: Ответы не становятся длиннее, качество растет за счет глубины мышления, а не болтовни

## Результаты и производительность

SRL показывает впечатляющие результаты по сравнению с традиционными методами:

- **AIME24**: +3.4% (13.3% → 16.7%) на модели 7B
- **SRL→RLVR**: 57.5% на AMC23 (greedy)
- **Код-агенты**: 14.8% oracle resolve rate
- **Инженерные задачи**: Из 5K траекторий было создано 134K пошаговых примеров, SRL дал 8.6% фиксов кода с greedy - выше, чем SFT-coder

## Сравнение с другими методами

| Метод | Основной принцип | Преимущества | Ограничения |
|-------|------------------|--------------|-------------|
| SFT (Supervised Fine-Tuning) | Обучение на готовых примерах | Простота, стабильность | Ограниченное рассуждение |
| RLHF (Reinforcement Learning from Human Feedback) | Оптимизация под человеческую оценку | Высокое качество ответов | Сложность в создании оценочной модели |
| SRL (Supervised Reinforcement Learning) | Пошаговая оценка и планирование | Глубокие рассуждения, раннее планирование | Более сложная подготовка данных |

## Применение и области использования

1. **Математические задачи**: Решение сложных математических задач с пошаговым обоснованием
2. **Кодирование**: Создание и отладка кода с пошаговой проверкой корректности
3. **Логические рассуждения**: Задачи, требующие логических выводов и дедукции
4. **Научные исследования**: Формулирование гипотез и проверка гипотез пошагово

## Новые концепции и термины

- **Expert Trajectories**: Последовательности шагов, выполненных экспертом, используемые для обучения
- **Step-wise Reasoning**: Рассуждение, разбитое на последовательные логические шаги
- **GRPO (Gradient-based Policy Optimization)**: Метод оптимизации стратегии с градиентным подходом
- **Trajectory Decomposition**: Процесс разбиения экспертного решения на составляющие шаги

## Связи с другими темами

- [[sft_rlvr_methodology.md]] - Сравнимый метод обучения с промежуточными рассуждениями
- [[reasoning_patterns.md]] - Паттерны рассуждения, которые могут быть усилены с помощью SRL
- [[llm/reasoning/logical_reasoning_in_llms.md]] - Логическое мышление в LLM, которое может быть улучшено SRL
- [[reinforcement_learning/index.md]] - Основы обучения с подкреплением, на которых базируется SRL
- [[critical_thinking]] - Критическое мышление, которое может быть развито через пошаговую проверку

## Ссылки на источники

- Оригинальная статья: arXiv:2510.25992 - "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning"
- Связанная методология: SFT+RLVR (Supervised Fine-Tuning + Reinforcement Learning with Verifiable Rewards)