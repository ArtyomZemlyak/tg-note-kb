# Обучение без эталонов (Reference-Free Learning) для LLM

## Краткое описание

Обучение без эталонов (Reference-Free Learning) - это подход к обучению больших языковых моделей (LLM), который не требует наличия эталонных данных или человеко-созданных примеров. Вместо этого модели получают возможность самим генерировать обучающие сигналы, используя свои собственные выводы или внутренние механизмы проверки.

## Контекст и проблема

Традиционные методы дообучения LLM, такие как обучение с учителем (supervised fine-tuning) или обучение с подкреплением с человеческой обратной связью (RLHF), требуют дорогостоящих и трудоемких процессов создания эталонных данных. Для многих ценных задач - от творческого письма до клинических рекомендаций - такая чистая супервизия редка, дорога или попросту не существует. Это создает "бутылочное горлышко супервизии", ограничивающее масштабируемость и специализацию моделей.

## Подходы к обучению без эталонов

### 1. Self-Training и Self-Supervision

Модели учатся на данных, которые генерируют сами. Это может включать:
- Self-distillation: использование более крупной модели для создания обучающих данных для меньшей
- Denoising: обучение восстанавливать исходный текст из поврежденной версии
- Contrastive learning: создание позитивных и негативных примеров на основе собственных выводов

### 2. Self-Rewarding Methods

Модели самостоятельно создают сигналы вознаграждения:
- Программная проверка (для верифицируемых задач)
- Внутренние логические валидаторы
- Self-proposed rubrics (самопредложенные критерии)

### 3. Compute as Teacher (CaT)

Метод, при котором модель использует собственные параллельные роллауты для синтеза эталонного ответа, вместо простого выбора лучшего из них. Включает:
- Этап исследования: генерация нескольких роллаутов текущей политикой
- Этап синтеза: создание эталона с помощью якорной политики
- Этап верификации: преобразование эталона в сигнал вознаграждения

### 4. Reinforcement Learning с самонаграждением

Методы, подобные LASER (Last-Token Self-Rewarding), где модель генерирует собственные сигналы вознаграждения на основе последнего токена или внутренних проверок.

## Преимущества

### Масштабируемость
- Устраняет зависимость от дорогих человеческих аннотаций
- Позволяет использовать существующие вычислительные ресурсы более эффективно
- Обеспечивает возможность постоянного самосовершенствования

### Применимость к субъективным задачам
- Работает в доменах, где нет объективных эталонов
- Позволяет развивать специализированные навыки в отсутствие экспертной разметки

### Самокоррекция
- Возможность исправления собственных ошибок
- Развитие внутренних механизмов проверки и оценки

## Вызовы и ограничения

### Зависимость от начальной компетентности
- Эффективность зависит от качества исходной модели
- Слабые модели не могут генерировать качественные обучающие сигналы

### Проблема самоподтверждения
- Риск усиления существующих предубеждений и ошибок
- Отсутствие внешней проверки может привести к деградации

### Ограниченное разнообразие
- По мере улучшения модели роллауты становятся менее разнообразными
- Это может снизить эффективность синтеза эталонов

## Связанные концепции

### Self-Proposed Rubrics
- Механизм, при котором якорная политика генерирует специфичные для ответа критерии на основе синтезированного эталона
- Позволяет создавать надежные сигналы вознаграждения для непроверяемых задач

### Question-blind Anchoring
- В методах вроде CaT якорная политика не видит исходный промпт
- Это заставляет её выполнять реальные рассуждения на основе сопоставления роллаутов

### Synthesis over Selection
- Парадигма, при которой вместо выбора лучшего из существующих создается принципиально новый, улучшенный ответ
- Позволяет достигать правильных результатов, даже когда все индивидуальные ответы ошибочны

## Примеры реализации

### Compute as Teacher (CaT)
- Использует замороженную якорную политику для синтеза эталонов
- Применяет Group Relative Policy Optimization (GRPO) для обучения

### LASER (Last-Token Self-Rewarding)
- Использует самонаграждение последнего токена
- Согласует оценки самонаграждения с наградами за рассуждения

## Будущие направления

### Поощрение разнообразия
- Механизмы, поощряющие разнообразные роллауты для более эффективного синтеза
- Использование вознаграждений за исследование

### Интеграция промежуточных рассуждений
- Расширение синтеза на цепочки рассуждений, а не только на конечные ответы
- Построение более прозрачных и объяснимых систем

### Самостоятельное создание задач
- Интеграция самопредложенных вопросов для создания полностью независимого от данных цикла обучения
- Развитие внутренней мотивации и целеполагания

## Связи с другими темами

- [[compute_as_teacher.md]] - Один из ведущих методов обучения без эталонов
- [[self_proposed_rubrics.md]] - Механизм для создания критериев в непроверяемых задачах
- [[laser_reinforcement_learning.md]] - Другой метод самонаграждения
- [[rlhf.md]] - Традиционный подход, сравнение с которым важно
- [[llm_alignment.md]] - Связь с проблемой выравнивания моделей
- [[synthetic_training_data.md]] - Синтетические обучающие данные для LLM

## Ссылки на источники

- Compute as Teacher: https://arxiv.org/abs/2509.14234
- LASER: Reinforcement Learning with Last-Token Self-Rewarding
- Обзор подходов к обучению без эталонов: https://arxiviq.substack.com/p/compute-as-teacher-turning-inference