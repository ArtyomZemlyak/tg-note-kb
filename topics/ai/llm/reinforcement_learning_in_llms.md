# Обучение с подкреплением в LLM

## Описание

Обучение с подкреплением (Reinforcement Learning) в больших языковых моделях (LLM) представляет собой набор методов, используемых для улучшения поведения и ответов моделей через систему наград и штрафов.

## Методы обучения с подкреплением в LLM

### Использование модели как актора и критика
Для случаев, когда нет верифицируемой награды, модель может использоваться одновременно как актор и как критик. Актор генерирует набор ответов, которые критик попарно сравнивает относительно набора аспектов. Сам критик обновляется за счёт verifiable-сигналов.

Этот подход был использован в Kimi K2, где модель использовалась одновременно как актора, и как критика, когда не было доступной проверяемой награды.

## Другие методы обучения с подкреплением

### RLHF (Reinforcement Learning from Human Feedback)
Обучение с подкреплением на основе человеческой обратной связи, где предпочтения людей используются для обучения функции вознаграждения.

### PPO (Proximal Policy Optimization)
Алгоритм оптимизации политики, который стабилизирует процесс обучения, ограничивая изменения политики.

### DPO (Direct Preference Optimization)
Прямая оптимизация на основе предпочтений без использования промежуточной функции вознаграждения.

## Проблемы и вызовы

### Отсутствие проверяемой награды
Во многих задачах сложно определить объективную функцию награды, что приводит к необходимости использовать внутренние модели для оценки.

### Устойчивость обучения
Обучение с подкреплением может привести к деградации базовых возможностей модели, если не применять соответствующие методы регуляризации.

## Связи с другими темами

- [[rlhf.md]] - Обучение с подкреплением по отзывам человека
- [[ppo_algorithm.md]] - Алгоритм PPO
- [[dpo_optimization.md]] - Прямая оптимизация по предпочтениям
- [[kimi_k2.md]] - Применение RL методов в Kimi K2