# Reinforcement Learning with Verifiable Rewards (RLVR)

## Общее описание

Reinforcement Learning with Verifiable Rewards (RLVR) - это метод обучения с подкреплением, используемый в моделях DeepSeek V3.2, который сочетает обучение с верифицируемыми наградами для задач математики и программирования с обучением с использованием LLM-моделей вознаграждения для общих задач. Этот подход позволяет эффективно обучать модели рассуждению и агентным задачам при сохранении вычислительной эффективности.

## Принцип работы

### Две категории задач
RLVR различает два типа задач при обучении:

1. **Задачи с верифицируемыми решениями** (например, математика, программирование):
   - Используют четко определенные метрики для оценки качества решения
   - Решения могут быть проверены программно или аналитически
   - Применяются традиционные методы оценки (тестирование кода, проверка математических ответов)

2. **Общие задачи** (например, общий интеллект, рассуждения, агентство):
   - Используют LLM-модели вознаграждения (GRM - Generative Reward Models)
   - Оценка производится через специфичные рубрики и критерии
   - Применяются более субъективные оценки качества

### Методы оптимизации

#### Group Relative Policy Optimization (GRPO)
- Основной алгоритм RL, используемый в RLVR
- Позволяет обучать модели с одновременными обновлениями по группе семплов
- Повышает стабильность и эффективность обучения

#### Техники стабилизации
- **Unbiased KL Estimate**: исправление систематической ошибки в оценке дивергенции KL
- **Off-Policy Sequence Masking**: маскировка последовательностей, отклоняющихся от исходной модели
- **Keep Routing**: сохранение маршрутов экспертов в MoE моделях
- **Keep Sampling Mask**: сохранение масок семплирования для согласования пространств действий

## Применение в DeepSeek V3.2

### Гибридный подход
DeepSeek V3.2 использует гибридный подход, применяя:

- **Верифицируемые награды** для задач математики и программирования
- **LLM-модели вознаграждения** для общих задач

Это позволяет модели достичь высокой производительности в специализированных задачах при сохранении универсальности для более широкого спектра задач.

### Self-verification и Self-refinement
- Модели используют внутренние механизмы проверки и улучшения своих ответов
- Вдохновлено работами DeepSeekMath V2, где используется цикл генератор-верификатор
- Позволяет модели улучшать свои решения до окончательной выдачи

## Преимущества RLVR

### Вычислительная эффективность
- Позволяет использовать верифицируемые награды для задач, где они возможны
- Снижает зависимость от дорогостоящих человеческих оценок
- Повышает масштабируемость процесса обучения

### Качество обучения
- Обеспечивает более точную и надежную обратную связь для определенных классов задач
- Позволяет модели лучше понимать требования для задач с объективными критериями
- Сохраняет высокую производительность в задачах, требующих субъективной оценки

## Технические детали

### Unbiased KL Estimate
Одним из ключевых улучшений в GRPO является внедрение Unbiased KL Estimate:

- **Проблема**: В оригинальном GRPO KL-регуляризация оценивалась с систематической ошибкой
- **Эффект**: Когда токены имели значительно более низкую вероятность под текущей политикой πθ, по сравнению со старой политикой πold, градиент оригинального лосса назначал непропорционально большие веса
- **Последствия**: шумные градиентные обновления, нестабильная динамика обучения, деградация качества сэмплов
- **Решение**: "Unbiased KL Estimate" - исправление, заключающееся в перевзвешивании KL-члена с тем же самым коэффициентом важности (importance ratio), что и используется для основной функции потерь

### Off-Policy Sequence Masking
- Для стабилизации обучения и улучшения толерантности к off-policy обновлениям
- Маскировка отрицательных последовательностей, которые вносят значительную дивергенцию политики
- Измеряется KL-дивергенцией между политикой выборки данных π_old и текущей политикой πθ

## Сравнение с традиционными методами RLHF/RLAIF

| Аспект | RLHF/RLAIF | RLVR |
|--------|------------|------|
| Награды | Человеческие оценки / AI оценки | Комбинация верифицируемых и AI оценок |
| Масштабируемость | Ограниченная из-за необходимости человеческих данных | Высокая, особенно для задач с верифицируемыми решениями |
| Тип задач | Общие задачи | Оба типа задач |
| Точность оценки | Зависит от качества оценщиков | Высокая для верифицируемых задач |
| Вычислительная сложность | Высокая для AI-оценок | Более низкая для верифицируемых задач |

## Применения

### Математика
- Решение задач с доказуемыми ответами
- Проверка математических выкладок
- Оценка корректности рассуждений

### Программирование
- Выполнение и тестирование сгенерированного кода
- Проверка корректности алгоритмов
- Оценка эффективности решений

### Общие задачи
- Использование GRM для оценки качества общих ответов
- Применение рубрик для оценки рассуждений
- Оценка агентной способности и планирования

## Значение для отрасли

RLVR представляет собой важный шаг вперед в обучении с подкреплением для LLM:

1. **Гибкость**: Позволяет эффективно использовать разные типы обратной связи для разных задач
2. **Масштабируемость**: Снижает зависимость от дорогостоящих человеческих оценок
3. **Производительность**: Повышает качество моделей в задачах с объективными критериями
4. **Практичность**: Позволяет комбинировать точные методы оценки с более гибкими подходами

## Связи с другими темами

- [[deepseek_v3_2_reinforcement_learning_agent_training.md]] - Подробности RL и подготовки агентов в V3.2
- [[group_relative_policy_optimization.md]] - Основной алгоритм RL в RLVR
- [[generative_reward_model_grm.md]] - Используемые в RLVR модели вознаграждения
- [[unbiased_kl_estimate_in_grpo.md]] - Улучшение стабильности GRPO
- [[off_policy_sequence_masking.md]] - Метод стабилизации обучения
- [[deepseek_models_evolution_v3_to_v3_2.md]] - Контекст использования RLVR
- [[deepseek_v3_2_speciale.md]] - Применение RLVR в флагманской модели

## Источники

- Технический отчет: "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models"
- Оригинальные исследования по RLVR и GRPO
- Публикации о методах обучения с подкреплением в LLM
- Статья о DeepSeek V3.2 на Hugging Face