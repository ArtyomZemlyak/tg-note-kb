# Обучение с подкреплением с человеческой обратной связью (RLHF)

## Краткое описание

Обучение с подкреплением с человеческой обратной связью (Reinforcement Learning from Human Feedback, RLHF) - это метод улучшения больших языковых моделей (LLM), при котором используются предпочтения людей для обучения модели генерировать более полезные, честные и безвредные ответы. Это включает этапы сбора предпочтений, обучения модели вознаграждению и оптимизации политики с использованием методов обучения с подкреплением.

## Исторический контекст и мотивация

Традиционные методы обучения LLM, такие как обучение с учителем, часто приводят к моделям, которые не соответствуют человеческим ценностям или ожиданиям. RLHF был разработан как способ улучшить выравнивание (alignment) моделей с человеческими предпочтениями и ценностями.

## Трехступенчатый процесс RLHF

### 1. Обучение с учителем (Supervised Fine-Tuning, SFT)

- Создание датасета с инструкциями и эталонными человеческими ответами
- Дообучение LLM на этих данных методом максимального правдоподобия
- Создание базовой модели, способной следовать инструкциям

### 2. Сбор предпочтений (Reward Modeling)

- Генерация нескольких ответов на один и тот же промпт с помощью SFT-модели
- Попарное ранжирование этих ответов людьми по качеству
- Обучение модели вознаграждению (reward model), которая предсказывает человеческие предпочтения

### 3. Оптимизация политики (Policy Optimization)

- Использование алгоритма обучения с подкреплением (например, PPO) для оптимизации SFT-модели
- Модель вознаграждения обеспечивает сигнал для обучения
- Достижение компромисса между сохранением способностей и соответствием предпочтениям

## Технические детали

### Модель вознаграждения (Reward Model)

- Обычно это та же архитектура, что и у основной модели, но с сигмоидальным выходом
- Принимает промпт и ответ, возвращает оценку качества
- Обучается на датасете пар (ответ A, ответ B) с предпочтениями

### Алгоритм PPO (Proximal Policy Optimization)

- Один из наиболее распространенных алгоритмов для RLHF
- Использует функцию ценности для оценки качества состояния
- Ограничивает изменения политики для стабильности обучения

### Техники стабилизации

- Использование коэффициента энтропии для предотвращения преждевременной сходимости
- Учет расхождения KL между новой и старой политиками
- Техники для предотвращения "сноса" (hacking) сигнала вознаграждения

## Преимущества RLHF

### Улучшенное выравнивание
- Ответы лучше соответствуют человеческим предпочтениям
- Повышенная полезность и безвредность
- Более честное и этичное поведение

### Практические результаты
- Значительное улучшение качества взаимодействия
- Лучшее следование инструкциям
- Повышенная способность к рассуждению

## Ограничения и вызовы

### Масштабируемость
- Зависимость от дорогостоящей человеческой разметки
- Трудоемкий процесс сбора предпочтений
- Ограничения по объему доступных данных

### Культурные и этические вопросы
- Предпочтения могут различаться в разных культурах
- Сложность определения универсальных ценностей
- Возможность усиления предвзятостей в данных разметки

### Технические проблемы
- Сложность оценки качества модели вознаграждения
- Возможность "обмана" сигнала вознаграждения
- Деградация первоначальных способностей модели

## Современные альтернативы

### Обучение с прямым предпочтением (DPO)
- Прямая оптимизация по предпочтениям без модели вознаграждения
- Более стабильная и вычислительно эффективная альтернатива

### Обучение с подкреплением с AI-обратной связью (RLAIF)
- Использование предпочтений от AI-судей вместо людей
- Более масштабируемая альтернатива RLHF

### Обучение без эталонов (Reference-Free Learning)
- Методы, такие как Compute as Teacher, устраняющие зависимость от человеческой разметки
- Использование собственных выводов модели для создания обучающих сигналов

## Применения

- Дообучение коммерческих LLM (например, ChatGPT, Claude)
- Улучшение способности к безопасному рассуждению
- Адаптация моделей к специфическим задачам и стилям общения
- Повышение качества генерации в открытых вопросах

## Сравнение с другими методами

| Метод | Зависимость от данных | Вычислительная сложность | Качество выравнивания |
|-------|----------------------|-------------------------|---------------------|
| SFT | Высокая (человеческие ответы) | Низкая | Низкое |
| RLHF | Высокая (человеческие предпочтения) | Высокая | Высокое |
| DPO | Высокая (человеческие предпочтения) | Средняя | Высокое |
| RLAIF | Средняя (AI-предпочтения) | Высокая | Высокое |
| CaT | Низкая (без эталонов) | Высокая | Перспективное |

## Будущие направления

### Автоматизированные судьи
- Развитие более надежных AI-моделей для оценки ответов
- Снижение зависимости от человеческой разметки

### Выборочные методы
- Методы, которые не требуют человеческой разметки или предпочтений
- Такие как Compute as Teacher (CaT)

### Теоретические основы
- Понимание связи между способностями и выравниванием
- Формализация концепций этичности и полезности

## Связи с другими темами

- [[compute_as_teacher.md]] - Современный метод, устраняющий зависимость от человеческой разметки
- [[dpo_direct_preference_optimization.md]] - Прямая оптимизация по предпочтениям, альтернатива RLHF
- [[rlaif.md]] - Использование AI-судей вместо людей
- [[reference_free_learning.md]] - Обучение без эталонных данных
- [[llm_alignment.md]] - Более общая тема выравнивания LLM
- [[ppo_algorithm.md]] - Алгоритм, используемый в RLHF

## Ссылки на источники

- Оригинальная статья RLHF: "Training language models to follow instructions with human feedback"
- Обзор RLHF: "Constitutional AI: Harmlessness from AI Feedback"
- Сравнительные исследования RLHF и альтернативных методов