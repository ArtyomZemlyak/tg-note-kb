# Новые направления в скейлинг-законах LLM: Data-Constrained, Optimal LR и Step Law

## Обзор

Несмотря на существование фундаментальных работ о скейлинг-законах, таких как оригинальная работа 2020 года и Chinchilla 2022, исследования в этой области продолжаются. Недавние публикации проливают свет на различные аспекты масштабирования, учитывая ограничения данных, оптимальные гиперпараметры и зависимости от архитектуры.

## Scaling Data-Constrained Language Models (2023)

### Контекст и проблематика

Первые работы о скейлинг-законах предполагали, что токены практически бесконечны. В публикации "Scaling Data-Constrained Language Models" (NeurIPS 2023) авторы рассматривают противоположную ситуацию, где данные ограничены или их изначально мало. Исследование ставит вопросы о целесообразности повторения данных и возможных способах их замены.

### Ключевые экспериментальные результаты

#### Эффект повторения данных
- В экспериментах брали датасет, делили его на части, первую часть (100 миллионов токенов) повторяли несколько эпох
- Выяснилось, что при повторении до четырёх раз качество модели растёт
- При дальнейшем повторении (более 4 раз) качество начинает падать
- Это справедливо для не очень больших моделей; в противном случае лосс будет увеличиваться

#### Рекомендации для ситуаций с ограниченными данными
- Если данные ограничены, лучше заняться обучением небольшой модели с повторением данных, чем тренировкой крупной LLM
- Подход с повторением данных может быть эффективнее, чем попытки обучения большой модели с недостаточным объёмом данных

### Методы улучшения качества при ограниченных данных

#### Интеграция кода в текстовые данные
- Авторы предлагают вливать код (в публикации использовался Python) в текстовую информацию
- Это помогает улучшить качество моделей при использовании метода повторений

#### Использование perplexity-filter
- Применение perplexity-filter помогает поднимать качество при использовании метода повторений данных
- Этот фильтр позволяет отсеивать низкокачественные или дублирующиеся фрагменты данных

### Значение для практики
- Предлагает стратегии для ситуаций, когда доступ к данным ограничен
- Расширяет понимание скейлинга за рамки идеализированных условий с неограниченными данными
- Предоставляет решения для организаций с ограниченными ресурсами данных

## Scaling Optimal LR Across Token Horizons (2024)

### Основной фокус

Исследование Microsoft "Scaling Optimal LR Across Token Horizons" рассматривает, как оптимальный Learning Rate (LR) переносится между обучениями с разным числом токенов. Работа также анализирует влияние увеличения размера батча (BS) на оптимальный LR.

### Ключевые находки

#### Зависимость оптимального LR от горизонта токенов
- Оптимальный LR уменьшается при увеличении горизонта (количества токенов)
- Это справедливо даже при увеличении размера батча (BS)
- Наблюдается систематическая зависимость, которая позволяет прогнозировать оптимальный LR

#### Влияние размера батча
- Увеличение BS также влияет на оптимальный LR
- Нужно учитывать взаимодействие между количеством токенов, размером батча и оптимальным LR
- Рекомендуется адаптировать гиперпараметры при изменении масштаба обучения

### Практическое применение
- Позволяет более точно подбирать гиперпараметры при масштабировании
- Предоставляет систематический подход к настройке LR при изменении объёма данных
- Помогает избежать неэффективного обучения из-за неправильно подобранных гиперпараметров

## Predictable Scale: Part I, Step Law — Optimal Hyperparameter Scaling Law (2025)

### Общая концепция

"Predictable Scale: Part I, Step Law" представляет собой новое исследование, которое фокусируется на оптимальных гиперпараметрах при предобучении LLM. Авторы изучают проблему оптимального LR и BS при разном количестве параметров и токенов, проверяя зависимости от расписания LR и архитектуры модели.

### Математическая формулировка

#### Оптимальный Learning Rate
Авторы выводят следующую формулу для оптимального LR:

**LR = 1.79 × N^(-0.713) × D^(0.307)**

Где:
- N — число параметров модели
- D — количество данных в токенах

#### Оптимальный Batch Size
Для оптимального BS в публикации указывается формула:

**BS = 0.58 × D^(0.571)**

### Особенности и проверки

#### Влияние расписания LR
- Исследовали две стратегии: decay (min_LR = max_LR / 10) и фиксированный min_LR (10^(-5))
- Выяснили, что оптимум смещается, но в целом закон выполняется
- Расписания LR влияют на точные значения, но не разрушают общее масштабирование

#### Влияние архитектуры
- Проверяли, как распределение параметров внутри модели влияет на скейлинг-законы
- Обнаружили, что несмотря на различные архитектурные решения, основные закономерности сохраняются
- Предлагаемые законы имеют универсальный характер для различных архитектур

### Практические выводы
- Предоставляет систематический подход к выбору гиперпараметров на основе размера модели и данных
- Позволяет предсказывать оптимальные гиперпараметры без проведения дорогостоящих экспериментов
- Упрощает масштабирование и настройку больших моделей

## Сравнение с предыдущими работами

### Эволюция подходов к скейлингу
- **2020 год**: Оригинальные скейлинг-законы, подчеркивающие важность размера модели
- **Chinchilla (2022)**: Переосмысливание в пользу меньших моделей с большим объемом данных
- **Data-Constrained (2023)**: Учет ограничений по данным и эффективности повторения
- **Optimal LR (2024)**: Глубокое изучение гиперпараметров и их зависимости от масштаба
- **Step Law (2025)**: Систематическая формулировка оптимальных гиперпараметров

### Противоречия и разрешения
- Результаты различных исследований иногда кажутся противоречивыми
- Однако каждое исследование рассматривает разные аспекты и ограничения
- Современное понимание учитывает контекст применения, доступность данных и вычислительные ресурсы

## Практические рекомендации

### Для организаций с ограниченными данными
1. Рассмотрите использование небольших моделей с повторением данных
2. Интегрируйте разнообразные типы данных (например, код) для улучшения качества
3. Используйте фильтрацию (perplexity-filter) для повышения эффективности

### Для масштабных разработок
1. Учитывайте зависимости между количеством токенов и оптимальным LR
2. Адаптируйте BS в зависимости от объема данных
3. Используйте формулы Step Law для предварительной настройки гиперпараметров

### Для исследовательских целей
1. Учитывайте архитектурные особенности при формулировке скейлинг-законов
2. Проводите эксперименты с разными расписаниями LR
3. Оценивайте влияние распределения параметров внутри модели

## Связь с другими темами

- [[chinchilla_scaling_laws.md]] - Классические скейлинг-законы, с которыми сравниваются новые подходы
- [[farseer_scaling_law.md]] - Уточнённые скейлинг-законы, следующие эволюционной линии
- [[llm_scaling_architectures.md]] - Архитектурные аспекты скейлинга, которые учитываются в новых работах
- [[compute_optimal_training.md]] - Общие принципы оптимального распределения вычислительных ресурсов

## Источники

1. "Scaling Data-Constrained Language Models" - NeurIPS 2023. Исследование масштабирования моделей при ограниченных данных, включая анализ эффективности повторения данных и методов улучшения качества при ограничениях.
2. "Scaling Optimal LR Across Token Horizons" - Microsoft, 2024. Исследование зависимости оптимального Learning Rate от горизонта токенов и размера батча.
3. "Predictable Scale: Part I, Step Law -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining" - 2025. Работа, предлагающая математические формулы для оптимального выбора LR и BS при разных параметрах модели и данных.
4. "Scaling Laws for Neural Language Models" - Оригинальная работа 2020 года, заложившая основы для всех последующих исследований в этой области.
5. "Chinchilla: Training Compute-Optimal Large Language Models" - Работа 2022 года, изменившая понимание оптимального соотношения параметров и данных.