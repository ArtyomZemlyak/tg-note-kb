# Уточнённый скейлинг-закон Farseer: Переосмысление пропорции данных и параметров в LLM

## Обзор

Farseer - это уточнённый скейлинг-закон для больших языковых моделей (LLM), который улучшает точность прогнозирования на различных масштабах. Этот закон представляет собой значительное улучшение по сравнению с предыдущими скейлинг-законами, такими как закон Chinchilla, предлагая более точные предсказания эффективности обучения и оптимального распределения вычислительных ресурсов.

## Отличия от закона Chinchilla

### 1. Повышенная точность прогнозирования
- **Farseer** предлагает значительно лучшее соответствие эмпирическим данным по сравнению с законом Chinchilla
- Показывает превосходные возможности экстраполяции, уменьшая ошибку экстраполяции на **433%** по сравнению с Chinchilla
- Систематически строит поверхность потерь модели L(N,D), которая лучше соответствует эмпирическим данным

### 2. Уточнённая пропорция данных к параметрам
- В то время как **Chinchilla** устанавливал оптимальное соотношение D/N ≈ 20 (20 токенов на параметр) для больших моделей
- **Farseer** показывает, что в диапазоне больших мощностей соотношение D/N должно быть **меньше**
- Другими словами, для оптимального обучения нужно использовать **меньшие модели на большем объеме данных**, чем предписывал закон Chinchilla

### 3. Надежная оценка стратегии
- Позволяет с уверенностью экстраполировать выводы из небольших аблляционных исследований для предсказания эффективности на большой шкале
- Решает проблему, когда инсайты от небольших масштабов не переносятся на дорогостоящие производственные системы

## Математический каркас

Farseer использует уточнённую модель поверхности потерь L(N,D), где:
- **N** = размер модели (количество параметров)
- **D** = размер датасета (количество токенов)
- **L** = функция потерь

Хотя точная формула не раскрыта в доступных источниках, каркас систематически строит эту поверхность потерь для лучшего прогнозирования эффективности на разных масштабах.

## Практические выводы для обучения LLM

### 1. Преодоление масштабного разрыва
- Решает проблему, когда инсайты из экспериментов на малом масштабе часто не переносятся на дорогостоящие производственные системы
- Обеспечивает надежную оценку конкурирующих стратегий обучения на всех настройках (N,D)

### 2. Оптимальное распределение вычислительных ресурсов
- Предоставляет новые инсайты об оптимальном распределении вычислительных ресурсов
- Лучше отражает тонкие потребности современного обучения LLM
- Указывает на то, что оптимальный баланс между параметрами и данными отличается от того, что предполагал Chinchilla

### 3. Снижение стоимости
- Помогает решить проблему prohibitively expensive (чрезвычайно дорогого) характера обучения LLM за счет возможности использования экспериментов на малом масштабе для прогнозирования эффективности на большом масштабе
- Позволяет эффективно инновировать и тестировать различные подходы к обучению

### 4. Обобщаемые прогнозы
- Предлагает точные, надежные и высокообобщаемые прогнозы
- В отличие от предыдущих подходов, Farseer способен давать надежные прогнозы даже за пределами диапазона данных, на которых он был обучен

## Экспериментальная валидация

- Обучено приблизительно 1000 LLM на различных масштабах и конфигурациях
- Использовано примерно 3 миллиона GPU-часов NVIDIA H100
- Все модели, данные, результаты и логи опубликованы открыто на GitHub: https://github.com/Farseer-Scaling-Law/Farseer

## Связь с другими темами

- [[chinchilla_scaling_laws.md]] - Классические скейлинг-законы, с которыми сравнивается Farseer
- [[llm_scaling_architectures.md]] - Общее сравнение скейлинга различных архитектур LLM
- [[compute_optimal_training.md]] - Оптимальное распределение вычислительных ресурсов для обучения моделей

## Источники

1. [Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs](https://openreview.net/pdf?id=2Gnp8sdwVe) - Оригинальная работа о законе Farseer, представленная на NeurIPS
2. [Farseer: A Refined Scaling Law in Large Language Models](https://huggingface.co/papers/2506.10972) - Работа, описывающая улучшенные скейлинг-законы для LLM с лучшей точностью прогнозирования
3. [Chinchilla: Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) - Оригинальная работа о скейлинг-законах Chinchilla, с которой сравнивается Farseer
4. [Neural scaling law](https://en.wikipedia.org/wiki/Neural_scaling_law) - Общая информация о нейронных скейлинг-законах, включая оптимальное соотношение токенов к параметрам для закона Chinchilla