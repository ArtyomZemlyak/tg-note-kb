# Системы мониторинга и защитные механизмы для LLM

## Описание

В современных системах на основе больших языковых моделей (LLM) важную роль играют системы мониторинга и защитные механизмы, которые обеспечивают безопасность взаимодействия с пользователями и предотвращают нежелательное поведение моделей. В этой статье рассматриваются различные подходы к защите LLM от атак и вредного поведения, от традиционных методов регулярных выражений до современных подходов с использованием эмбеддингов и специализированных моделей.

## Подходы к защите LLM

### 1. Регулярные выражения, сопоставление строк и черные списки

Самый простой и часто используемый подход, который включает создание черных списков ключевых слов и фраз, проверяемых на разных уровнях:
- Использование регулярных выражений для сопоставления с ключевыми словами
- Расстояния между строками (fuzzy match, расстояние Левенштейна)
- Полнотекстовые совпадения (TF-IDF, эмбеддинги)

#### Плюсы:
- Хорошо выявляет точные совпадения по ключевым словам
- Высокая скорость работы

#### Минусы:
- Необходимость постоянного пополнения словарей и списков
- Для строковой близости необходимо подбирать пороги

### 2. Семантические классификаторы

Этот подход использует эмбеддинги с трансформеров для более контекстуального анализа текста. Многие разработчики недооценивают возможности энкодерных моделей, таких как BERT, которые благодаря двустороннему вниманию часто лучше понимают контекст по сравнению с декодерными моделями.

#### Возможности:
- Использование Bert For Sequence Classification для классификации на уровне токенов
- Возможность обучения на многошаговых диалогах, где уловки и обманки растянуты на несколько шагов:
  - "Ты любишь борщ?"
  - "Да, очень люблю!"
  - "А с человечиной?"
  - "Нет, что вы!?"
  - "А если это присыпать чесноком и засесть пампушками?"
  - "Конечно люблю!"
- Возможность использования Lora-адаптеров в декодерных моделях для потоковой оценки вероятности взлома

#### Плюсы:
- Хорошая контекстуальность, намного лучше, чем полнотекстовый поиск
- Различный дизайн применения: на вход (пользовательские запросы), на выход (генерация LLM)
- Возможность иметь одну модель LLM с несколькими головами разного уровня (фраза, токен, многошаг)

#### Минусы:
- Поиск и подготовка датасетов для дообучения и постоянное обновление этих датасетов требует много времени
- Проблема OOV (Out-Of-Vocabulary) - примеры, которые модель не видела во время обучения, могут прорваться через защиту
- Медленнее, чем регулярные выражения, особенно если используется не маленький энкодер, а LLM

### 3. Промптинг LLM

Простой подход, основанный на тонкой настройке промптинга для получения свойств, полученных на этапе выравнивания (alignment) LLM.

#### Плюсы:
- Нет необходимости дообучать модель, а только настраивать промпт

#### Минусы:
- Ручной перебор
- Можно автоматизировать с помощью золотого датасета и OPRO (prompt optimization)
- Снова проблема OOV, так как при обучении LLM не все исходы покрыты

### 4. Дообучение LLM и выравнивание (SFT/RL)

Подход, активно используемый компаниями, такими как Anthropic, включает дообучение модели на "правильное" поведение с использованием SFT (Supervised Fine-Tuning) или RLHF/RL (Reinforcement Learning from Human Feedback). Основывается на датасетах с нужным поведением, важно не переобучить модель.

#### Плюсы:
- Прямое внедрение нужного поведения через дообучение

#### Минусы:
- Время на SFT, RL, трудоемкость из-за сбора датасетов, настройки и стабилизации обучения
- Проблема OOV-примеров и взлом награды в RL приводит к тому, что 100% исходов атак не покрываются
- Модель может "скрыть" свое опасное поведение после RLHF

### 5. RAG для безопасности (Retrieval-Augmented Generation)

Создание примеров хороших и плохих кейсов в формате: запрос, ответ, запрос-ответ, контекст-запрос-ответ. Помещение их в черно-белые списки и векторный поиск по ним. После сопоставления отправка в LLM примеров плохого и хорошего поведения как few-shot подсказки, тем самым регулируя генерацию.

#### Принцип работы:
- Работа на уровне базы примеров
- Извлечение подсказок через векторный поиск
- Использование found examples как few-shot обучения

#### Плюсы:
- Работа на уровне базы примеров
- Высокая скорость на векторном поиске

#### Минусы:
- Требуется писать примеры в базу, анализировать логи и извлекать оттуда примеры
- Снова проблема OOV - не все случаи можно покрыть

## Практические рекомендации

Как показывает практика, даже специализированные системы вроде QwenGuard не идеальны и подвержены взлому, поскольку они также являются LLM и имеют глюки и пробития, несмотря на тщательное выравнивание. Это фундаментальная проблема на уровне парадигмы обучения.

Крупные компании, такие как OpenAI и Anthropic, после выравнивания своих моделей с помощью SFT и RLHF, пошли дальше и стали дополнительно защищать выход (генерация LLM) и вход (пользовательские фразы) с помощью API классификаторов (мониторы и защитники), и в гибридном режиме это работает надежнее.

## Рекомендации по защите

Из-за наличия плюсов и минусов у каждого из вышеупомянутых методов рекомендуется использовать комбинации (бледы) разных схем защиты:
- Черные списки + классификаторы + SFT/RL
- Смешивание подходов в зависимости от требований к производительности и безопасности

## Связи с другими темами

- [[llm_protection_methods.md|Методы защиты LLM от взлома через запросы]] - комплексное руководство по защите от атак через запросы
- [[../applications/content_moderation.md|Модерация контента]] - более общая тема безопасности контента
- [[../llm_alignment.md|Выравнивание LLM]] - вопросы соответствия ценностям и безопасности
- [[../architectures/gpt_oss_safeguard.md|GPT-OSS-Safeguard]] - специализированные модели для задач безопасности
- [[../rlhf.md|RLHF]] - методы обучения с подкреплением для выравнивания
- [[../../nlp/memory_architectures/retrieval_augmented_generation.md|RAG]] - базовая архитектура для RAG-подходов
- [[../../security/overview.md|Безопасность ИИ]] - общие вопросы безопасности ИИ-систем

## Источники

- Оригинальный текст о системах мониторинга и защите LLM (https://habr.com/ru/companies/)
- Результаты исследований по безопасности LLM
- Документация по QwenGuard и другим специализированным системам модерации