# Специализированные механизмы внимания

## Общее описание

Специализированные механизмы внимания представляют собой улучшенные версии стандартного механизма внимания в архитектуре трансформеров, направленные на повышение эффективности вычислений, улучшение производительности и решение специфических задач обработки последовательностей. Эти механизмы разработаны для оптимизации работы с длинными контекстами, снижения потребления памяти и улучшения качества моделей.

## Multi-Query Attention (MQA)

### Описание
Multi-Query Attention - это оптимизированный вариант многоголового внимания, в котором используются общие матрицы ключей (K) и значений (V) для всех голов внимания, в отличие от стандартного многоголового внимания, где каждая голова имеет свои собственные матрицы K и V.

### Технические детали
- В стандартном многоголовом внимании: `W^K_i` и `W^V_i` для каждой головы i
- В MQA: один общий `W^K` и `W^V` для всех голов
- Это значительно уменьшает размер KV-кеши при автoregressive генерации

### Преимущества
- **Снижение использования памяти**: Значительно уменьшает требования к KV-кешированию
- **Более быстрая генерация**: Ускоряет автoregressive инференс
- **Меньшая вычислительная сложность**: Особенно при генерации текста

### Компромиссы
- Небольшое снижение качества по сравнению с полным многоголовым вниманием
- Меньшая выразительность из-за разделяемых K и V матриц

Для более подробного описания см. [[flash_attention_and_grouped_mechanisms.md|FlashAttention и групповые механизмы внимания]].

## Grouped-Query Attention (GQA)

### Описание
Grouped-Query Attention представляет собой промежуточный вариант между стандартным многоголовым вниманием и Multi-Query Attention. В GQA головы внимания группируются, и внутри каждой группы используются общие матрицы K и V.

### Технические детали
- Головы внимания разбиваются на G групп
- Каждая группа делит K и V матрицы
- Если G равно количеству голов, это становится MQA
- Если G равно 1 (т.е. все головы в одной группе), это становится стандартным многоголовым вниманием

### Преимущества
- **Баланс между качеством и эффективностью**: Лучшее качество, чем MQA, но более эффективное, чем стандартное многоголовое внимание
- **Уменьшенный KV-кеширование**: Меньше требования к памяти по сравнению со стандартным вниманием
- **Гибкость**: Позволяет выбирать баланс между качеством и эффективностью

Для более подробного описания см. [[flash_attention_and_grouped_mechanisms.md|FlashAttention и групповые механизмы внимания]].

## Sparse Attention (Разреженное внимание)

### Описание
Разреженное внимание ограничивает количество токенов, которые могут взаимодействовать друг с другом в механизме внимания, снижая квадратичную сложность стандартного внимания.

### Варианты

#### Fixed Sparse Patterns
- Использует заранее заданные шаблоны для определения, какие токены могут обращаться к другим
- Примеры: Local attention (ограниченное локальное окно), Strided attention (равномерно распределенные токены)

#### Learnable Sparse Attention
- Структура внимания обучается в процессе тренировки
- Позволяет модели адаптироваться к структуре данных

#### DeepSeek Sparse Attention (DSA)
- Нативно обучаемый механизм разреженного внимания
- Разработан для оптимизации эффективности обучения и вывода в сценариях длинных контекстов
- Использует специализированные ядра для реализации, включая FlashMLA и индексаторы логитов

#### Mixture of Sparse Attention (MoSA)
- Новый подход к разреженному вниманию, использующий обучаемую, основанную на содержании маршрутизацию
- Вдохновлён концепцией Смесь Экспертов (Mixture of Experts, MoE), где каждый головной элемент внимания динамически выбирает разреженное подмножество токенов на основе содержания ввода
- В отличие от фиксированных шаблонов, разреженность в MoSA адаптивна и обучаема
- Утверждается, что это первый метод разреженного внимания, который последовательно превосходит традиционные модели с полным вниманием по производительности

### Преимущества
- **Сниженная вычислительная сложность**: От O(n²) до O(n) или O(n log n)
- **Улучшенная масштабируемость**: Возможность работы с более длинными последовательностями
- **Меньшее использование памяти**: Меньше KV-кеширования
- **Адаптивность**: Разреженность адаптируется к содержанию, а не использует фиксированные шаблоны
- **Лучшая производительность**: В отличие от других методов разреженного внимания, MoSA превосходит традиционные плотные модели по некоторым метрикам


## Star Attention

### Описание
Star Attention - это новая архитектура разреженного блочного внимания, разработанная NVIDIA для эффективного вывода в LLM на длинных последовательностях. Механизм позволяет достичь значительного ускорения (до 11x) при сохранении 97-100% точности по сравнению с глобальным вниманием за счет распределения вычислений по нескольким хостам и минимизации накладных расходов на коммуникацию.

### Технические детали
- Является блочно-разреженным приближением механизма внимания
- Использует двухфазный подход: сначала кодирование контекста, затем кодирование запроса и генерация токенов
- Контекст разбивается на соприкасающиеся блоки, каждый из которых (кроме первого) начинается с якорного блока (anchor block)
- Использует минимальную коммуникацию между хостами: только один скаляр (сумма экспонент) и вектор (локальный результат внимания) передаются от каждого хоста к query хосту
- Якорные блоки решают проблему attention sinks, которая возникает при независимой обработке блоков

### Применение
- Эффективный инференс LLM с длинными контекстами
- Задачи, где традиционное внимание становится вычислительно затратным
- Сценарии с распределенными вычислениями

### Преимущества
- **Значительное ускорение**: До 11x быстрее по сравнению с базовыми методами
- **Сохранение точности**: Поддержание 97-100% точности по сравнению с глобальным вниманием
- **Совместимость**: Работает с большинством Transformer-моделей, обученных с глобальным вниманием, без дополнительного обучения
- **Ортогональность**: Совместим с другими методами оптимизации, такими как Flash Attention и методы сжатия KV-кеширования
- **Масштабируемость**: Демонстрирует значительное ускорение на задачах с длинными последовательностями (от 16K до 128K токенов)

## Multihead Latent Attention (MLA)

### Описание
Multihead Latent Attention использует низкоранговые приближения стандартного многоголового внимания, проектируя скрытые векторы в низкоразмерные "латентные пространства" перед вычислением внимания.

### Технические детали
- Проектирует скрытые векторы в низкоразмерные латентные пространства
- Вычисляет внимание в этих латентных пространствах
- Минимизирует размер KV-кеширования, кешируя только низкоразмерные KV векторы

### Преимущества
- **Уменьшенный KV-кеширование**: Существенное снижение требований к памяти
- **Сохранение качества**: Сохранение важных аспектов стандартного многоголового внимания
- **Вычислительная эффективность**: Более быстрые вычисления в низкоразмерных пространствах

### Связь с Kimi Linear
- MLA используется в гибридной архитектуре Kimi Linear вместе с Kimi Delta Attention (KDA) в соотношении 3:1 для достижения высокой эффективности при сохранении качества

## Linear Attention

### Описание
Linear Attention преобразует квадратичную сложность стандартного внимания в линейную, используя различные математические приближения.

### Принцип работы
- Переписывает вычисление внимания как скалярное произведение
- Использует приближения для упрощения вычислений
- Позволяет обрабатывать очень длинные последовательности

### Преимущества
- **Линейная вычислительная сложность**: O(n) вместо O(n²)
- **Работа с очень длинными контекстами**: Возможность обработки намного более длинных последовательностей
- **Эффективное использование памяти**: Значительно меньше требования к памяти

## LoLCATs (Linearizing Large Language Models with Cascaded Amortized Variational Inference)

### Описание
LoLCATs (Linearizing Large Language Models with Cascaded Amortized Variational Inference) - это метод, разработанный для эффективного замещения квадратичного softmax-внимания на линейное внимание в больших языковых моделях, сохраняя при этом качество и точность модели. Метод представляет собой двухэтапный подход к оптимизации архитектуры внимания с использованием каскадной амортизированной вариационной инференции.

### Технические детали
- Заменяет квадратичное softmax-внимание на линейное внимание
- Использует каскадную амортизированную вариационную инференцию для эффективного аппроксимирования сложных распределений внимания
- Применяется к существующим архитектурам (например, GigaChat) как двухэтапный процесс:
  1. Этап 1: Предварительное обучение линейного приближения к традиционному вниманию
  2. Этап 2: Тонкая настройка модели с использованием линейного внимания с уменьшенным KV-кешированием

### Преимущества
- **Снижение памяти**: Значительное уменьшение требований к KV-кешированию (до 50-75% от оригинального размера)
- **Повышенная эффективность**: Линейная вычислительная сложность позволяет обрабатывать более длинные последовательности
- **Сохранение качества**: Модель сохраняет высокое качество генерации и понимания языка
- **Масштабируемость**: Улучшенная масштабируемость для более длинных контекстов

## Sliding Window Attention

### Описание
Sliding Window Attention ограничивает каждую позицию в последовательности только обращением к ограниченной области (окну) предыдущих токенов.

### Технические детали
- Устанавливает фиксированный размер окна для внимания
- Каждый токен может обращаться только к токенам в пределах этого окна
- Иногда включает глобальное внимание для особых токенов (например, CLS токена)

### Преимущества
- **Предсказуемая сложность**: O(n*w) где w - размер окна
- **Сохранение локальной информации**: Хорошо работает для задач, где локальный контекст наиболее важен
- **Эффективность**: Более быстрые вычисления и меньше памяти

## Flash Attention

### Описание
Flash Attention - это оптимизированный алгоритм вычисления внимания, который улучшает эффективность использования памяти и вычислительную эффективность за счет алгоритмических и реализационных оптимизаций.

### Особенности
- Не изменяет математический результат внимания
- Оптимизирует порядок вычислений и использование памяти
- Использует тайлинг и другие методы для повышения производительности

### Преимущества
- **Более эффективное использование памяти**: Уменьшает пиковое использование памяти
- **Более быстрые вычисления**: особенно на GPU
- **Поддержка более длинных последовательностей**: Благодаря эффективному использованию памяти

Для более подробного описания см. [[flash_attention_and_grouped_mechanisms.md|FlashAttention и групповые механизмы внимания]].

## Adamas Attention

### Описание
Adamas - новая технология, которая ускоряет механизм self-attention до 4.4 раз, сохраняя качество обработки длинных контекстов. Основная идея заключается в том, чтобы сделать внимание «разреженным» без потери смысла. В отличие от классического attention, где каждый токен сравнивается со всеми остальными, модель Adamas использует только 128 релевантных токенов для каждого запроса.

### Технические детали
- Применение преобразования Адамара к векторам запросов и ключей
- Сглаживание экстремальных значений и возможность сжатия
- Разбиение значений на четыре уровня и кодирование в 2 бита
- Компактные коды хранятся в кэше
- Быстрое вычисление сходства с помощью метрики Manhattan distance
- Выбор наиболее важных токенов и выполнение обычного attention только над ними

### Преимущества
- **Значительное ускорение**: До 4.4× ускорения self-attention и около 1.5× ускорения инференса в целом
- **Сохранение качества**: Точность остаётся почти такой же, как у полного внимания
- **Эффективность памяти**: Практически не требует дополнительной памяти, лишь небольшой 2-битный код на токен
- **Простота интеграции**: Может встраиваться в существующие LLM без переобучения

## Log-Linear Attention

### Описание
Log-Linear Attention - это новая архитектура механизма внимания, разработанная для повышения эффективности вычислений в моделях машинного обучения, особенно в трансформерах. Название указывает на логарифмически-линейную сложность вычислений, что позволяет достичь более высокой эффективности по сравнению с традиционным механизмом внимания. Это промежуточный вариант между стандартным Attention и линейными по длине альтернативами. Токены разбиваются на корзинки с экспоненциально растущим числом токенов, где свежие токены получают больший вес. Механизм вычисляет линейный attention по корзинкам, а затем суммирует результаты с обучаемыми коэффициентами (предсказанными отдельной MLP).

### Технические детали
- В отличие от стандартного механизма внимания, который требует квадратичного времени для вычисления всех парных взаимодействий между токенами, Log-Linear Attention использует приближенные методы или специальные структуры данных для эффективного вычисления внимания
- Токены разбиваются на корзинки с экспоненциально растущим размером
- Сложность: O(n log n), где n - длина последовательности
- Может применяться поверх Linear Attention, Mamba-2 и DeltaNet
- Можно представить как структурированную матрицу HODLR (Hierarchically Off-Diagonal Low-Rank)
- Для эффективной реализации используются деревья Фенвика
- Стремится улучшить вычислительную эффективность, уменьшая сложность с O(n²) до O(n log n)

## Differential Attention

### Описание
Differential Attention - это механизм внимания, используемый в Дифференциальном Трансформере, который вычисляет внимание на основе разницы между представлениями токенов, а не их абсолютных значений. Это позволяет модели лучше захватывать относительные отношения между токенами в последовательности.

### Технические детали
- Вместо вычисления внимания на основе абсолютных представлений токенов, дифференциальное внимание основывается на разностях между ними
- Позволяет модели естественным образом учитывать относительные позиции и отношения между токенами
- Уменьшает зависимость от позиционных кодировок
- Улучшает интерпретируемость паттернов внимания
- Сохраняет глобальную информацию, но изменяет способ её обработки

### Преимущества
- Лучшее понимание относительных позиций
- Повышенная производительность на задачах, требующих понимания относительных отношений
- Сниженная потребность в сложных схемах позиционного кодирования
- Улучшенная интерпретируемость механизма внимания

### Преимущества
- **Высокая вычислительная эффективность**: Значительно снижает сложность с O(n²) до O(n log n)
- **Экономия памяти**: Требует меньше памяти для хранения промежуточных вычислений по сравнению с O(n²) механизмами
- **Баланс между эффективностью и качеством**: Промежуточная сложность между O(n) и O(n²)
- **Гибкость**: Может использоваться как надстройка над различными линейными архитектурами
- **Математическая основа**: Использует структурированные матрицы HODLR
- **Эмпирическое улучшение**: Показывает улучшения на некоторых задачах по сравнению с базовыми линейными архитектурами
- **Сохранение качества**: Предположительно обеспечивает качество, сравнимое с традиционными механизмами внимания
- **Быстродействие**: Улучшает скорость инференса и обучения для длинных последовательностей

## Multi-Token Attention (MTA)

### Описание
Multi-Token Attention (MTA) - это новый механизм внимания, разработанный для решения проблемы "бутылочного горлышка" в стандартных механизмах внимания, когда веса определяются одним вектором запроса и одним вектором ключа. MTA позволяет определять веса внимания на основе нескольких векторов query и key одновременно, что улучшает способность модели комбинировать информацию из различных частей контекста.

### Технические детали
MTA включает три ключевых компонента:
- **Key-Query Convolution**: комбинирует несколько векторов key и query внутри головы внимания, применяя обучаемую свёртку к логитам внимания
- **Head Mixing Convolution**: позволяет перемешивать информацию между разными головами внимания в пределах одного временного шага
- **Group Normalization with Depth Scaling**: улучшает поток градиентов, используя GroupNorm и масштабирование для каждой головы

Свертки могут применяться до (pre-softmax) или после (post-softmax) вычисления softmax, что приводит к мультипликативным или аддитивным взаимодействиям соответственно.

### Преимущества
- **Лучшая способность комбинировать информацию из нескольких токенов**: особенно полезно для задач, требующих сопоставления нескольких элементов в последовательности
- **Улучшенные результаты на задачах с долгосрочными зависимостями**: эксперименты показывают улучшение на LAMBADA и NeedleIn-A-Haystack
- **Более точное моделирование взаимодействий**: позволяет модели находить более сложные паттерны и зависимости

### Компромиссы
- **Повышенные требования к памяти**: в 3 раза больше памяти по сравнению с обычным вниманием
- **Более высокие вычислительные затраты**: в 5 раз больше FLOPS в текущей реализации (в основном из-за отсутствия оптимизированного CUDA ядра)

## Сравнение механизмов внимания

| Механизм | Сложность | Память | Качество | Описание |
|----------|-----------|--------|----------|----------|
| Стандартное MHA | O(n²) | O(n²) | Высокое | Традиционное многоголовое внимание |
| MQA | O(n²) | O(n) | Немного ниже | Разделяет K/V для всех голов |
| GQA | O(n²) | O(n) | Среднее | Групповое разделение K/V |
| Sparse Attention | O(n) или O(n log n) | O(n) | Зависит от шаблона | Ограниченные соединения |
| Linear Attention | O(n) | O(n) | Ниже для сложных зависимостей | Приближенные вычисления |
| Sliding Window | O(n*w) | O(n*w) | Хорошее для локальных задач | Ограниченное окно внимания |
| Adamas | O(n) | O(n) | Высокое | Разреженное внимание с компрессией |
| Log-Linear | O(n log n) | O(n log n) | Высокое | Баланс между точностью и эффективностью |
| Multi-Token Attention | O(n²) | O(n²) | Высокое | Комбинирование информации из нескольких токенов |
| Star Attention | O(n) | O(n) | Высокое | Блочно-разреженное внимание с распределением по хостам |
| Ouro Looped Architecture | O(n) | O(n) | Высокое | Итеративные вычисления в латентном пространстве вместо явного внимания |
| Kimi Delta Attention (KDA) | O(n) | O(n) | Высокое | Мелкозернистое гейтирование на основе Delta Rule, гибрид с MLA |

## Применение в современных моделях

- **GPT-3.5/GPT-4**: Используют стандартное или модифицированное многоголовое внимание
- **PaLM**: Используют шаблоны разреженного внимания
- **Mixture of Attention Heads**: Комбинация различных типов внимания
- **DeepSeek-V3**: Использует DeepSeek Sparse Attention (DSA)
- **RWKV**: Используют рекуррентно-взвешенные ключевые-значение механизмы (альтернатива трансформерам)
- **Adamas**: Использует разреженное внимание с компрессией и быстрым вычислением сходства
- **MTA**: Используется в моделях, требующих комбинации информации из нескольких токенов
- **Star Attention**: Используется для эффективного инференса в LLM с длинными контекстами, может быть интегрирован в существующие модели без переобучения
- **MoSA**: Применяется в новых архитектурах, где требуется высокая эффективность и адаптивная разреженность на основе содержания
- **Kimi Linear**: Использует гибридную архитектуру с Kimi Delta Attention (KDA) и Multi-Head Linear Attention (MLA) в соотношении 3:1 для достижения высокой эффективности и качества
- **Qwen3-Next**: Использует гибрид Gated DeltaNet и Gated Attention в соотношении 3:1

## Выбор механизма внимания

Выбор конкретного механизма внимания зависит от:
- **Типа задачи** (длинные контексты, реальное время, точность)
- **Вычислительных ограничений** (память, время инференса)
- **Требований к качеству** (баланс между эффективностью и точностью)
- **Доступных ресурсов для обучения**

## Связи с другими темами

- [[cache_based_model_communication.md]] - Использование KV-кеширования для коммуникации между моделями
- [[llm_architectures_comparison.md]] - Общее сравнение архитектур LLM
- [[models/deepseek_sparse_attention.md]] - Подробное описание DeepSeek Sparse Attention
- [[models/nanoGPT.md]] - Пример базовой архитектуры трансформера
- [[models/adamas_attention_mechanism.md]] - Подробное описание технологии Adamas
- [[log_linear_attention.md]] - Подробное описание Log-Linear Attention
- [[models/kimi_linear.md]] - Подробное описание Kimi Linear, гибридной архитектуры с Kimi Delta Attention и MLA
- [[architectures/kimi_delta_attention.md]] - Подробное описание Kimi Delta Attention (KDA), улучшенной версии Gated DeltaNet
- [[architectures/gated_deltanet.md]] - Подробное описание Gated DeltaNet, базовой архитектуры для KDA
- [[../nlp/transformers/differential_transformer.md]] - Дифференциальный трансформер с механизмом дифференциального внимания, улучшающий понимание относительных отношений между токенами
- [[multi_token_attention.md]] - Подробное описание Multi-Token Attention, нового подхода к комбинированию информации из нескольких токенов
- [[star_attention_mechanism.md]] - Подробное описание Star Attention, блочно-разреженного механизма для эффективного инференса в LLM с длинными контекстами
- [[mixture_of_sparse_attention.md]] - Подробное описание Mixture of Sparse Attention (MoSA), нового подхода к разреженному вниманию с обучаемой маршрутизацией выбора эксперта
- [[ouro_llm.md]] - Ouro-LLM: альтернативный подход к эффективности через зацикленные архитектуры с итеративными вычислениями в латентном пространстве

## Источники

- Исследования по MQA и GQA от Microsoft и других
- Статья "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"
- Исследования по Flash Attention от Stanford и других
- Техническая документация современных LLM