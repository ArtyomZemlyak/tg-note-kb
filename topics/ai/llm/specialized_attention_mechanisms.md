# Специализированные механизмы внимания

## Общее описание

Специализированные механизмы внимания представляют собой улучшенные версии стандартного механизма внимания в архитектуре трансформеров, направленные на повышение эффективности вычислений, улучшение производительности и решение специфических задач обработки последовательностей. Эти механизмы разработаны для оптимизации работы с длинными контекстами, снижения потребления памяти и улучшения качества моделей.

## Multi-Query Attention (MQA)

### Описание
Multi-Query Attention - это оптимизированный вариант многоголового внимания, в котором используются общие матрицы ключей (K) и значений (V) для всех голов внимания, в отличие от стандартного многоголового внимания, где каждая голова имеет свои собственные матрицы K и V.

### Технические детали
- В стандартном многоголовом внимании: `W^K_i` и `W^V_i` для каждой головы i
- В MQA: один общий `W^K` и `W^V` для всех голов
- Это значительно уменьшает размер KV-кеши при автoregressive генерации

### Преимущества
- **Снижение использования памяти**: Значительно уменьшает требования к KV-кешированию
- **Более быстрая генерация**: Ускоряет автoregressive инференс
- **Меньшая вычислительная сложность**: Особенно при генерации текста

### Компромиссы
- Небольшое снижение качества по сравнению с полным многоголовым вниманием
- Меньшая выразительность из-за разделяемых K и V матриц

## Grouped-Query Attention (GQA)

### Описание
Grouped-Query Attention представляет собой промежуточный вариант между стандартным многоголовым вниманием и Multi-Query Attention. В GQA головы внимания группируются, и внутри каждой группы используются общие матрицы K и V.

### Технические детали
- Головы внимания разбиваются на G групп
- Каждая группа делит K и V матрицы
- Если G равно количеству голов, это становится MQA
- Если G равно 1 (т.е. все головы в одной группе), это становится стандартным многоголовым вниманием

### Преимущества
- **Баланс между качеством и эффективностью**: Лучшее качество, чем MQA, но более эффективное, чем стандартное многоголовое внимание
- **Уменьшенный KV-кеширование**: Меньше требования к памяти по сравнению со стандартным вниманием
- **Гибкость**: Позволяет выбирать баланс между качеством и эффективностью

## Sparse Attention (Разреженное внимание)

### Описание
Разреженное внимание ограничивает количество токенов, которые могут взаимодействовать друг с другом в механизме внимания, снижая квадратичную сложность стандартного внимания.

### Варианты

#### Fixed Sparse Patterns
- Использует заранее заданные шаблоны для определения, какие токены могут обращаться к другим
- Примеры: Local attention (ограниченное локальное окно), Strided attention (равномерно распределенные токены)

#### Learnable Sparse Attention
- Структура внимания обучается в процессе тренировки
- Позволяет модели адаптироваться к структуре данных

#### DeepSeek Sparse Attention (DSA)
- Нативно обучаемый механизм разреженного внимания
- Разработан для оптимизации эффективности обучения и вывода в сценариях длинных контекстов
- Использует специализированные ядра для реализации, включая FlashMLA и индексаторы логитов

### Преимущества
- **Сниженная вычислительная сложность**: От O(n²) до O(n) или O(n log n)
- **Улучшенная масштабируемость**: Возможность работы с более длинными последовательностями
- **Меньшее использование памяти**: Меньше KV-кеширования

## Multihead Latent Attention (MLA)

### Описание
Multihead Latent Attention использует низкоранговые приближения стандартного многоголового внимания, проектируя скрытые векторы в низкоразмерные "латентные пространства" перед вычислением внимания.

### Технические детали
- Проектирует скрытые векторы в низкоразмерные латентные пространства
- Вычисляет внимание в этих латентных пространствах
- Минимизирует размер KV-кеширования, кешируя только низкоразмерные KV векторы

### Преимущества
- **Уменьшенный KV-кеширование**: Существенное снижение требований к памяти
- **Сохранение качества**: Сохранение важных аспектов стандартного многоголового внимания
- **Вычислительная эффективность**: Более быстрые вычисления в низкоразмерных пространствах

## Linear Attention

### Описание
Linear Attention преобразует квадратичную сложность стандартного внимания в линейную, используя различные математические приближения.

### Принцип работы
- Переписывает вычисление внимания как скалярное произведение
- Использует приближения для упрощения вычислений
- Позволяет обрабатывать очень длинные последовательности

### Преимущества
- **Линейная вычислительная сложность**: O(n) вместо O(n²)
- **Работа с очень длинными контекстами**: Возможность обработки намного более длинных последовательностей
- **Эффективное использование памяти**: Значительно меньше требования к памяти

## Sliding Window Attention

### Описание
Sliding Window Attention ограничивает каждую позицию в последовательности только обращением к ограниченной области (окну) предыдущих токенов.

### Технические детали
- Устанавливает фиксированный размер окна для внимания
- Каждый токен может обращаться только к токенам в пределах этого окна
- Иногда включает глобальное внимание для особых токенов (например, CLS токена)

### Преимущества
- **Предсказуемая сложность**: O(n*w) где w - размер окна
- **Сохранение локальной информации**: Хорошо работает для задач, где локальный контекст наиболее важен
- **Эффективность**: Более быстрые вычисления и меньше памяти

## Flash Attention

### Описание
Flash Attention - это оптимизированный алгоритм вычисления внимания, который улучшает эффективность использования памяти и вычислительную эффективность за счет алгоритмических и реализационных оптимизаций.

### Особенности
- Не изменяет математический результат внимания
- Оптимизирует порядок вычислений и использование памяти
- Использует тайлинг и другие методы для повышения производительности

### Преимущества
- **Более эффективное использование памяти**: Уменьшает пиковое использование памяти
- **Более быстрые вычисления**: особенно на GPU
- **Поддержка более длинных последовательностей**: Благодаря эффективному использованию памяти

## Adamas Attention

### Описание
Adamas - новая технология, которая ускоряет механизм self-attention до 4.4 раз, сохраняя качество обработки длинных контекстов. Основная идея заключается в том, чтобы сделать внимание «разреженным» без потери смысла. В отличие от классического attention, где каждый токен сравнивается со всеми остальными, модель Adamas использует только 128 релевантных токенов для каждого запроса.

### Технические детали
- Применение преобразования Адамара к векторам запросов и ключей
- Сглаживание экстремальных значений и возможность сжатия
- Разбиение значений на четыре уровня и кодирование в 2 бита
- Компактные коды хранятся в кэше
- Быстрое вычисление сходства с помощью метрики Manhattan distance
- Выбор наиболее важных токенов и выполнение обычного attention только над ними

### Преимущества
- **Значительное ускорение**: До 4.4× ускорения self-attention и около 1.5× ускорения инференса в целом
- **Сохранение качества**: Точность остаётся почти такой же, как у полного внимания
- **Эффективность памяти**: Практически не требует дополнительной памяти, лишь небольшой 2-битный код на токен
- **Простота интеграции**: Может встраиваться в существующие LLM без переобучения

## Сравнение механизмов внимания

| Механизм | Сложность | Память | Качество | Описание |
|----------|-----------|--------|----------|----------|
| Стандартное MHA | O(n²) | O(n²) | Высокое | Традиционное многоголовое внимание |
| MQA | O(n²) | O(n) | Немного ниже | Разделяет K/V для всех голов |
| GQA | O(n²) | O(n) | Среднее | Групповое разделение K/V |
| Sparse Attention | O(n) или O(n log n) | O(n) | Зависит от шаблона | Ограниченные соединения |
| Linear Attention | O(n) | O(n) | Ниже для сложных зависимостей | Приближенные вычисления |
| Sliding Window | O(n*w) | O(n*w) | Хорошее для локальных задач | Ограниченное окно внимания |
| Adamas | O(n) | O(n) | Высокое | Разреженное внимание с компрессией |

## Применение в современных моделях

- **GPT-3.5/GPT-4**: Используют стандартное или модифицированное многоголовое внимание
- **PaLM**: Использует шаблоны разреженного внимания
- **Mixture of Attention Heads**: Комбинация различных типов внимания
- **DeepSeek-V3**: Использует DeepSeek Sparse Attention (DSA)
- **RWKV**: Использует рекуррентно-взвешенные ключевые-значение механизмы (альтернатива трансформерам)
- **Adamas**: Использует разреженное внимание с компрессией и быстрым вычислением сходства

## Выбор механизма внимания

Выбор конкретного механизма внимания зависит от:
- **Типа задачи** (длинные контексты, реальное время, точность)
- **Вычислительных ограничений** (память, время инференса)
- **Требований к качеству** (баланс между эффективностью и точностью)
- **Доступных ресурсов для обучения**

## Связи с другими темами

- [[llm_architectures_comparison.md]] - Общее сравнение архитектур LLM
- [[models/deepseek_sparse_attention.md]] - Подробное описание DeepSeek Sparse Attention
- [[models/nanoGPT.md]] - Пример базовой архитектуры трансформера
- [[models/adamas_attention_mechanism.md]] - Подробное описание технологии Adamas

## Источники

- Исследования по MQA и GQA от Microsoft и других
- Статья "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"
- Исследования по Flash Attention от Stanford и других
- Техническая документация современных LLM