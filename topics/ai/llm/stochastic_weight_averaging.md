# Stochastic Weight Averaging (SWA): Усреднение весов в процессе обучения

## Краткое описание

Stochastic Weight Averaging (SWA) - это метод оптимизации нейронных сетей, при котором усреднение весов модели происходит в процессе обучения для улучшения обобщающей способности. Техника особенно эффективна для обучения LLM и других глубоких моделей.

## Контекст и проблема

Классические методы оптимизации (например, SGD, Adam) могут застревать в острых минимумах функции потерь, что приводит к худшей обобщающей способности. SWA помогает находить более широкие и устойчивые минимумы, улучшая стабильность и качество модели.

## Подход: SWA (Stochastic Weight Averaging)

### Основная идея

SWA усредняет веса модели на протяжении всего процесса обучения, а не только в конце. Это позволяет модели находить более широкие минимумы в пространстве потерь, которые обычно ассоциируются с лучшей обобщающей способностью.

### Алгоритм

1. **Обычный этап обучения**: Первоначальный этап обучения с использованием стандартного оптимизатора (SGD, Adam и т.д.)
2. **Переключение на SWA**: После достижения определенного прогресса обучения (например, 75% эпох) переключение на усреднение
3. **Усреднение весов**: Периодическое обновление усредненных весов:
   ```
   θ_swa = (n * θ_swa + θ_current) / (n + 1)
   ```
   где θ_swa - усредненные веса, θ_current - текущие веса, n - количество обновлений

### Варианты SWA

#### SWA с циклическим LR

Использование циклической схемы изменения learning rate для лучшего охвата пространства весов.

#### SWA с остыванием (SWA with annealing)

Применение постепенно уменьшающегося learning rate в финальной фазе SWA.

#### Обновленный SWA (SWATS)

Комбинация Adam и SGD с помощью SWA для адаптивного переключения между оптимизаторами.

## Преимущества SWA

- **Улучшенная обобщающая способность**: Более широкие минимумы приводят к лучшему обобщению
- **Стабильность**: Усреднение снижает влияние шума в градиентах
- **Простота реализации**: Требует минимальных изменений в существующих системах обучения
- **Совместимость**: Работает с большинством существующих оптимизаторов

## Отличия от Model Souping

- **Время применения**: SWA работает в процессе обучения, а Souping - после обучения различных чекпоинтов
- **Механизм усреднения**: SWA усредняет веса одного процесса обучения, а Souping - объединяет разные обученные модели
- **Цель оптимизации**: SWA улучшает один процесс обучения, а Souping объединяет разные подходы к решению задачи

## Практические применения

### В обучении LLM

- Улучшение стабильности процесса предобучения
- Повышение качества моделей на различных бенчмарках
- Снижение вариативности результатов при разных запусках

### В тонкой настройке

- Улучшение обобщающей способности при адаптации к специфическим задачам
- Снижение вероятности переобучения на небольших датасетах
- Повышение стабильности результатов тонкой настройки

## Связи с другими темами

- [[model_souping.md]] - Метод, применяемый после обучения, в отличие от SWA
- [[checkpoint_averaging.md]] - Родственный метод усреднения, применяемый после обучения
- [[optimization/optimization_algorithms.md]] - Другие методы оптимизации
- [[optimization/deep_optimizers.md]] - Другие методы оптимизации для глубоких моделей

## Источники

1. [Stochastic Weight Averaging in PyTorch](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/) - Официальный блог PyTorch о методе SWA
2. [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254) - Статья, обсуждающая связанные методы усреднения чекпоинтов
3. [Original Research on SWA](https://arxiv.org) - Исследования, лежащие в основе метода Stochastic Weight Averaging