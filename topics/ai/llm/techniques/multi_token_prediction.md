# Многотокенное предсказание (Multi-Token Prediction, MTP)

## Краткое описание

Многотокенное предсказание (Multi-Token Prediction, MTP) — подход в обучении языковых моделей, при котором модель обучается предсказывать не только следующий токен, но и несколько последующих токенов одновременно. Это позволяет модели лучше планировать на несколько шагов вперед и ускоряет процесс обучения и инференса.

## Принцип работы

В отличие от традиционного next-token prediction, где модель предсказывает только один следующий токен на основе предыдущего контекста, MTP обучает модель предсказывать сразу 2-3 и более следующих токенов. Это особенно эффективно при использовании архитектур типа Virtual Width Networks (VWN), которые расширяют пространство эмбеддингов, позволяя лучше упаковывать планирование на несколько шагов вперёд.

## Преимущества

- **Ускорение обучения**: Модели, использующие MTP, могут обучаться быстрее, так как они получают больше информации о будущем контексте за каждый шаг
- **Улучшенное планирование**: За счёт предсказания нескольких токенов, модель может лучше планировать структуру выхода
- **Эффективность с VWN**: Особенно эффективен при использовании с Virtual Width Networks, где расширенное пространство эмбеддингов позволяет лучше кодировать информацию для многотокенного предсказания

## Применение

- В архитектуре Qwen3 для оптимизации инференса и ускорения обучения
- В аудио-процессинге (например, в Qwen3-Omni), где MTP-модель используется для предсказания всех оставшихся RVQ-токенов сразу, а не последовательно
- Сочетание с Virtual Width Networks для ускорения обучения MoE моделей

## Примеры использования

### В аудио-генерации Qwen3-Omni
- Используется MTP-модуль (авторегрессионная dense-модель размером 80М)
- Вместо последовательного предсказания RVQ-токенов, предсказываются сразу все токены (multi token prediction)

## Ограничения

- Требует более сложного подхода к обучению, так как ошибка может накапливаться быстрее
- Может быть менее точным для более длинных последовательностей, чем next-token prediction
- Требует больше памяти при обучении из-за необходимости хранения нескольких прогнозов

## Источники

- Статья о применении MTP в сочетании с Virtual Width Networks от исследователей из ByteDance
- Документация по архитектуре Qwen3

## См. также

- [[../nlp/transformers/virtual_width_networks.md]] - виртуальные широкие сети, особенно эффективные с MTP
- [[../nlp/transformers/transformer_architecture.md]] - базовая архитектура, на которой реализуется MTP
- [[../../architectures/qwen3_next.md]] - архитектура Qwen3, включающая MTP
- [[../nlp/transformers/next_gen_transformer_architectures.md]] - перспективные архитектуры трансформеров и механизмы внимания