# Off-Policy Sequence Masking в обучении с подкреплением для LLM

## Общее описание

Off-Policy Sequence Masking - это техника стабилизации обучения с подкреплением (RL) в больших языковых моделях, разработанная для решения проблемы off-policy поведения при масштабном RL. Техника была впервые успешно применена в обучении DeepSeek V3.2, где модели генерировали много вариантов ответов, а затем использовали их для нескольких шагов оптимизации.

## Контекст и проблема

### Проблема эффективности GPU в масштабном RL

Одна из больших проблем масштабного RL - это эффективность работы GPU. Многие фреймворки разделяют серверы на тренировочные и rollout-серверы, делающие генерации в параллель. В силу разных причин так бывает, что тренировочные серверы ждут, пока rollout-серверы догенерируют часть ответов, чтобы сделать оценку.

### Подход DeepSeek

DeepSeek применяют иной подход: они генерируют сразу много вариантов, а затем разбивают их на маленькие батчи и делают несколько шагов оптимизации. Но эта практика неизбежно привносит поведение off-policy, когда обновления считаются не для текущей модели, а для неё в прошлом (после первого шага оптимизации модель уже не та же). И в какой-то момент тренировка может развалиться.

## Техника Off-Policy Sequence Masking

### Принцип работы

- Фильтруют по KL-дивергенции семплы с негативным advantage
- Маскируют последовательности, которые отклоняются от исходной модели больше некоторого порога
- Отклонения оцениваются по сравнению с качеством средней генерации в группе

### Как применяется

1. Модель генерирует много вариантов ответов (rollouts)
2. Сравниваются генерации по качеству (оценка через reward модели или тесты)
3. Последовательности, которые хуже средней в группе генераций по оценке, анализируются на предмет отклонения
4. Если отклонение превышает установленный порог, последовательность маскируется и не используется для обучения

## Технические детали

### KL-дивергенция как метрика отклонения

- Используется для измерения отклонения новой политики от исходной
- Порог устанавливается эмпирически в зависимости от задачи
- Позволяет количественно оценить, насколько сильно новая генерация отклоняется от "ожидаемого" поведения

### Advantages и Disadvantages

#### Преимущества:
- Стабилизация процесса обучения с подкреплением
- Предотвращение деградации модели из-за сильных off-policy обновлений
- Позволяет использовать более агрессивные стратегии генерации в RL

#### Ограничения:
- Ограниченная гибкость в изучении новых стратегий
- Необходимость тонкой настройки порогов
- Может замедлить обучение в некоторых случаях

## Применения

### В DeepSeek V3.2

- Применяется в масштабном RL для стабилизации обучения
- Позволяет использовать более 10% вычислительных мощностей на RL (по сравнению с 1% для R1)
- Важная часть архитектуры подготовки агентных систем

### В других моделях

- Аналогичные техники используются в OpenAI, Anthropic и других системах
- Полезна в задачах, где off-policy обучение неизбежно (например, при использовании синтетических данных)

## Влияние на обучение

### Стабильность

- Позволяет избежать деградации модели во время тренировки
- Повышает надежность RL процессов
- Делает процесс обучения более предсказуемым

### Качество модели

- Сохраняет качество при использовании сложных RL методов
- Позволяет масштабировать RL до больших моделей
- Повышает эффективность использования синтетических данных

## Сравнение с альтернативными методами

### Классический on-policy RL
- Требует ждать завершения всех генераций перед обновлением
- Менее эффективное использование GPU
- Более стабильный, но более медленный

### Pure off-policy без маскировки
- Более агрессивное обучение
- Высокий риск нестабильности
- Может привести к деградации модели

### Off-Policy Sequence Masking
- Баланс между эффективностью и стабильностью
- Позволяет использовать преимущества off-policy обучения
- Сохраняет стабильность за счет адаптивной фильтрации

## Будущие направления

### Адаптивные пороги
- Разработка методов автоматической настройки порогов маскировки
- Использование динамических критериев в зависимости от состояния обучения

### Интеграция с другими методами стабилизации
- Комбинирование с PPO, DPO и другими методами
- Использование в гибридных RL подходах

## Связи с другими темами

- [[deepseek_v3_2_reinforcement_learning_agent_training.md]] - подробности о применении в DeepSeek V3.2
- [[ai/reinforcement_learning/ppo_algorithm.md]] - связь с другими методами RL
- [[ai/llm/reinforcement_learning/rler_reinforcement_learning_with_evolving_rubrics.md]] - современные методы RL
- [[ai/llm/synthetic_training_data.md]] - связь с обучением на синтетических данных
- [[reinforcement_learning_in_llms.md]] - общие принципы RL в LLM

## Связи с другими темами

- [[deepseek_v3_2_reinforcement_learning_agent_training.md]] - подробности о применении в DeepSeek V3.2
- [[ai/reinforcement_learning/ppo_algorithm.md]] - связь с другими методами RL
- [[ai/llm/reinforcement_learning/rler_reinforcement_learning_with_evolving_rubrics.md]] - современные методы RL
- [[ai/llm/synthetic_training_data.md]] - связь с обучением на синтетических данных
- [[reinforcement_learning_in_llms.md]] - общие принципы RL в LLM
- [[thinking_retention_mechanism.md]] - метод сохранения контекста в агентных системах
- [[generative_reward_model_grm.md]] - оценка сгенерированных последовательностей
- [[agent_training_with_synthetic_data.md]] - подготовка агентов с синтетическими данными

## Источники

- Технический отчет: https://huggingface.co/deepseek-ai/DeepSeek-V3.2
- Статья о DeepSeek V3.2: "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models"
- Исследования по стабильности RL в больших языковых моделях
- Материалы по Off-Policy Learning в контексте LLM