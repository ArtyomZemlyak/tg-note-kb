# DLLM (Simple Diffusion Language Modeling) - библиотека

## Общее описание

DLLM (Simple Diffusion Language Modeling) - это библиотека, объединяющая обучение и оценку диффузионных языковых моделей (DLLM), с акцентом на прозрачность и воспроизводимость в процессе разработки. Библиотека предоставляет унифицированный фреймворк для работы с различными моделями диффузионного языкового моделирования.

## Основные возможности

### 1. Масштабируемые пайплайны обучения
- Вдохновлены тренером библиотеки transformers
- Поддержка LoRA (Low-Rank Adaptation), DeepSpeed и FSDP (Fully Sharded Data Parallel)
- Возможности распределенного обучения

### 2. Унифицированные пайплайны оценки
- Вдохновлены подходом lm-evaluation-harness
- Абстрагирует детали инференса
- Простая настройка под задачу

### 3. Предустановленные рецепты
- Минимальные рецепты предобучения/дообучения/оценки для моделей с открытыми весами
- Реализации алгоритмов обучения, таких как Edit Flows

## Архитектура проекта

### Структура проекта

```
dllm/
├── core/                  # Основные переиспользуемые модули
│   ├── generation/
│   ├── schedulers/
│   └── trainers/
├── data/
├── pipelines/             # Специфичные для модели пайплайны
│   ├── bert/
│   ├── dream/
│   ├── editflow/
│   └── llada/
└── utils/
examples/
├── bert/
├── dream/
├── editflow/
└── llada/
```

### Ключевые компоненты

- **Core Modules**: Общие функции, используемые во всех пайплайнах
- **Pipelines**: Специфичные реализации обучения и инференса для моделей
- **Data**: Утилиты обработки данных
- **Tools/Utils**: Вспомогательные функции и утилиты

## Поддерживаемые модели

### 1. LLaDA (Large Language Diffusion Assistant)
- Предобучение, дообучение и оценка
- Основана на диффузионных принципах для генерации текста

### 2. Dream
- Предобучение, дообучение и оценка
- Использует диффузионные процессы для генерации языка

### 3. BERT
- Дообучение для чат-ботов с малым объемом параметров
- Применение диффузионных методов к маскированному языковому моделированию

### 4. EditFlow
- Образовательный эталон для обучения моделей с операциями редактирования
- Используется для понимания и разработки новых диффузионных подходов

## Технические детали

### Требования к установке
- Python 3.10
- PyTorch 2.6.0 с CUDA 12.4
- Дополнительные зависимости для фреймворка оценки

### Примеры команд обучения

#### Локальное обучение с ZeRO-2:
```bash
accelerate launch --config_file scripts/accelerate_configs/zero2.yaml \
    examples/llada/sft.py --num_train_epochs 4 \
    --load_in_4bit True --lora True
```

#### Обучение на кластере Slurm:
```bash
sbatch --gres=gpu:8 scripts/train.slurm.sh \
    --accelerate_config "fsdp" \
    --script_path "examples/llada/sft.py" \
    --num_train_epochs 4
```

## Примеры обучения

Библиотека предоставляет примеры для различных сценариев обучения:
- Предобучение с распределенными методами (DDP, ZeRO, FSDP)
- Контролируемое дообучение с LoRA и 4-битной квантизацией
- Поддержка подмножеств наборов данных и их конкатенации

## Система оценки

- Интегрирована с фреймворком lm-evaluation-harness
- Поддержка различных бенчмарков, включая MMLU_Pro
- Унифицированные пайплайны оценки с поддержкой шаблонов чатов

## Ключевые преимущества

1. **Унифицированный подход**: Единая библиотека для обучения и оценки
2. **Независимость от модели**: Поддержка нескольких типов диффузионных моделей
3. **Масштабируемость**: Поддержка распределенного обучения
4. **Воспроизводимость**: Четкие рецепты и примеры
5. **Расширяемость**: Модульная архитектура для добавления новых моделей

## Недавние разработки

- Выпуск BERTs с дообучением для выполнения инструкций: ModernBERT-{large,base}-chat-v0
- Доказательство концепции, показывающее, что внутренние знания BERT можно использовать для генеративных задач через маскированное обучение инструкциям

## Связи с другими темами

- [[../architectures/diffusion/diffusion_llm_architectures.md]] - Общие архитектуры диффузионных LLM
- [[../diffusion_models.md]] - Общее описание диффузионных моделей
- [[../architectures/diffusion/text_diffusion_models.md]] - Текстовые диффузионные модели
- [[../architectures/diffusion/bert_diffusion_connection.md]] - Связь BERT и диффузионных моделей
- [[../pretraining_techniques.md]] - Методы предобучения
- [[../techniques/lora_optimization.md]] - Low-Rank Adaptation (LoRA) оптимизация

## Источники

1. [DLLM GitHub Repository](https://github.com/ZHZisZZ/dllm) - Официальный репозиторий библиотеки DLLM с документацией, примерами и исходным кодом