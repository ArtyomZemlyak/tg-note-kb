# MoE-Quant

## Описание

MoE-Quant - это фреймворк для квантизации моделей с архитектурой Mixture of Experts (MoE). Этот инструмент предназначен для сжатия и оптимизации MoE-моделей, позволяя уменьшить требования к памяти и ускорить инференс при сохранении приемлемой точности модели.

## Ключевые особенности

### Пост-обучение квантования (Post-Training Quantization)
- Поддержка пост-обучения квантования для моделей MoE
- Включает в себя методики data-aware сжатия
- Позволяет квантовать уже обученные модели без необходимости в повторном обучении

### Бенчмаркинг
- Официальная реализация для бенчмаркинга различных методов квантования MoE моделей
- Позволяет сравнивать эффективность разных подходов к квантованию

### Совместимость с MoE-архитектурами
- Учитывает специфические особенности архитектур MoE при квантовании
- Обрабатывает роутинг-механизмы без ущерба для производительности
- Поддерживает разные стратегии квантования для различных экспертов

## Применение

MoE-Quant используется в таких проектах, как:
- IST-DASLAB/MOE-QUANT - код для сжатия моделей DeepSeek
- UNITES-LAB/MOE-QUANTIZATION - официальный код для бенчмарка пост-обучения квантования MoE

## Технические детали

- Предоставляет инструменты для сжатия моделей с сохранением точности
- Поддерживает различные форматы и методы квантования
- Рассчитан на работу с крупными MoE-моделями, такими как Kimi K2

## Связь с MoonShot AI

Согласно информации, MoonShot AI использовала кодовую базу MoE-Quant для квантизации своей модели Kimi-K2-Thinking, что указывает на важность этого фреймворка в экосистеме оптимизации MoE-моделей.

## Связи с другими темами

- [[../architectures/mixture_of_experts.md]] - Подробное объяснение архитектуры MoE
- [[../model_quantization_techniques.md]] - Техники квантования моделей
- [[compressed_tensors.md]] - Формат сжатых тензоров
- [[kimi_k2_quantization.md]] - Квантизация моделей Kimi