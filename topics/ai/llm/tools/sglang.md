# SGLang

## Описание

SGLang - это высокопроизводительный фреймворк для обслуживания больших языковых моделей (LLM) и визуально-языковых моделей. Он разработан для обеспечения низкой задержки и высокой пропускной способности инференса в различных конфигурациях, от одного GPU до крупных распределенных кластеров.

### Ключевые особенности

1. **Быстрый бэкенд-рантайм**:
   - RadixAttention для кэширования префиксов
   - Планировщик с нулевыми накладными расходами на CPU
   - Дезагрегация префил-декодирования
   - Спекулятивное декодирование
   - Непрерывное батчирование
   - Paged attention
   - Параллелизм по тензорам/конвейерам/экспертам/данным
   - Структурированные выводы
   - Chunked prefill
   - Квантизация (FP4/FP8/INT4/AWQ/GPTQ)
   - Мульти-LoRA батчинг
   - Детерминированный инференс (см. ниже)

2. **Обширная поддержка моделей**:
   - Генеративные модели (Llama, Qwen, DeepSeek, Kimi, GLM, GPT, Gemma, Mistral и др.)
   - Модели эмбеддингов (e5-mistral, gte, mcdse)
   - Модели вознаграждения (Skywork)
   - Совместимость с моделями Hugging Face и OpenAI API

3. **Обширная поддержка оборудования**:
   - NVIDIA GPU (GB200/B300/H100/A100/Spark)
   - AMD GPU (MI355/MI300)
   - Intel Xeon CPU
   - Google TPU
   - Ascend NPU

4. **Производительность**:
   - До 5x более быстрый инференс с RadixAttention
   - 3x более быстрое декодирование JSON с сжатым конечным автоматом
   - 7x более быстрое DeepSeek MLA
   - 1.5x более быстрая производительность torch.compile
   - Планировщик батчей с нулевыми накладными расходами

### Программная модель

SGLang имеет **двойную модель программирования**:

1. **Бэкенд-рантайм**: оптимизирован для производительности с продвинутыми техниками, такими как RadixAttention для кэширования префиксов, планировщик с нулевыми накладными расходами на CPU и различные стратегии параллелизма.

2. **Фронтенд-язык**: интуитивный интерфейс для программирования приложений с LLM с возможностями:
   - Последовательные вызовы генерации
   - Продвинутый промпт-инжиниринг
   - Контрольные потоки
   - Мультимодальные входы
   - Параллелизм
   - Внешние взаимодействия

### Использование

1. **Установка**: доступна через PyPI (`pip install sglang`)
2. **Быстрый старт**: документация предоставляет примеры отправки запросов
3. **Поддержка API**: совместимо с OpenAI API
4. **Руководства**: доступны руководства как для бэкенда, так и для фронтенда
5. **Интеграция**: легко интегрируется с моделями Hugging Face

### Сравнение с другими фреймворками

SGLang отличается от других фреймворков несколькими способами:

1. **Производительность**:
   - Быстрее, чем TensorRT-LLM и vLLM для обслуживания Llama3
   - Превосходная производительность декодирования JSON с сжатым конечным автоматом
   - RadixAttention обеспечивает значительное ускорение (до 5x)

2. **Архитектура**:
   - Уникальная дезагрегация префил-декодирования
   - Планировщик с нулевыми накладными расходами на CPU
   - Продвинутые механизмы кэширования
   - Детерминированный инференс с batch-invariant операциями

3. **Поддержка моделей**:
   - Широкая поддержка различных архитектур
   - Поддержка новых моделей "с первого дня" (например, DeepSeek V3/R1, OpenAI gpt-oss)
   - Совместимость с Hugging Face

4. **Поддержка оборудования**:
   - Более широкая совместимость с оборудованием, чем у многих альтернатив (NVIDIA, AMD, Intel, Google, Ascend)

5. **Масштаб**:
   - Используется более чем 300,000 GPU по всему миру
   - Обрабатывает триллионы токенов в производстве ежедневно
   - Поддерживает развертывание как на одном GPU, так и на крупных распределенных кластерах

### Детерминированный инференс

Одной из важных возможностей SGLang является детерминированный инференс, который обеспечивает согласованные выходные данные LLM при разных запусках. Это особенно важно для:

- **Обучения с подкреплением (RL)**: Обеспечивает стабильность logprobs при разных запусках, уменьшает стохастический шум и делает тренировку RL более стабильной, воспроизводимой и поддающейся отладке
- **Тестирования и отладки**: Позволяет воспроизводить результаты
- **Продуктового использования**: Повышает надежность и качество пользовательского опыта

Даже при температуре=0 стандартный инференс LLM может давать разные выходы из-за динамического батчинга и различных порядков редукции в GPU-ядрах.

#### Причина недетерминированности

Основным источником недетерминированности является изменяющийся размер батча. Разные размеры батча заставляют GPU-ядра по-разному разделять операции редукции, что приводит к различным порядкам сложения. Из-за неассоциативности чисел с плавающей запятой ((a + b) + c ≠ a + (b + c)) это дает разные результаты даже для одинаковых входов.

#### Решение в SGLang

На основе batch-invariant операторов Thinking Machines Lab, SGLang достигает полностью детерминированного инференса, поддерживая при этом совместимость с:
- Chunked prefill
- CUDA графами
- Radix cache
- Нежадной выборкой

#### Поддерживаемые бэкенды внимания

Детерминированный инференс поддерживается только с следующими тремя бэкендами внимания:
- **FlashInfer**
- **FlashAttention 3 (FA3)**
- **Triton**

Совместимость функций по разным бэкендам внимания:

| Бэкенд внимания | CUDA Graph | Chunked Prefill | Radix Cache | Нежадная выборка (Temp > 0) |
|------------------|------------|-----------------|-------------|-------------------------------|
| FlashInfer       | ✅ Да     | ✅ Да          | ❌ Нет       | ✅ Да                       |
| FlashAttention 3 (FA3) | ✅ Да | ✅ Да          | ✅ Да      | ✅ Да                       |
| Triton           | ✅ Да     | ✅ Да          | ✅ Да      | ✅ Да                       |

#### Использование

Для включения детерминированного инференса используйте флаг `--enable-deterministic-inference`:

```bash
python3 -m sglang.launch_server \
    --model-path Qwen/Qwen3-8B \
    --attention-backend fa3 \
    --enable-deterministic-inference
```

#### Детерминированный инференс с нежадной выборкой (температура > 0)

SGLang поддерживает детерминированный инференс даже с нежадной выборкой, используя сиды выборки. Это особенно полезно для сценариев обучения с подкреплением, таких как GRPO (Group Relative Policy Optimization), где требуется несколько разнообразных, но воспроизводимых ответов.

По умолчанию SGLang использует сид выборки 42 для воспроизводимой выборки. Для получения различных ответов при сохранении воспроизводимости (например, для тренировки GRPO) можно указывать разные сиды выборки в запросах.

### Связь с другими темами

- [[../inference/vllm_integration.md]] - Подробное описание vLLM, одного из основных конкурентов SGLang в области LLM-сервинга
- [[slime.md]] - SLiME, фреймворк пост-тренировки LLM для масштабирования RL, тесно интегрированный с SGLang
- [[../inference/deterministic_inference.md]] - Общее описание детерминированного инференса в LLM

### Промышленное применение

SGLang имеет значительное корпоративное применение, включая:
- Крупные технологические компании: xAI, AMD, NVIDIA, Intel, LinkedIn, Cursor
- Облачные провайдеры: Oracle Cloud, Google Cloud, Microsoft Azure, AWS
- Академические учреждения: MIT, UCLA, Вашингтонский университет, Стэнфорд, UC Berkeley, Университет Цинхуа
- Используется для официальной демонстрации LLaVA v1.6

### Ресурсы

- Документация: https://docs.sglang.ai/
- GitHub: https://github.com/sgl-project/sglang
- Сообщество: канал Slack и встречи разработчиков каждые две недели
- Проект размещается под некоммерческой организацией LMSYS