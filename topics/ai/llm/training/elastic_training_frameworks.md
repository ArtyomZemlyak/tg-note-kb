# Эластичные тренировочные фреймворки для LLM

## Описание

Эластичные тренировочные фреймворки - это подходы к обучению нейронных сетей, при которых одна "родительская" модель содержит внутри себя веса для нескольких "дочерних" моделей разного размера. Эти фреймворки позволяют эффективно обучать семейства моделей в одном процессе, что значительно сокращает вычислительные затраты по сравнению с независимым обучением каждой модели.

## Принцип работы

### Вложенные архитектуры
- В одной модели содержатся несколько подсетей разного размера
- Подсети представляют собой непрерывные срезы тензоров родительской модели
- Малые подсети наследуют знания из большей родительской модели через общие веса

### Методы оценки важности
- **Ширина**: Используются магнитуды активаций для определения важности нейронов/эмбеддингов
- **Глубина**: Используется нормализованная MSE для оценки важности слоев:
  s_j = sum((M_full - M_-j)^2) / sum(M_full^2)
- **Компоненты SSM**: Учитываются групповые ограничения для Mamba компонентов

### Динамическая маршрутизация
- **Дифференцируемый роутер**: Обучаемый компонент, выбирающий архитектуру на основе целевого бюджета
- **Gumbel-Softmax релаксация**: Позволяет делать дискретный выбор архитектуры дифференцируемым
- **Температурный отжиг**: Постепенное преобразование мягкого распределения в четкий выбор архитектуры

## Архитектурные примеры

### Nemotron Elastic
- **Комбинация**: Гибридная архитектура с Mamba и Attention
- **Размеры**: Создание семейства моделей (6B, 9B, 12B) из одного чекпоинта
- **Обучение**: Использование двухэтапного учебного плана с curriculum learning
- **Преимущества**: Сокращение расхода токенов более чем в 360 раз по сравнению с обучением с нуля

### Matryoshka Representation Learning (MRL)
- **Область применения**: В основном для эмбеддингов и векторных представлений
- **Принцип**: Один энкодер создает векторы разных размеров (например, 512, 256, 128, 64) из одного входа
- **Функция потерь**: InfoNCE Loss рассчитывается для каждого размера вектора независимо

## Методы тренировки

### Curriculum Learning подходы
- **Равномерное сэмплирование бюджета**: На начальных этапах роутер выбирает конфигурации разных размеров с равной вероятностью
- **Неравномерное сэмплирование**: На поздних этапах приоритет отдается большей модели для сохранения способностей к сложным задачам
- **Двухэтапный подход**: Обучение сначала на коротких, затем на длинных контекстах

### Zero-Shot Slicing
- **Принцип**: После обучения одна модель может обслуживать разные конфигурации с помощью масок
- **Эффективность**: Единый слепок памяти может удовлетворять разные требования к латентности
- **Практическое применение**: Снижение требований к памяти и вычислительным ресурсам при деплое

## Преимущества

### Экономическая эффективность
- **Сокращение затрат**: Значительное уменьшение количества токенов, необходимых для обучения семейства моделей
- **Упрощение деплоя**: Одна модель может обслуживать разные сценарии использования
- **Меньше чекпоинтов**: Упрощение управления и обслуживания моделей

### Сохранение качества
- **Передача знаний**: Малые подсети наследуют знания из родительской модели
- **Сохранение способностей**: Даже урезанные версии сохраняют ключевые навыки (например, рассуждения)

### Гибкость
- **Адаптивность**: Возможность динамического выбора архитектуры под текущие требования
- **Масштабируемость**: Возможность масштабирования до разных размеров без повторного обучения

## Ограничения

- **Зависимость от родительской модели**: Качество подсетей зависит от качества родительской модели
- **Сложность реализации**: Требуется сложная архитектура роутера и системы маскирования
- **Требования к настройке**: Необходимость тщательного тюнинга гиперпараметров (например, температуры Gumbel-Softmax)
- **Ограниченная проверка**: Большинство исследований выполнено на моделях до 12B параметров

## Применение

### В LLM архитектурах
- Для создания семейств моделей с разным количеством параметров
- В сценариях, где требуется гибкость в использовании ресурсов
- Для задач, требующих разных уровней сложности рассуждений

### В системах с ограниченными ресурсами
- Для деплоя на устройствах с разными вычислительными возможностями
- В системах, где важна латентность и время отклика
- Для сценариев с динамическими требованиями к ресурсам

## Новые концепции и термины

- **Вложенные подсети**: Подмножества параметров родительской модели, которые функционируют как самостоятельные модели
- **Динамическая маршрутизация**: Процесс выбора архитектуры модели во время инференса на основе требований
- **Zero-Shot Slicing**: Возможность извлекать подсети из родительской модели без дополнительного обучения
- **Гибридные эластичные архитектуры**: Эластичные модели, использующие комбинацию разных типов слоев (например, Mamba + Attention)

## Примеры использования

### Nemotron Elastic
- Создание семейства рассуждающих моделей (6B, 9B, 12B) с высокой эффективностью
- Сохранение способностей к рассуждению на длинных контекстах даже в уменьшенных подсетях

### Другие реализации
- Возможные будущие реализации для Vision Transformers, мультимодальных моделей

## Связи с другими темами

- [[nemotron_elastic_architecture.md]] - Конкретная реализация эластичного фреймворка
- [[matryoshka_representation_learning.md]] - Связанная концепция вложенных представлений
- [[hybrid_architectures.md]] - Гибридные архитектуры, часто используемые в эластичных моделях
- [[dynamic_sparsity_approaches.md]] - Динамические подходы к разреженности, связанные тема
- [[pruning_and_compression.md]] - Альтернативные методы сжатия моделей

## Источники

1. [Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs](https://arxiv.org/abs/2511.16664) - Основная статья, описывающая фреймворк Nemotron Elastic как пример эластичного тренировочного подхода
2. [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147) - Статья о вложенных представлениях, концепция, на которой базируются эластичные модели