# Curriculum Learning для Сверхдлинного Контекста в HSA-UltraLong

## Общее описание

Curriculum Learning для сверхдлинного контекста — это стратегия обучения, при которой модель сначала обучается на задачах с коротким контекстом, постепенно увеличивая длину контекста в процессе обучения. Эта методика критически важна для моделей, таких как HSA-UltraLong, которые должны обобщать знания с коротких контекстов (например, 32K токенов) на экстремально длинные (до 16M токенов).

## Контекст проблемы

Традиционные подходы к обучению моделей на длинном контексте сталкиваются с проблемой "эффекта качелей" (seesaw effect), когда модель чрезмерно полагается на локальный контекст для предсказания следующего токена и фактически "забывает", как пользоваться глобальным поиском. Это убивает способность к обобщению по длине.

## Принципы Curriculum Learning для длинного контекста

### 1. Постепенное увеличение длины контекста

Вместо того чтобы сразу обучать модель на экстремально длинных последовательностях, подход использует постепенное увеличение длины контекста:
- Начальные этапы обучения: короткие контексты (например, 512 токенов)
- Промежуточные этапы: средние контексты (1K-8K токенов)
- Завершающие этапы: длинные контексты (32K-16M токенов)

### 2. Стадия разогрева (Warmup) с контролем баланса

Особенность подхода в HSA-UltraLong — введение стадии "разогрева", где:
- Окно скользящего окна (SWA) искусственно держится маленьким (512 токенов)
- Глобальный top-K в HSA остаётся плотным
- Это заставляет глобальный механизм внимания выучивать паттерны поиска на ранних этапах

### 3. Баланс между локальным и глобальным вниманием

Ключевая задача — поддерживать баланс между локальным скользящим окном и глобальным разреженным поиском. Это достигается через:
- Адаптивное управление размерами окон
- Контроль соотношения задач с разными длинами контекста
- Использование специализированных функций потерь для обучения механизму поиска

## Архитектурные особенности, поддерживающие Curriculum Learning

### 1. Двухуровневая структура внимания

- **Sliding Window Attention (SWA)**: для непосредственного локального контекста
- **Hierarchical Sparse Attention (HSA)**: для глобального поиска

Эта структура позволяет модели сначала освоить локальные зависимости, а затем перейти к более сложным глобальным зависимостям.

### 2. Стратегия позиционного кодирования

Использование "RoPE для короткого, NoPE для длинного":
- Локальные модули SWA используют Rotary Positional Embeddings (RoPE)
- Глобальный механизм HSA использует No Positional Encoding (NoPE)
- Полагается исключительно на адресацию по контенту (content-based addressing)

## Техники оптимизации Curriculum Learning

### 1. Контроль эффекта качелей

- Мониторинг соотношения локального и глобального внимания
- Использование специальных стратегий отжига (annealing) при переходе от коротких к длинным контекстам
- Регулярная проверка способности модели к глобальному поиску в процессе обучения

### 2. Смешивание задач разной длины

- Использование динамического распределения задач разной длины
- Адаптивное изменение пропорции задач с разными длинами в зависимости от прогресса модели
- Поддержание баланса между задачами на поиск и задачами на рассуждение

## Практические результаты

### 1. Обобщающая способность

- Модели, обученные с использованием Curriculum Learning, показывают способность обобщаться с 32K обучающих окон на 16M контекста при инференсе без переобучения
- Достижение почти идеальной точности поиска даже на экстремальных масштабах (16 миллионов токенов)

### 2. Стабильность обучения

- Более стабильный процесс обучения по сравнению с подходами, использующими сразу длинные контексты
- Лучшее сохранение способности к глобальному поиску при переходе к длинным последовательностям

## Ограничения и вызовы

### 1. Комплексность настройки гиперпараметров

- Необходимость тонкой настройки расписания изменения длины контекста
- Определение оптимальных пропорций задач разной длины
- Калибровка стратегии отжига для перехода между режимами

### 2. Потенциальные проблемы с переобучением

- Риск переобучения на коротких последовательностях в начале обучения
- Потеря способности к глобальному поиску при неправильной схеме обучения
- Файнтюнинг на данных с коротким контекстом может вредить способностям модели работать с длинным контекстом

## Сравнение с альтернативными подходами

| Метод | Обобщающая способность | Сложность настройки | Стабильность обучения |
|-------|----------------------|-------------------|---------------------|
| Curriculum Learning | Высокая | Высокая | Высокая |
| Direct Long-Context Training | Низкая | Низкая | Низкая |
| Random Length Sampling | Средняя | Средняя | Средняя |

## Применение в HSA-UltraLong

В HSA-UltraLong Curriculum Learning особенно критично, так как:
- Модель должна обучаться дифференцируемой маршрутизации выбора чанков
- Необходимо обучать модель локальной и глобальной навигации одновременно
- Архитектура требует баланса между локальным вниманием и глобальным поиском

## Будущие направления

### 1. Автоматизированное проектирование учебного плана

- Использование методов метаобучения для автоматического определения оптимального расписания длины контекста
- Адаптивные стратегии, изменяющиеся в зависимости от прогресса модели

### 2. Интерпретируемость процесса обучения

- Понимание, как модель переходит от локального к глобальному вниманию
- Мониторинг эффективности механизма маршрутизации во время обучения

## Источники

- Основная статья: "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models" (Xiang Hu, Zhanchao Zhou, Ruiqi Liang, Zehuan Li, Wei Wu, Jianguo Li)
- URL статьи: https://arxiv.org/abs/2511.23319
- Код: https://github.com/ant-research/long-context-modeling

## Дополнительные материалы

- "Understanding Curriculum Learning for Long-Context Transformers" - более глубокое исследование принципов
- "Dynamic Context Length Scheduling in LLM Training" - альтернативные подходы к управлению длиной контекста

## Связи с другими темами

- [[../models/hsa_ultralong.md]] - Основная модель, использующая Curriculum Learning
- [[../mixture_of_experts_architecture.md]] - Архитектура, для которой важно правильное обучение
- [[pretraining_techniques.md]] - Общие техники предобучения, включая Curriculum Learning
- [[../memory/llm_memory_overview.md]] - Связь с архитектурами памяти в LLM
- [[fine_tuning_preserving_skills_best_practices.md]] - Методы тонкой настройки, сохраняющие способности к длинному контексту
- [[specialized_attention_mechanisms_comparison.md]] - Сравнение различных механизмов разреженного внимания с различными подходами к обучению