# Вариационные автоэнкодеры (Variational Autoencoders, VAE)

## Общее описание

Вариационные автоэнкодеры (VAE) - это генеративные модели, которые обучают энкодер для преобразования данных в скрытое (латентное) пространство и декодер для восстановления данных из латентного представления. В отличие от обычных автоэнкодеров, VAE регуляризуют латентное пространство, обучая его следовать заданному априорному распределению (обычно нормальному).

## Математическая основа

VAE максимизируют нижнюю оценку логарифмического правдоподобия (Evidence Lower BOund - ELBO):
log P(x) ≥ E[log P(x|z)] - KL(q(z|x)||P(z))

где:
- E[log P(x|z)] - точность восстановления
- KL(q(z|x)||P(z)) - дивергенция Кульбака-Лейблера между апостериорным и априорным распределением

## Архитектура

### Энкодер
- Извлекает признаки из входных данных
- Выход - параметры распределения в латентном пространстве (обычно среднее и дисперсия нормального распределения)

### Латентное пространство
- Обычно фиксированного размера с распределением, близким к стандартному нормальному
- Позволяет генерировать новые данные через сэмплирование

### Декодер
- Восстанавливает данные из латентного представления
- Пытается воссоздать входные данные как можно точнее

## Вариации VAE

### β-VAE
Добавляет вес β к KL-дивергенции для лучшей дисентангуляции (разделения) признаков в латентном пространстве.

### VAE с нормализующими потоками
Использует нормализующие потоки для более гибкого моделирования апостериорного распределения.

### Conditional VAE
Условные VAE, которые генерируют данные на основе дополнительной информации/условий.

## Обучение

VAE обучается путем оптимизации ELBO, что эквивалентно минимизации расстояния между истинным и приближенным апостериорными распределениями, с добавлением регуляризации, заставляющей латентное пространство следовать априорному распределению.

## Применение

- Генерация изображений, текста и аудио
- Изучение латентных представлений
- Денойзинг и заполнение пропущенных данных
- Оценка плотности
- Уменьшение размерности

## Преимущества

- Устойчивость к переобучению
- Гладкое и структурированное латентное пространство
- Возможность генерации новых данных
- Теоретически обоснованная архитектура

## Ограничения

- Коллапс апостериорного распределения (posterior collapse)
- Иногда более размытые результаты по сравнению с GAN
- Сложность с обучением при высокоразмерных данных

## Варианты регуляризации

### KL clipping
Техника, используемая для предотвращения коллапса апостериорного распределения, ограничивающая значение KL-дивергенции.

### Двойной Dropout
Применение Dropout как к входным данным, так и к латентному вектору для создания более устойчивых представлений.

## Связи с другими темами

- [[energy_based_models.md]] - Альтернативные подходы к генеративному моделированию
- [[calm_continuous_autoregressive_language_models.md]] - Использование VAE для создания надежного латентного пространства в CALM
- [[generative_models.md]] - Общие принципы генеративных моделей

## Источники

1. [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) - Оригинальная статья, в которой представлены VAE, Kingma & Welling, 2013
2. [Understanding the Variational Autoencoder](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f705107dac1e) - Объяснение VAE с интуицией и примерами
3. [β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework](https://openreview.net/pdf?id=Sy2fzU9gl) - Вариация VAE с лучшей дисентангуляцией