# Потеря пластичности в глубоком continual learning

## Определение

Потеря пластичности (loss of plasticity) - это явление, при котором нейронные сети теряют способность обучаться новой информации со временем, даже если они сталкиваются с той же информацией, что и модели, обученные с нуля. Это отличается от катастрофического забывания, поскольку проблема заключается не в потере предыдущей информации, а в неспособности усваивать новую информацию.

## Проблема

При continual learning (непрерывном обучении), где модель обучается задача за задачей, нейросеть начинает работать все хуже и хуже по сравнению с базовой моделью, которая сразу обучается на последней задаче, хотя у них перед глазами одни и те же данные. В некоторых случаях случайная инициализация работает лучше предварительного обучения, если данных достаточно.

## Причины проблемы

Основная причина потери пластичности заключается в появлении так называемых "спящих юнитов" (dormant units) - нейронов, которые скатываются к постоянному предсказанию (например, к 0 при использовании ReLU). Градиенты перестают течь через эти нейроны, и они становятся бесполезными для обучения.

## Экспериментальный сценарий

В статье рассматривается задача class-incremental CIFAR-100 - облегченный вариант, где модель сначала обучается на 5 классах, затем на 10, затем на 15 и так далее. В отличие от стандартных сценариев приращения класса, здесь специально избегают проблемы забывания, фокусируясь только на пластичности.

## Решения проблемы

### Shrink and Perturb
Метод, который добавляет L2-регуляризацию и зашумление весов, позволяя весам вернуться к точке инициализации. С его помощью потеря пластичности замедляется.

### Continual Backpropagation
Новый подход, предложенный в статье, который предполагает более точечную работу с плохими юнитами:

1. Для каждого нейрона поддерживается функция полезности (utility function), которая измеряет его "вклад" в следующие слои - это сумма выходных весов, умноженная на выход нейрона
2. Используется скользящее среднее для усреднения по времени
3. На каждом шаге доля весов с наименьшей полезностью переинициализируется - входные веса из шума, выходные в нули
4. Переинициализированным юнитам выдается иммунитет на 100 шагов
5. Доля пересоздаваемых юнитов мала - всего 10^-5 от всех юнитов за шаг

Этого достаточно, чтобы полностью решить проблему потери пластичности на задаче class-incremental CIFAR-100 и удерживать долю спящих юнитов достаточно близкой к нулю.

## Ограничения

Метод Continual Backpropagation может казаться "бесплатным обедом" - дополнительным улучшением пластичности без потерь. Однако на самом деле он потребляет часть ресурсов, которые раньше простаивали, выделив определенную память, которая затем начинает неэффективно использоваться. Чем сложнее задача, тем меньший "кусок обеда" удается съесть, и тем больше ресурсов выбрасывается в мусор.

## Связи с другими темами

- [[../catastrophic_forgetting/catastrophic_forgetting.md]] - Катастрофическое забывание: связанная, но отличная проблема в continual learning
- [[../class_incremental_learning/class_incremental_learning.md]] - Приращение класса: сценарий, в котором проявляется потеря пластичности
- [[dormant_units.md]] - Спящие юниты: механизм, вызывающий потерю пластичности
- [[continual_backpropagation.md]] - Continual Backpropagation: метод решения проблемы