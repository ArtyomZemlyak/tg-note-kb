# Attention Head архитектура

## Описание

Attention Head архитектура - это компонент нейронной сети, использующий механизмы самовнимания (self-attention) для обработки и агрегации информации из разных частей входных данных. В контексте систем визуального поиска, Attention Head используется для улучшения качества эмбеддингов изображений и текстов.

## Архитектура

### Компоненты:
1. **Self-Attention Token Reducer** - уменьшает количество токенов, применяя механизм самовнимания
2. **Attention Pooling** - объединяет информацию из токенов с использованием внимания
3. **Снижение размерности** - преобразует многомерные признаки в меньшее пространство
4. **Нормализация с multi-head attention** - нормализует признаки, используя несколько голов внимания
5. **UnLearnable Token** - недоступный для обучения параметр, действующий как семантический якорь

### Структура:
- Вход: последовательность токенов/признаков
- Процесс: применение self-attention и pooling
- Выход: фиксированный вектор заданной размерности (например, 512)

## Применение в системах визуального поиска

### В системе поиска по фото Wildberries:
- Используется как дополнительная архитектура поверх SigLIP 2
- Улучшает качество векторных представлений изображений
- Обеспечивает стабильность против коллапса внимания
- Повышает качество финальных эмбеддингов

### Преимущества:
1. **Стабильность** - UnLearnable Token предотвращает коллапс внимания
2. **Качество** - семантический якорь улучшает качество представлений
3. **Гибкость** - поддерживает разные размеры векторов на выходе

## Технические детали

### UnLearnable Token:
- Недоступный для обучения параметр
- Инициализируется как средняя матрица параметров датасета Query-параметров
- Используется с Multi-Head Attention
- Действует как семантический якорь, улучшая финальное качество

### Multi-Head Attention:
- Позволяет модели одновременно обрабатывать информацию из разных подпространств
- Улучшает способность модели захватывать различные аспекты входных данных
- Повышает общее качество эмбеддингов

## Обучение

### Подход к обучению:
- Attention Head обучается совместно с основной моделью (например, SigLIP 2)
- Использует подходы, такие как MRL (Matryoshka Representation Learning)
- Применяется InfoNCE Loss для обучения контрастивных представлений

### Техники улучшения:
- Ортогональная инициализация для ускорения сходимости
- Использование стабильных численных методов

## Примеры использования

### Визуальный поиск:
- Улучшение качества поиска по схожести изображений
- Повышение точности выдачи релевантных товаров

### Мультимодальные задачи:
- Объединение визуальной и текстовой информации
- Улучшение сопоставления изображений и описаний

## Сравнение с другими подходами

### Отличия от простого глобального пулинга:
- Внимание позволяет модели фокусироваться на наиболее важных частях изображения
- Повышает интерпретируемость модели
- Позволяет лучше обрабатывать сложные сцены с несколькими объектами

## Новые концепции и термины

- **Self-Attention**: механизм, позволяющий модели учитывать зависимости между всеми частями входных данных
- **Multi-Head Attention**: использование нескольких независимых механизмов внимания для захвата разных аспектов информации
- **UnLearnable Token**: фиксированный параметр, действующий как семантический якорь для стабилизации обучения
- **Attention Pooling**: способ агрегации информации с использованием весов внимания

## Преимущества и недостатки

### Преимущества:
- Повышает качество представлений
- Стабилизирует обучение
- Повышает интерпретируемость

### Потенциальные недостатки:
- Увеличивает вычислительную сложность
- Требует больше ресурсов для обучения

## Связи с другими темами

- [[ai/machine_learning/reasoning_models/matryoshka_representation_learning.md]] - Подход MRL, часто используемый вместе с Attention Head
- [[ai/computer_vision/visual_search/wildberries_photo_search.md]] - Применение в системе поиска по фото от Wildberries
- [[ai/computer_vision/multimodal_models.md]] - Мультимодальные модели, использующие механизмы внимания
- [[ai/llm/specialized_attention_mechanisms.md]] - Специализированные механизмы внимания в LLM