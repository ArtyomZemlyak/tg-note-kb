# Архитектура TRM (Tiny Recursive Model)

## Общая структура

TRM использует очень простую архитектуру - одну маленькую сеть, являющуюся стандартным блоком трансформера: [self-attention, norm, MLP, norm]. В оригинальной идее было 4 таких блока, но после экспериментов пришли к 2 блокам как к оптимальным.

## Рекурсивный процесс

Входной сигнал TRM состоит из трех элементов:
- **input (x)**: входные данные задачи
- **latent (z)**: скрытое состояние рассуждений
- **prediction (y)**: предсказание/ответ

На начальном этапе приходит только x, z и y инициализируются нулями.

### Три уровня рекурсии

TRM реализует три уровня рекурсивного процесса:

#### 1. Latent Recursion (скрытая рекурсия)
- На глубоком уровне модель обновляет скрытый признак z (z_L) в течение n шагов: z ← net(x, y, z)
- Формула рекурсии: f_L(x + y + z)
- Вход x остается постоянным, а z и y обновляются рекурсивно

#### 2. Deep Recursion (глубокая рекурсия)
- Уровнем выше происходит T итераций процесса f_H, последовательно улучшая оба значения z и y
- Формула рекурсии: f_H(y + z)
- Первые T-1 итераций проходят без отслеживания градиентов для эффективного приближения к решению
- Только последняя итерация позволяет градиентам пройти через все вызовы

#### 3. Deep Supervision (глубокий контроль)
- На внешнем уровне цикл обучения включает до N_sup=16 шагов супервизии
- На каждом шаге модель выполняет процесс deep recursion
- Градиенты распространяются только через последнюю итерацию внешнего цикла

## Параметры рекурсии

- **T**: Количество итераций внешнего цикла (deep recursion), обычно T=3
- **n**: Количество шагов внутреннего цикла (latent recursion), обычно n=6
- **N_sup**: Количество шагов глубокой супервизии, до 16

Общая эффективная глубина: (n+1) * T (например, (6+1) * 3 = 21 вызовов сети за шаг супервизии, и 21 * 16 = 336 за весь процесс).

## Архитектурные особенности

### Упрощение ACT
TRM упрощает механизм адаптивного времени вычислений:
- Отказ от отдельного вычисления для continue loss
- Использование только вероятности остановки (halting probability)
- Это уменьшает количество проходов модели и ускоряет процесс

### Использование EMA
- Для стабильности при обучении на малом количестве данных используется экспоненциальное скользящее среднее (EMA) с коэффициентом 0.999
- Это помогает избежать переобучения и расходимости

### Оптимальность 2 слоев
- Эксперименты показали, что 2 слоя - это оптимум для TRM
- Добавление большего количества слоев приводит к переобучению
- 2-слойная модель дает лучший результат (87.4% вместо 79.5% на Sudoku-Extreme) и меньше параметров (5M вместо 10M)

### Различные версии архитектуры
- **Версия с attention**: 7M параметров, лучше работает для задач, требующих контекста (Maze, ARC-AGI)
- **Версия с MLP**: 5M параметров для Sudoku, 19M для других задач, лучше работает для задач, где важнее локальные зависимости (Sudoku)

## Переинтерпретация HRM
TRM предлагает более интуитивную интерпретацию:
- **y (ранее z_H)**: Текущий (в виде эмбеддинга) выходной ответ
- **z (ранее z_L)**: Скрытый признак, представляющий след рассуждений или "цепочку мыслей"

## Сравнение с Coconut
Интересно, что подход отличается от латентного рассуждения в стиле Coconut, где оно было на уровне токенов при авторегрессивной генерации. В TRM же рассуждение разворачивается в другом измерении - в глубине вызовов модели.

## Связанные темы
- [[tiny_recursive_model_trm.md]] - Общая информация о TRM
- [[trm_vs_hrm_comparison.md]] - Сравнение с HRM
- [[trm_experiments_results.md]] - Эксперименты и результаты
- [[less_is_more_philosophy.md]] - Философия "меньше значит больше" в архитектурах ИИ