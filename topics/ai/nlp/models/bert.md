# BERT (Bidirectional Encoder Representations from Transformers)

## Описание

BERT (Bidirectional Encoder Representations from Transformers) - это революционная модель в области обработки естественного языка, представленная в 2018 году командой Google. Она использует двунаправленную архитектуру трансформера для предварительного обучения на неаннотированных текстах.

## Архитектура

BERT использует encoder-only архитектуру трансформера и обучается с использованием двух методов:
- Маскированное языковое моделирование (MLM) - случайно маскирует часть входных токенов и учит модель предсказывать их
- Предсказание следующего предложения (NSP) - обучает модель предсказывать, являются ли два текста последовательными

## Применение

BERT показал выдающиеся результаты в различных задачах NLP, включая:
- Понимание естественного языка (GLUE)
- Вопрос-ответ (SQuAD)
- Классификацию текста
- Извлечение информации

## Варианты

- RoBERTa (Robustly Optimized BERT Pretraining Approach) - улучшенная версия BERT без NSP и с другими оптимизациями
- DistilBERT - уменьшенная и более быстрая версия BERT
- ALBERT - архитектура с меньшим количеством параметров
- DeBERTa - улучшенная версия с декомпозицией внимания

## Связь с диффузионными моделями

Интересно, что BERT можно рассматривать как одностадийную версию текстовой диффузии. Маскированное языковое моделирование, которое использует BERT, по сути является частным случаем диффузионного моделирования текста, где модель обучается восстанавливать замаскированные токены из частично зашумленного входа. Для более подробной информации см. [[../llm/diffusion_models.md]] - общее описание диффузионных моделей, [[../llm/architectures/diffusion/text_diffusion_models.md]] и [[../llm/architectures/diffusion/bert_diffusion_connection.md]].

## Ссылки на источники

- Оригинальная статья: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
- [[ai/nlp/transformers/transformer_architecture.md]] - Архитектура трансформеров
- [[ai/nlp/transformers/transformers_and_llms.md]] - Связь с другими моделями