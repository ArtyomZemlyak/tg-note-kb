# Текстовые диффузионные модели (Text Diffusion Models)

## Описание

Текстовые диффузионные модели - это класс генеративных моделей, которые применяют принципы диффузионного моделирования к задачам генерации текста. В отличие от традиционных автогрессивных моделей (например, GPT), диффузионные модели генерируют текст постепенно, итеративно уточняя результат через серию шагов восстановления.

## Принцип работы

Диффузионные модели работают на основе двух ключевых процессов:

1. **Прямой процесс (диффузия)**: постепенное добавление "шума" к чистым данным (в случае текста - замена токенов специальным маскирующим токеном или случайными токенами)
2. **Обратный процесс (дедиффузия)**: модель обучается восстанавливать чистые данные из зашумленных, постепенно уменьшая уровень шума

## Связь с BERT

Интересно, что маскированное языковое моделирование (MLM), используемое в BERT, на самом деле является частным случаем диффузионного моделирования текста:
- BERT выполняет один шаг "восстановления" из частично замаскированного текста
- При добавлении нескольких шагов восстановления BERT превращается в полноценную диффузионную генеративную модель
- Это открывает возможность для создания моделей, которые совмещают понимание и генерацию текста в одном процессе

## Методы и реализации

### Маскировочная диффузия
- Замена токенов маскирующим токеном <MASK> с переменной вероятностью
- Обучение модели предсказывать оригинальные токены на основе частично замаскированного текста
- Использование расписания уровней маскировки от высоких к низким

### RoBERTa Diffusion
- Экспериментальная реализация, показывающая, что RoBERTa может быть дообучена для генерации текста
- Модель использует несколько шагов диффузии вместо одного
- Результаты показали, что модель способна генерировать связный текст без автогрессивного декодирования

## Преимущества

- Возможность генерации целых блоков текста за один проход (в отличие от посимвольной/последовательной генерации)
- Потенциал для лучшего глобального понимания текста из-за неавтогрессивной природы
- Архитектура encoder-only, которая может быть более эффективной по сравнению с decoder-only моделями

## Недостатки

- Замедленный процесс генерации по сравнению с автогрессивными моделями из-за необходимости нескольких итераций
- Сложность в реализации эффективных схем денойзинга
- Потенциально более низкое качество генерации по сравнению с хорошо настроенными автогрессивными моделями

## Применение

- Генерация связного текста
- Текстовая инфилляция (заполнение пропусков)
- Текстовая интерполяция
- Улучшение качества генерации

## Связи с другими темами

- [[bert.md]] - BERT и его связь с диффузионными моделями
- [[ai/nlp/models/bert_diffusion_connection.md]] - Подробное рассмотрение связи BERT и диффузии
- [[ai/llm/models/generative_models.md]] - Общие генеративные модели
- [[../../machine_learning/reasoning_models/few_shot_learning.md]] - Модели с способностью к рассуждению