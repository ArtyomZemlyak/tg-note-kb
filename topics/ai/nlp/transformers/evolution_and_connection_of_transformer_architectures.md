# Эволюция и взаимосвязи архитектур трансформеров

## Общее описание

Архитектура трансформеров, представленная в статье "Attention is All You Need" в 2017 году, стала основой для современных больших языковых моделей (LLM). С тех пор произошло множество улучшений и вариаций, каждая из которых направлена на решение специфических проблем или улучшение производительности в определенных задачах.

## Основные этапы эволюции

### 1. Оригинальная архитектура трансформеров

Оригинальный трансформер состоял из энкодера и декодера, каждый из которых содержал:
- Механизмы самовнимания (self-attention)
- Позиционные кодировки
- Полносвязные сети
- Остаточные соединения и нормализацию слоев

**Особенности**:
- Encoder-decoder архитектура для задач преобразования текста в текст
- Всестороннее внимание в энкодере
- Каузальное внимание в декодере
- Cross-attention между энкодером и декодером

### 2. Encoder-only архитектуры (BERT-подобные)

**Разработаны для**: задач понимания языка, классификации, эмбеддингов

**Основные характеристики**:
- Используют только энкодерную часть архитектуры
- Всестороннее внимание (все токены могут обращаться ко всем другим токенам)
- Маскированное языковое моделирование как задача обучения
- Хорошо подходят для задач, где нужен полный контекст

**Примеры**: BERT, RoBERTa, DistilBERT, Sentence-BERT

**Связи**: [[transformer_architecture.md]] - подробное описание архитектуры трансформеров, [[../../llm/models/bert.md]] - детали BERT модели

### 3. Decoder-only архитектуры (GPT-подобные)

**Разработаны для**: генерации текста, автoregressive задач

**Основные характеристики**:
- Используют только декодерную часть архитектуры (без cross-attention с энкодером)
- Каузальное внимание (токен может обращаться только к предыдущим токенам)
- Автoregressive языковое моделирование как задача обучения
- Идеально подходят для генерации текста

**Примеры**: GPT, GPT-2, GPT-3, GPT-3.5, GPT-4, LLaMA, Mistral

**Связи**: [[../../llm/llm_architectures_comparison.md]] - сравнение различных архитектур LLM

### 4. Encoder-decoder архитектуры (T5-подобные)

**Разработаны для**: задач преобразования текста в текст

**Основные характеристики**:
- Сохраняют полную архитектуру оригинального трансформера
- Энкодер для понимания входа
- Декодер для генерации выхода
- Может быть адаптирован для различных задач с помощью префиксов

**Примеры**: T5, BART, ProphetNet

## Современные улучшения архитектур

### 1. Mixture of Experts (MoE)

**Цель**: Повышение эффективности параметров и масштабируемости

**Принцип работы**:
- Заменяет плотные слои полносвязной сети (FFN) разреженными слоями MoE
- Содержит несколько "экспертов" и сеть-шлюз, определяющую, какие токены идут к каким экспертам
- Только часть параметров активируется для каждого входа

**Преимущества**:
- Более быстрое предварительное обучение
- Более быстрый инференс
- Эффективное использование параметров

**Примеры**: Switch Transformers, GLaM, Mixtral 8x7B

**Связи**: [[../../llm/mixture_of_experts_architecture.md]] - подробное описание MoE архитектур

### 2. Специализированные механизмы внимания

#### Multi-Query и Grouped-Query Attention (MQA/GQA)
- Уменьшают требования к KV кэшу
- Улучшают скорость инференса
- Снижают потребление памяти

#### Multihead Latent Attention (MLA)
- Использует низкоранговое приближение стандартного Multihead Attention
- Минимизирует KV кэш
- Повышает эффективность при сохранении качества

#### DeepSeek Sparse Attention (DSA)
- Новая технология разреженного внимания
- Улучшает эффективность обучения и вывода в задачах с длинными контекстами
- Сохраняет качество модели

#### Log-Linear Attention
- Сложность O(n log n), промежуточный вариант между стандартным вниманием и линейными альтернативами
- Улучшает вычислительную эффективность при работе с длинными последовательностями

#### Mixture of Sparse Attention (MoSA)
- Использует обучаемую, основанную на содержании маршрутизацию
- Динамически выбирает разреженное подмножество токенов
- Превосходит традиционные модели с полным вниманием

#### Star Attention
- Новая архитектура разреженного блочного внимания
- Обеспечивает значительное ускорение при сохранении точности
- Совместима с существующими моделями без переобучения

**Связи**: [[../../llm/specialized_attention_mechanisms.md]] - подробное описание специализированных механизмов внимания

### 3. Диффузионные трансформеры

**Цель**: Альтернативный подход к генерации с использованием принципов диффузии

**Принцип работы**:
- Многократное маскирование и восстановление для генерации
- Encoder-only архитектура для генерации текста
- Основан на диффузионных моделях, адаптированных для NLP

**Связи**: [[../../llm/diffusion_models.md]] - общее описание диффузионных моделей, [[../../llm/architectures/diffusion/text_diffusion_models.md]] - архитектура диффузионных моделей для текста

### 4. Условные VAE-трансформеры

**Цель**: Интеграция вероятностного вывода в архитектуру трансформеров

**Принцип работы**:
- Использование латентных переменных для улучшения рассуждений
- Комбинация архитектур VAE и трансформеров
- Улучшение способности к логическим рассуждениям

**Связи**: [[free_transformer.md]] - интеграция VAE в трансформеры для улучшения рассуждений

### 5. Дифференциальные трансформеры

**Цель**: Улучшение понимания относительных позиций и отношений

**Принцип работы**:
- Механизм внимания, вычисляющий внимание на основе разницы между представлениями токенов
- Улучшенное моделирование относительных отношений между токенами
- Более точная обработка относительных позиций

**Связи**: [[differential_transformer.md]] - архитектура дифференциальных трансформеров

## Сравнение архитектур

| Архитектура | Основные задачи | Преимущества | Недостатки | Примеры моделей |
|-------------|-----------------|--------------|------------|-----------------|
| Encoder-only | Понимание текста, классификация | Хорошее понимание контекста | Не подходит для генерации | BERT, RoBERTa |
| Decoder-only | Генерация текста, автoregressive задачи | Хорошая генерация | Однонаправленная обработка | GPT-3, LLaMA |
| Encoder-decoder | Текст-в-текст задачи | Гибкость, понимание и генерация | Более сложная архитектура | T5, BART |
| MoE | Масштабируемые задачи | Эффективность параметров | Сложность обучения | Switch Transformers, Mixtral |
| Sparse Attention | Длинные последовательности | Повышенная эффективность | Потенциальная потеря информации | Mixture of Sparse Attention, Star Attention |

## Современные тренды и разработки

### 1. Слияние архитектур
Современные архитектуры часто комбинируют различные подходы:
- MoE + Sparse Attention
- Encoder-decoder + Diffusion
- Conditional VAE + Transformers

### 2. Улучшение эффективности
- Использование разреженных вычислений
- Улучшение KV-кэша
- Оптимизация для длинных контекстов

### 3. Унификация архитектур
- Модели, способные к нескольким задачам
- Общие архитектуры для разных модальностей
- Повышение универсальности и адаптивности

## Связи с другими темами

- [[transformer_architecture.md]] - подробное описание базовой архитектуры трансформеров
- [[evolution_of_nlp_methods.md]] - эволюция NLP методов, ведущих к трансформерам
- [[transformers_and_llms.md]] - связь трансформеров с большими языковыми моделями
- [[../../llm/llm_architectures_comparison.md]] - сравнение различных архитектур LLM
- [[../../llm/autoregressive_models.md]] - авторегрессивные модели на основе трансформеров
- [[../../llm/models/qwen/qwen3-vl.md]] - пример применения Vision Transformer
- [[../../continual_learning/plasticity/continual_backpropagation.md]] - методы решения проблем с обучением в трансформерных архитектурах
- [[../../llm/reasoning/circuit_based_reasoning_verification.md]] - использование трансформерных архитектур для логических рассуждений
- [[next_gen_transformer_architectures.md]] - перспективные архитектуры трансформеров (2024-2025)

## Источники

1. [Attention is All You Need](https://arxiv.org/abs/1706.03762) - оригинальная статья о трансформерах от Google Brain
2. [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - статья о BERT от Google
3. [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - статья о GPT-3 от OpenAI
4. [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) - статья о MoE архитектурах от Google
5. [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) - статья о T5 архитектуре