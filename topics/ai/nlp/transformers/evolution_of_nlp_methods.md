# Эволюция методов NLP

## Общее описание

Обработка естественного языка (NLP) прошла долгий путь от простых статистических методов до современных трансформеров. Эволюция методов NLP отражает развитие понимания языка в компьютерных системах и стремление к более глубокому и контекстуальному пониманию.

## Исторические этапы развития NLP

### 1. Ранние статистические методы (1950-1980-е годы)

- **Правила и словари**: ранние системы NLP основывались на жестких лингвистических правилах и предопределенных словарях.
- **Машинный перевод**: первые попытки машинного перевода основывались на правилах и словарях, что ограничивало гибкость и точность.
- **Проблемы**: трудно масштабировать, трудоемко поддерживать, плохая обработка языковых исключений.

### 2. Статистические методы (1980-2000-е годы)

- **Скрытые марковские модели (HMM)**: использовались для задач разметки частей речи и распознавания речи.
- **N-граммные модели**: простые статистические модели для оценки вероятности последовательностей слов.
- **Метод максимальной энтропии**: подход к задачам классификации, позволяющий использовать различные признаки.
- **Преимущества**: более гибкие, чем правила, могут работать с большими объемами данных.
- **Ограничения**: ограниченное понимание контекста, трудности с редкими словами.

### 3. Методы глубокого обучения (2000-2017 годы)

- **Векторные представления слов (word embeddings)**: Word2Vec, GloVe, FastText позволяли захватывать семантические отношения между словами.
- **Рекуррентные нейронные сети (RNN)**: позволяли обрабатывать последовательности переменной длины, захватывая контекст.
- **Долгая краткосрочная память (LSTM)** и **Gated Recurrent Units (GRU)**: улучшенные версии RNN, способные захватывать долгосрочные зависимости.
- **Сверточные нейронные сети (CNN)**: использовались для задач классификации текста и выделения признаков.
- **Преимущества**: автоматическое извлечение признаков, лучшее понимание семантики.
- **Проблемы**: проблемы с долгосрочными зависимостями в RNN, трудности с параллелизацией.

### 4. Архитектура трансформеров (с 2017 года)

- **Статья "Attention is All You Need"**: представила архитектуру трансформеров, полностью основанную на механизмах внимания.
- **Self-attention**: позволяет модели учитывать всю последовательность при обработке каждого элемента.
- **Encoder и Decoder**: гибкая архитектура, подходящая для различных задач NLP.
- **Преимущества**: лучшее понимание контекста, эффективная параллелизация, способность захватывать долгосрочные зависимости.
- **Примеры моделей**: BERT (encoder-only), GPT (decoder-only), T5 (encoder-decoder).
- **Диффузионные модели**: Современное направление, показывающее, что BERT по сути является одностадийной диффузионной моделью (см. [[../../nlp/models/bert_diffusion_connection.md]] - новое понимание связи между классическим маскированным языковым моделированием и диффузионными процессами).

### 5. Современные большие языковые модели (LLM)

- **Масштабирование**: модели с миллиардами параметров, обученные на огромных корпусах текста.
- **Предобучение и тонкая настройка**: подход, при котором модели сначала предобучаются на общем языковом моделировании, а затем тонко настраиваются под конкретные задачи.
- **Few-shot и zero-shot обучение**: способность решать задачи с минимальным количеством примеров.
- **Примеры**: GPT-3, GPT-4, LLaMA, PaLM, Chinchilla.

## Ключевые инновации на каждом этапе

1. **От правил к статистике**: переход от фиксированных лингвистических правил к обучению на данных.
2. **От статистики к нейронным сетям**: использование нейронных сетей для автоматического извлечения признаков.
3. **От RNN к трансформерам**: использование механизмов внимания вместо рекуррентных соединений.
4. **От трансформеров к LLM**: масштабирование моделей и данных для достижения общего понимания языка.

## Влияние на современное состояние NLP

- **Качество**: значительное улучшение результатов по большинству метрик NLP.
- **Многофункциональность**: одна модель может справляться с множеством различных задач NLP.
- **Доступность**: развитие open-source моделей и инструментов для работы с NLP.
- **Новые задачи**: появление новых возможностей, которые были невозможны с предыдущими подходами.

## Связи с другими темами

- [[transformer_architecture.md]] - подробное описание архитектуры трансформеров, вершины эволюции NLP
- [[course_stanford_cme295.md]] - курс, изучающий эволюцию методов NLP до современных трансформеров
- [[transformers_and_llms.md]] - связь трансформеров, как вершины эволюции NLP, с большими языковыми моделями
- [[../../llm/models/laser_reinforcement_learning.md]] - применение современных методов NLP в сочетании с обучением с подкреплением