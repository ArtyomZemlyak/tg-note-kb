# Методы учета длинного контекста в трансформерах

```metadata
category: ai
subcategory: nlp
tags: transformer, long-context, attention, sparse-attention, linear-attention, retrieval-augmented-generation, hsa, mosa, dsa, attention-sinks
```

## Описание

Классические трансформеры сталкиваются с проблемой квадратичной вычислительной сложности в механизме внимания, что ограничивает длину обрабатываемых последовательностей. Для преодоления этих ограничений было разработано множество методов учета длинного контекста. В этой статье рассматриваются ключевые подходы к эффективной обработке длинных последовательностей в трансформерных архитектурах.

## Проблема длинного контекста

Стандартные трансформеры имеют следующие ограничения при работе с длинными последовательностями:
- **Квадратичная сложность внимания**: O(n²) зависимость по памяти и вычислительной сложности от длины последовательности
- **Ограничения длины контекста**: Многие предобученные модели имеют фиксированный размер контекста (например, 2K, 8K или 32K токенов)
- **Ограниченная способность к запоминанию**: Даже в пределах допустимого контекста модель может испытывать трудности с доступом к информации из начала очень длинной последовательности
- **Информационное бутылочное горлышко**: Модели могут терять важную информацию при обработке длинных контекстов

## Методы учета длинного контекста

### 1. Разреженное внимание (Sparse Attention)

Разреженное внимание - это ключевая стратегия, которая позволяет снизить вычислительную сложность с O(n²) до O(n) или O(n log n) за счет обращения не ко всем, а только к подмножеству токенов в последовательности.

#### 1.1. Фиксированные паттерны разреженности

- **Longformer**: Использует комбинацию локального внимания (токены в скользящем окне), глобального внимания (специальные токены, такие как [CLS]) и случайного внимания
- **BigBird**: Сочетает локальное, глобальное и случайное внимание с теоретическими гарантиями универсальной аппроксимации
- **Sparse Transformer**: Использует фиксированные паттерны внимания, где каждый токен обращается только к определенным удаленным токенам

#### 1.2. Обучаемое разреженное внимание

- **Hierarchical Sparse Attention (HSA)**: Иерархическая структура, где входные данные разбиваются на чанки, и для каждого чанка создается "лендмарк" (сжатое векторное представление). Выбор чанков для внимания осуществляется через обучаемую операцию, что позволяет модели учиться, где искать информацию. Механизм работает аналогично Mixture of Experts - токен генерирует запрос, вычисляет скалярные произведения с лендмарками прошлых чанков, выбирает топ-K чанков и выполняет плотное внимание только внутри этих выбранных чанков. Это позволяет HSA-UltraLong обобщаться с 32K обучающих окон на 16M контекста при инференсе без переобучения. [[models/hsa_ultralong.md]] - подробное описание HSA-UltraLong, модели с 16M контекстом
- **Mixture of Sparse Attention (MoSA)**: Вдохновлён архитектурой Mixture of Experts (MoE), каждый головной элемент внимания динамически выбирает разреженное подмножество токенов на основе содержания ввода. Использует механизм маршрутизации выбора эксперта (expert-choice routing), позволяющий модели адаптивно определять, на какие токены обращать внимание. Превосходит традиционные плотные модели по некоторым метрикам с сокращением ошибок предсказания до 27%. [[mixture_of_sparse_attention.md]] - подробное описание MoSA
- **DeepSeek Sparse Attention (DSA)**: Использует тонко настраиваемое разреженное внимание с top-K селектором для динамического определения, на какие токены обращать внимание. Снижение сложности до O(n*<w>), где <w> - средний размер адаптивного окна после тонкого отбора. Оборудование-ориентированная разработка с требованием специализированных ядер (FlashMLA и индексаторы логитов). [[models/deepseek_sparse_attention.md]]
- **STAR Attention**: Специализированный механизм разреженного внимания для эффективного инференса с длинными контекстами [[../../llm/star_attention_mechanism.md]]

#### 1.3. Адаптивное разреженное внимание

- **FlashAttention**: Оптимизированные вычисления внимания с улучшенной эффективностью использования памяти, снижением потребления памяти и ускорением обучения и инференса
- **FlashAttention-3**: Усовершенствованная версия с улучшенной производительностью для длинных контекстов
- **Log-Linear Attention**: Альтернативный подход к эффективному вниманию с логарифмически-линейной сложностью
- **Enhanced MLA with Top-K Selector**: Модифицированная Mixture of Logit Attention (MLA), усиленная за счет интеграции с DSA через top-K селектор для улучшенной эффективности на длинных контекстах [[../../llm/attention/enhanced_mla_with_top_k_selector.md]]

### 2. Линейное внимание (Linear Attention)

Линейное внимание приближает механизм стандартного внимания с линейной сложностью O(n) вместо квадратичной O(n²).

#### 2.1. Кернельные методы

- **Performer**: Использует FAVOR+ (Fast Attention Via positive Orthogonal Random features) для аппроксимации механизма внимания через случайные функции
- **Linear Transformer**: Применяет линейное приближение к softmax-вниманию, используя кернельные методы
- **Linformer**: Проектирует ключи и значения в более низкоразмерное пространство для снижения вычислительной сложности

#### 2.2. Альтернативные подходы

- **Mamba**: State Space Model, который эффективно обрабатывает длинные контексты с линейной сложностью [[mamba_architecture.md]]
- **RetNet**: Использует многоголовую LSTM для моделирования длинных последовательностей
- **RWKV**: RNN-подобная архитектура с трансформерным качеством, но линейной сложностью

### 3. Извлечение и дополнение (Retrieval-Augmented Methods)

Retrieval-Augmented Generation (RAG) и связанные методы позволяют моделям получать доступ к информации за пределами их фиксированного контекста, используя внешние источники знаний.

#### 3.1. Классические RAG-подходы

- **RAG (Retrieval-Augmented Generation)**: Извлекает релевантные документы из внешней базы знаний и использует их как дополнительный контекст для генерации [[retrieval_augmented_generation.md]]
- **DPR (Dense Passage Retrieval)**: Использует плотные эмбеддинги для эффективного поиска релевантных фрагментов
- **ColBERT**: Использует контекстно-зависимые эмбеддинги для более точного сопоставления документов

#### 3.2. Адаптивные RAG-подходы

- **Self-RAG**: Обучаемая стратегия извлечения, которая может адаптировать извлекаемые фрагменты в зависимости от запроса
- **RA-DIT**: Динамическая вставка извлеченной информации в различные слои трансформера
- **RAG-Token и RAG-Sequence**: Разные подходы к интеграции извлеченной информации на каждом шаге генерации

#### 3.3. Иерархические методы извлечения

- **MemWalker**: Использует древовидную структуру с иерархическими саммари для эффективного поиска информации [[memwalker_method.md]]
- **Hierarchical Retrieval**: Разбиение длинных документов на иерархические уровни для многоуровневого поиска

### 4. Методы с рекуррентностью/памятью

#### 4.1. Трансформеры с внешней памятью

- **Transformer-XL**: Использует рекуррентный механизм для продолжения контекста между сегментами
- **Compressive Transformer**: Расширяет Transformer-XL с дополнительным сжатым хранилищем для долгосрочной информации
- **RMT (Retrieval-Modified Transformer)**: Использует внешнюю память для хранения и извлечения информации

#### 4.2. Дополненные памятью трансформеры (MAT)

- **Memory-Augmented Transformers**: Систематический подход к расширению трансформеров с дополнительными механизмами памяти, вдохновленными нейронаучными принципами [[memory_augmented_transformers.md]]
- **NTM-Based Transformers**: Используют внешние адресуемые памяти для хранения и извлечения информации
- **Differentiable Neural Computers**: Расширенные системы памяти с возможностью обучения операций чтения и записи

### 5. Иерархические методы

#### 5.1. Многоуровневая обработка

- **Document-Level Transformers**: Обработка документов на разных уровнях (токены → предложения → абзацы → документы)
- **Chunk-and-Combine**: Разбиение длинного контекста на чанки, независимая обработка, затем объединение
- **Recursive Transformers**: Рекурсивная обработка с иерархическим объединением представлений

#### 5.2. Древовидные структуры

- **Tree Transformers**: Использование древовидных структур для эффективного моделирования иерархических зависимостей
- **Hierarchical Attention**: Разные механизмы внимания на разных уровнях иерархии
- **StructBERT и другие**: Модели, использующие структурные ограничения для обработки иерархической информации

### 6. Методы экстраполяции и экстенсии позиционного кодирования

#### 6.1. Rotary Position Embeddings (RoPE)

- **RoPE**: Вращательные позиционные эмбеддинги, позволяющие моделям лучше обобщаться на более длинные последовательности
- **YaRN (Yet another RoPE extensioN)**: Улучшенная версия RoPE для работы с более длинными контекстами
- **NTK-aware RoPE**: Адаптированная версия RoPE с учетом NTK (Neural Tangent Kernel) для лучшей экстраполяции

#### 6.2. Другие методы позиционного кодирования

- **ALiBi (Attention with Linear Biases)**: Использует линейные смещения для моделирования позиционной информации
- **Relative Position Embeddings**: Кодирование относительных позиций между токенами
- **NoPE (No Positional Encoding)**: Полное отсутствие позиционного кодирования, полагаясь исключительно на адресацию по контенту (как в HSA-UltraLong)

### 7. Методы оптимизации инференса

#### 7.1. KV Cache оптимизации

- **KV Cache Pruning**: Удаление наименее важных элементов из кэша ключ-значение для экономии памяти
- **KV Cache Compression**: Сжатие KV-кэша для уменьшения потребления памяти
- **Sliding Window KV Cache**: Использование скользящего окна для ограничения размера кэша

#### 7.2. Chunked и Streaming инференс

- **Chunked Processing**: Обработка длинных последовательностей по частям с сохранением контекста между чанками
- **Streaming Transformers**: Архитектуры, оптимизированные для потоковой обработки
- **Stateful Transformers**: Модели с сохранением внутреннего состояния между вызовами

## Сравнение методов

| Метод | Сложность | Преимущества | Недостатки | Применимость |
|-------|-----------|--------------|------------|--------------|
| **Разреженное внимание** | O(n) или O(n log n) | Высокая эффективность, хорошая производительность | Меньшая гибкость, сложность реализации | Длинные контексты до 1M+ токенов |
| **Линейное внимание** | O(n) | Масштабируемость, низкое потребление памяти | Потенциальная потеря точности | Очень длинные последовательности |
| **RAG-методы** | O(n) | Доступ к внешним знаниям, актуальность информации | Требует вспомогательной инфраструктуры, задержки | Знание-интенсивные задачи |
| **Методы с памятью** | O(n) | Продление контекста, долгосрочная память | Сложность управления памятью | Задачи с хронологической информацией |
| **KV Cache оптимизации** | O(n) | Эффективность инференса, совместимость | Ограничения по точности, утечка информации | Практическое развертывание |

## Современные достижения

### HSA-UltraLong: Иерархическое разреженное внимание

[[models/hsa_ultralong.md]] представляет собой 8-миллиардную Mixture-of-Experts (MoE) языковую модель, способную обрабатывать контексты длиной до 16 миллионов токенов. Основной инновацией является механизм Hierarchical Sparse Attention (HSA), который рассматривает предыдущие блоки контекста как "экспертов", доступных для извлечения.

### Mixture of Sparse Attention (MoSA)

MoSA вдохновлён архитектурой Mixture of Experts (MoE), где каждый головной элемент внимания динамически выбирает разреженное подмножество токенов на основе содержания ввода. [[mixture_of_sparse_attention.md]] 

### DeepSeek Sparse Attention (DSA)

DSA позволяет достичь разреженности с тонкой настройкой, используя top-K селектор для динамического определения, на какие токены обращать внимание. [[models/deepseek_sparse_attention.md]]

## Взаимосвязь с явлением attention sinks

Явление attention sinks, при котором определенные токены получают избыточно высокие оценки внимания от других токенов, имеет особое значение при работе с длинным контекстом. [[attention_sinks_in_transformer_models.md]] 

- **Влияние на KV Cache**: Attention sinks могут занимать значительную часть KV-кэша, особенно в длинных контекстах. Это особенно критично при использовании методов с KV cache оптимизациями, где attention sinks могут препятствовать эффективному сжатию или прореживанию кэша
- **Оптимизация памяти**: Понимание attention sinks важно для эффективной оптимизации использования памяти в моделях с длинным контекстом. Некоторые методы KV cache прунинга могут ошибочно удалять важную информацию, связанную с attention sinks
- **Информационный поток**: В длинных контекстах attention sinks могут создавать искажения в потоке информации, влияя на способность модели к восприятию удаленных зависимостей. Это особенно актуально для retrieval-augmented методов, где attention sinks могут мешать эффективному извлечению релевантной информации
- **Проблема экстраполяции**: Attention sinks могут усугубляться при экстраполяции моделей на более длинные контексты, чем использовались при обучении, особенно в методах с экстраполяцией позиционных эмбеддингов
- **Влияние на методы разреженного внимания**: В разреженных архитектурах attention sinks могут приводить к неравномерному использованию чанков или токенов, снижая эффективность разрежения
- **Масштабирование**: Attention sink phenomenon наблюдается не только в плотных, но и в MoE-архитектурах, что делает его особенно важным для современных моделей с длинным контекстом, таких как HSA-UltraLong [[hunyuan_models_attention_sink_research.md]]
- **Безопасность и надежность**: В контексте длинного контекста attention sinks могут быть связаны с определенными уязвимостями и вопросами безопасности, особенно в системах с долгосрочной памятью

## Применения

### Документо-ориентированные задачи
- Анализ юридических документов
- Обработка научных статей
- Работа с книгами и длинными текстами
- Извлечение информации из длинных документов

### Задачи с хронологической информацией
- Анализ временных рядов
- Обработка исторических данных
- Продукты с долгосрочной историей взаимодействий
- Агентные системы с долгосрочной памятью

### Знание-интенсивные задачи
- Вопрос-ответные системы для длинных документов
- Суммаризация длинных текстов
- Факт-чекинг с внешними источниками
- Когнитивные задачи с внешними знаниями

## Практические примеры и реализации

### 1. Примеры моделей с длинным контекстом

- **GPT-4**: Поддерживает до 128K токенов, используя оптимизированное внимание и KV-cache управление
- **Claude 3 Opus**: Работает с контекстом до 200K токенов, используя комбинацию разреженного внимания и оптимизаций памяти
- **Mistral Large 2**: Использует адаптивное управление контекстом для работы с до 128K токенов
- **HSA-UltraLong**: Модель, способная обрабатывать до 16 миллионов токенов с помощью Hierarchical Sparse Attention
- **DeepSeek R1**: Использует DeepSeek Sparse Attention для эффективной обработки длинных контекстов

### 2. Практические реализации Sparse Attention

- **Longformer Implementation**: 
  ```python
  # Пример реализации Longformer с локальным и глобальным вниманием
  from transformers import LongformerModel, LongformerTokenizer
  
  model = LongformerModel.from_pretrained('allenai/longformer-base-4096')
  # Глобальные токены (например, [CLS]) получают доступ ко всему контексту
  # Локальные токены используют скользящее окно
  ```

- **HSA Implementation Concept**:
  - Разбиение на чанки фиксированного размера
  - Создание лендмарков для каждого чанка
  - Динамический выбор чанков через softmax на основе релевантности
  - Плотное внимание только внутри выбранных чанков

### 3. Практические реализации Linear Attention

- **Performer**:
  ```python
  # Использование кернельных аппроксимаций вместо softmax
  # FAVOR+ позволяет линейную сложность через случайные ортогональные функции
  # QK^T -> φ(Q)^T φ(K) где φ - случайная функция приближения
  ```

- **Linformer**:
  - Проекция ключей и значений в низкоразмерное пространство
  - Сохранение корреляций между токенами через обучаемые проекции
  - Снижение сложности с O(n²) до O(n)

### 4. Практические реализации RAG-методов

- **Базовая RAG-реализация**:
  ```python
  # Индексация документов
  from sentence_transformers import SentenceTransformer
  from faiss import IndexFlatIP
  
  encoder = SentenceTransformer('multi-qa-mpnet-base-dot-v1')
  # Извлечение релевантных фрагментов по эмбеддингам
  # Генерация ответа с контекстом извлеченных фрагментов
  ```

- **Advanced RAG**:
  - Использование гибридного поиска (лексический + семантический)
  - Реранжирование результатов перед генерацией
  - Использование дипломатических моделей для переформулировки запросов
  - Оценка достоверности и источника извлеченной информации

### 5. Оптимизации KV Cache

- **KV Cache Pruning Strategies**:
  - Удаление наименее важных токенов на основе attention scores
  - Использование энтропии для определения информативности токенов
  - Стратегии на основе важности контекста для конкретной задачи

- **KV Cache Compression**:
  - Квантование значений в кэше (например, до int8)
  - Использование low-rank приближений для K и V матриц
  - Адаптивное сжатие на основе важности информации

### 6. Curriculum Learning для длинного контекста

- **HSA-UltraLong Training Strategy**:
  - Постепенное увеличение длины контекста от 512 до 16M токенов
  - Использование стадии "разогрева" с ограничением локального окна
  - Баланс между локальным и глобальным вниманием через адаптивные веса

### 7. Аппаратные оптимизации

- **FlashAttention Implementation**:
  - Использование блочной обработки для эффективного использования памяти
  - Точная настройка под архитектуру GPU (например, Tensor Cores)
  - Контроль трафика между памятью и вычислительными блоками

- **Специализированные ядра для DSA**:
  - FlashMLA и индексаторы логитов для DSA
  - Аппаратная поддержка top-K операций
  - Оптимизация для специфических моделей (DeepSeek)

## Будущие направления

### 1. Гибридные подходы
- Комбинация нескольких методов для получения преимуществ всех подходов
- Адаптивные архитектуры, меняющие стратегию в зависимости от входных данных
- Метаобучение для автоматического выбора оптимального метода

### 2. Автоматическая настройка архитектуры внимания
- Использование NAS (Neural Architecture Search) для определения оптимального механизма внимания для конкретной задачи
- Адаптивные механизмы, меняющие тип внимания в зависимости от входных данных

### 3. Энергоэффективные решения
- Разработка более вычислительно эффективных архитектур для работы с длинным контекстом
- Оптимизация архитектур и алгоритмов для снижения энергопотребления

## Источники

- "Attention is All You Need" (Vaswani et al., 2017) - основополагающая статья о трансформерах
- "Longformer: The Long-Document Transformer" - статья о фиксированных паттернах разреженного внимания
- "Big Bird: Transformers for Longer Sequences" - расширение Longformer с теоретическими гарантиями
- "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention" - подход к линейному вниманию
- "Efficiently Scaling Transformer Inference" - подходы к оптимизации инференса
- "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models" - статья о HSA-UltraLong
- "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing" - статья о MoSA
- "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention" - статья о DSA
- "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading" - статья о MemWalker

## Связи с другими темами

- [[transformer_architecture.md]] - основы архитектуры трансформеров
- [[memory_augmented_transformers.md]] - дополненные памятью трансформеры
- [[mat_taxonomy.md]] - таксономия MAT, объединяющая функциональные цели, типы памяти и техники интеграции
- [[mat_memory_operations.md]] - операции памяти в MAT: чтение, запись и забывание
- [[../../nlp/memory_architectures/retrieval_augmented_generation.md]] - подробное описание RAG-архитектур
- [[../../llm/specialized_attention_mechanisms_comparison.md]] - сравнение различных механизмов разреженного внимания, включая HSA, MoSA, DSA и другие
- [[../../llm/training/ultra_long_context_curriculum_learning.md]] - Curriculum Learning для сверхдлинного контекста в HSA-UltraLong
- [[../../hunyuan_models_attention_sink_research.md]] - исследования attention sinks в MoE моделях
- [[../../mamba_architecture.md]] - альтернативный подход к обработке длинных последовательностей
- [[memwalker_method.md]] - специализированный метод для работы с длинными контекстами