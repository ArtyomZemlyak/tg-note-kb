# Таксономия дополненных памятью трансформеров (MAT)

## Введение

Систематический обзор "Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Enhanced Model Architectures" представляет новую многомерную таксономию, которая структурирует разнообразную и быстро развивающуюся область MAT по трем основным осям: функциональные цели, типы памяти и техники интеграции.

## Три измерения таксономии

### 1. Функциональные цели

Классификация моделей по их основной задаче:

#### Инженерные цели
- **Расширение временного контекста** — преодоление ограничений фиксированного контекстного окна
- **Расширение пространственного контекста** — обработка более сложных структур данных
- **Управление вычислительными ресурсами** — оптимизация использования памяти и вычислений

#### Когнитивные цели
- **Улучшение рассуждений** — поддержка логических и аналитических процессов
- **Интеграция знаний** — объединение внутренних знаний модели с внешней информацией
- **Адаптация к данным вне распределения (OOD)** — адаптация к новым доменам
- **Метапознание** — способность модели осознавать свои собственные познавательные процессы

### 2. Типы памяти

Проведение прямых параллелей с биологической памятью:

#### Параметрическая память
- **Описание**: знания, закрепленные непосредственно в весах модели
- **Биологическая аналогия**: долговременные синаптические изменения
- **Преимущества**: постоянное хранение, интеграция с вычислениями
- **Ограничения**: статичность после обучения, трудности с обновлением

#### Состоятебная память
- **Описание**: информация, хранящаяся в краткосрочных активациях или скрытых состояниях
- **Биологическая аналогия**: рабочая память мозга
- **Преимущества**: быстрый доступ, динамическое обновление
- **Ограничения**: ограниченная емкость, потеря при завершении сеанса

#### Явная память
- **Описание**: масштабируемые внешние банки памяти (key-value хранилища, графы знаний)
- **Биологическая аналогия**: система индексации гиппокампа
- **Преимущества**: масштабируемость, гибкость, возможность обновления
- **Ограничения**: задержки при доступе, сложность управления

#### Гибридные и многошкальные системы
- **Описание**: комбинации предыдущих типов для баланса между доступом, временем и постоянством
- **Преимущества**: оптимальный баланс производительности и емкости
- **Ограничения**: сложность архитектуры, необходимость согласования между системами

### 3. Техники интеграции

Методы взаимодействия памяти с ядром трансформера:

#### Слияние на основе внимания
- **Описание**: чтение памяти через механизмы внимания
- **Преимущества**: естественная интеграция с архитектурой трансформеров
- **Примеры**: стандартные механизмы внимания, обращение к внешним хранилищам

#### Ассоциативное извлечение
- **Описание**: контентно-адресуемый доступ к памяти
- **Преимущества**: эффективный поиск по схожести, биологическая правдоподобность
- **Примеры**: ARMT (Associative Retrieval in Memory Transformers) с доступом за O(1)

#### Управляющие шлюзовые механизмы
- **Описание**: управление записью и обновлением памяти
- **Преимущества**: контролируемая динамика, предотвращение интерференции
- **Примеры**: шлюзы на основе удивления, обучаемые политики записи

## Эволюционные тенденции в таксономии

### По пластичности
- **Fixed (F)** — фиксированные после обучения (ранние модели)
- **Test-time adaptable (TT)** — адаптируемые на этапе инференса (современные модели)
- **Continual learning (CL)** — адаптируемые во время инференса с возможностью обучения в процессе работы (например, Titans с LMM)

### По триггеру записи
- **Статические правила** — безусловная перезапись (ранние подходы)
- **Управление удивлением (Surprise-gated)** — запись при обнаружении новой информации
- **Обучаемые политики (Policy-learned)** — адаптивные решения о записи
- **Градиентно-управляемое удивление (Gradient-based surprise)** — запись, управляемая градиентами ошибки предсказания с использованием момента, как в Titans (LMM)

### По архитектуре интеграции
- **Внешнее хранилище** — отдельный блок памяти (как в классических MAT)
- **Гибридная интеграция** — комбинация разных подходов (MAC, MAG, MAL в Titans)

## Значение таксономии

Эта многомерная таксономия позволяет:
- Структурировать существующие подходы в области MAT
- Определять пробелы в исследованиях
- Планировать новые архитектуры с учетом всех аспектов
- Сравнивать модели по различным измерениям
- Направлять будущие исследования в области памяти для трансформеров

## Связи с другими темами

- [[memory_augmented_transformers.md]] - Общий обзор MAT
- [[neuroscience_principles_in_transformers.md]] - Нейронаучные принципы в MAT
- [[mat_memory_operations.md]] - Операции памяти в MAT
- [[transformer_architecture.md]] - Базовая архитектура трансформеров
- [[retrieval_augmented_generation.md]] - Дополненные извлечением системы
- [[../../continual_learning/nested_learning.md]] - Парадигма вложенного обучения, расширяющая MAT принципами многоуровневой оптимизации
- [[../../continual_learning/continuum_memory_system.md]] - Система непрерывной памяти как развитие концепции памяти в MAT
- [[../../continual_learning/titan_architecture.md]] - Архитектура, предшествующая Nested Learning, вдохновленная MAT
- [[../../continual_learning/hope_architecture.md]] - Современная архитектура, объединяющая MAT и NL
- [[../../llm/architectures/lmm_long_term_memory_module.md]] - Модуль долгосрочной памяти (LMM) из архитектуры Titans с обучением во время инференса
- [[../../llm/architectures/mac_mag_mal_architectures.md]] - Три архитектуры интеграции памяти в Titans: MAC, MAG, MAL