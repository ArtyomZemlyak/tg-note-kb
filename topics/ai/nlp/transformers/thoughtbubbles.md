# Thoughtbubbles: новая архитектура трансформера для параллельных рассуждений в латентном пространстве

## Описание

Thoughtbubbles — новая архитектура трансформера, которая учится динамически распределять параллельные вычисления в латентном пространстве. В отличие от Chain-of-Thought, где модели генерируют явный текст для рассуждений, в Thoughtbubbles модель может «разветвлять» (клонировать) или удалять residual streams для определённых токенов. Токены, требующие более глубокой обработки, формируют временные «пузыри» параллельных вычислений внутри сети, которые затем сливаются для получения итогового результата.

## Ключевые особенности

### 1. Параллельные рассуждения в латентном пространстве
- Модель может адаптивно распределять вычислительные ресурсы в своём внутреннем представлении (латентном пространстве)
- Не требует явного генерирования текста для рассуждений
- Позволяет «думать» в параллельных потоках внутри сети

### 2. Оценка и бюджетирование
- Использует «кумулятивную оценку» (P_cum), присваиваемую каждому residual stream
- Выучиваемая функция fθ(k) вычисляет «оценку сохранения» и «оценку ветвления» для каждого входящего residual stream
- Модель работает в рамках бюджета максимального размера блока (κ)
- Выполняет отбор top-K по всем потенциальным оценкам, гарантируя существование только самых перспективных вычислительных путей

### 3. Обучение через ослабление (attenuation)
- Кумулятивные оценки используются для прямой модуляции последующих вычислений
- Как механизм внимания, так и обновления residual stream масштабируются с помощью этих оценок
- Модель вынуждена «делать ставки» на то, какие потоки окажутся полезными

### 4. Слияние на выходе
- Если какой-либо токен представлен несколькими разветвлёнными потоками, его параллельные «мысли» объединяются
- Каждый поток декодируется в распределение вероятностей и объединяется через взвешенное среднее
- Итоговые кумулятивные оценки используются в качестве весов для получения финального предсказания токена

## Механизм ветвления и прунинга

1. **Оценка**: На каждом слое ветвления выучиваемая функция fθ(k) вычисляет «оценку сохранения» и «оценку ветвления» для каждого входящего residual stream.

2. **Бюджетирование**: Для управления ресурсами модель работает в рамках бюджета на максимальный размер блока, κ. Производится отбор top-K по всем потенциальным оценкам сохранения и ветвления.

3. **Обучение через ослабление**: Кумулятивные оценки используются для прямой модуляции или «ослабления» последующих вычислений. По сути, модель вынуждена «делать ставки» на то, какие потоки окажутся полезными.

4. **Слияние на выходе**: После последнего слоя разветвления объединяются через взвешенное среднее.

## Экспериментальные результаты

- **Превосходство в перплексии**: Thoughtbubbles стабильно достигает более низкой перплексии по сравнению с бейзлайнами на всех масштабах
- **Эффективность масштабирования**: Модель Thoughtbubbles на 319М параметров достигла на OpenWebText более низкой перплексии (20.23), чем гораздо более крупная базовая модель на 772М (21.22)
- **Высокая производительность в zero-shot**: Особенно хорош на задачах, требующих глубокого контекста или рассуждений (LAMBADA, HellaSwag)
- **Интерпретируемые вычисления**: Модель учится стратегически распределять вычисления в областях с более высокой предсказательной неопределённостью

## Сравнение с другими подходами

### Отличие от Chain-of-Thought (CoT)
- В CoT модели генерируют явную цепочку рассуждений в виде текста
- Thoughtbubbles выполняет рассуждения во внутреннем латентном пространстве без явной генерации текста
- CoT по своей природе последователен; Thoughtbubbles использует параллельные вычисления
- CoT требует инструкций для явного генерирования рассуждений; Thoughtbubbles учится этому в полностью необучаемом режиме

### Отличие от Free Transformer
- Free Transformer использует латентные переменные для кодирования высокоуровневой информации
- Thoughtbubbles реализует динамическое разветвление и слияние residual streams
- Free Transformer более сосредоточен на глобальном планировании; Thoughtbubbles - на адаптации вычислительного бюджета для отдельных токенов

## Ограничения и будущие направления

1. **Реальная производительность (Wall-Clock)**: Текущая реализация на чистом PyTorch не оптимизирована по скорости.

2. **Поток градиентов**: Механизм жёсткого отбора top-K может создавать «узкое место для градиентов».

3. **Масштабирование на более сложные задачи**: Модели не оценивались на сложных бенчмарках для рассуждений, таких как GSM8k.

## Значение для области

Thoughtbubbles представляет собой значительный шаг вперёд в создании более способных и эффективных моделей для рассуждений. В отличие от подходов, которые полагаются на хитрые техники промптинга, таких как Chain-of-Thought, для «выуживания» рассуждений из статичных архитектур, Thoughtbubbles предлагает будущее, в котором рассуждения — это внутреннее, эмерджентное свойство самой архитектуры. Переход от явных, последовательных токенов к неявным, динамическим «пузырям» латентной мысли даёт впечатляющее представление о новом поколении систем ИИ, которые обладают врождённой способностью думать.

## Связь с другими темами

- [[latent_variables_reasoning.md]] - подходы к использованию латентного пространства для рассуждений в трансформерах
- [[reasoning_patterns.md]] - другие паттерны рассуждения в моделях
- [[transformer_architecture.md]] - базовая архитектура, расширенная механизмом Thoughtbubbles
- [[free_transformer.md]] - альтернативный подход с использованием латентных переменных
- [[next_gen_transformer_architectures.md]] - перспективные архитектуры трансформеров

## Источники

- Основная статья: "Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space" (Liu, Murty, Manning, Csordás, 2025) - https://arxiv.org/abs/2510.00219
- Репозиторий: https://github.com/stanfordnlp/thoughtbubbles
- Обзор статьи: https://arxiviq.substack.com/p/thoughtbubbles-an-unsupervised-method