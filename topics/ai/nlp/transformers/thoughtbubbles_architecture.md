# Архитектура Thoughtbubbles: технические детали

## Общая структура

Thoughtbubbles представляет собой новую архитектуру трансформера, которая вводит специализированные «слои ветвления» между стандартными блоками трансформера. Эти слои позволяют модели динамически управлять параллельными вычислительными потоками для каждого токена.

## Механизм ветвления

### Функция оценки
- На каждом слое ветвления выучиваемая функция fθ(k) вычисляет:
  - «Оценку сохранения» для каждого входящего residual stream
  - «Оценку ветвления» для каждого входящего residual stream
- Эти оценки комбинируются с кумулятивной оценкой из предыдущего слоя
- Для численной стабильности эти оценки хранятся в логарифмическом пространстве

### Алгоритм ветвления
1. Вычисление оценок для каждого residual stream
2. Комбинация с кумулятивной оценкой
3. Применение бюджетного ограничения (максимальный размер блока κ)
4. Выбор top-K по всем потенциальным оценкам
5. Сохранение/разветвление потоков с высокими оценками
6. Прунинг (удаление) потоков с низкими оценками

## Механизм ослабления (Attenuation)

### Принцип работы
- Кумулятивные оценки используются для прямой модуляции последующих вычислений
- Обновления residual stream масштабируются с помощью этих оценок
- Механизм внимания также масштабируется этими оценками
- Модель вынуждена «делать ставки» на то, какие потоки окажутся полезными

### Обоснование
- Если модель обратит внимание на поток с низкой оценкой, который затем будет отброшен, её предсказательная способность пострадает
- Это давление заставляет функцию оценки учиться определять семантически или вычислительно важные токены без прямого надзора
- Обучение происходит исключительно на основе лосса языкового моделирования

## Модифицированный Rotational Position Embedding (RoPE)

### Проблема
- При разветвлении одного токена возникает необходимость отличать его разные разветвления
- В то же время нужно сохранить семантическую согласованность разветвлений

### Решение
- Использование модифицированного RoPE, который держит позиционные эмбеддинги разветвлений одного токена «ближе друг к другу»
- Это позволяет сохранить семантическую согласованность при различении разных вычислительных путей

## Слияние параллельных потоков

### Процесс
- После последнего слоя, если токен представлен несколькими разветвлёнными потоками, его параллельные «мысли» объединяются
- Каждый поток декодируется в распределение вероятностей
- Потоки объединяются через взвешенное среднее
- Веса определяются итоговыми кумулятивными оценками

## Сравнение с Chain-of-Thought

| Аспект | Chain-of-Thought | Thoughtbubbles |
|--------|------------------|----------------|
| Форма рассуждений | Явная генерация текста | Неявные вычисления в латентном пространстве |
| Параллелизм | Последовательные рассуждения | Параллельные рассуждения в латентном пространстве |
| Надзор | Требует инструкций для генерации рассуждений | Обучается в полностью необучаемом режиме |
| Использование вычислительного бюджета | Фиксированный для всей цепочки | Адаптивный для каждого токена |
| Интеграция в архитектуру | Внешний подход (через промпты) | Внутреннее свойство архитектуры |

## Экспериментальная валидация

### Архитектурные конфигурации
- Модели с 150М, 319М и 772М параметрами
- Обучение на датасетах OpenWebText и peS20
- Сравнение с базовыми трансформерами и моделями с дублированными токенами

### Основные результаты
- Модель на 319М параметров достигла лучшей перплексии (20.23), чем базовая модель на 772М параметров (21.22)
- Значительное превосходство на задачах, требующих глубокого контекста (LAMBADA) и рассуждений (HellaSwag)
- Модель учится стратегически распределять вычисления в областях с высокой предсказательной неопределённостью

## Технические особенности реализации

### Бюджетирование ресурсов
- Использование параметра κ для ограничения максимального размера блока
- Гарантия, что существуют только самые перспективные вычислительные пути
- Баланс между вычислительной эффективностью и качеством рассуждений

### Управление градиентами
- Потенциальная проблема с градиентными узкими местами из-за жёсткого отбора top-K
- Предложение использования рандомизации во время обучения (аналог Gumbel-Softmax)
- Передача градиентов через недифференцируемые шаги отбора

## Связь с другими темами

- [[transformer_architecture.md]] - основы архитектуры трансформеров, расширенные Thoughtbubbles
- [[free_transformer.md]] - другой подход к использованию латентного пространства для рассуждений
- [[differential_transformer.md]] - альтернативная архитектура трансформеров с модифицированным вниманием
- [[reasoning_patterns.md]] - паттерны рассуждений в моделях
- [[latent_variables_reasoning.md]] - использование латентных переменных для рассуждений в трансформерах