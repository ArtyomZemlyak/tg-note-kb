# Архитектура трансформеров (Transformer Architecture)

## Общее описание

Архитектура трансформеров - это тип нейронной сети, который революционизировал обработку естественного языка (NLP) и стал основой для большинства современных больших языковых моделей (LLM). Впервые представленная в статье "Attention is All You Need" в 2017 году, архитектура трансформеров использует механизмы внимания для обработки последовательностей, избегая необходимости в рекуррентных или сверточных слоях.

## Основные компоненты

### 1. Механизм внимания (Attention Mechanism)

Механизм внимания позволяет модели фокусироваться на наиболее релевантных частях входных данных при генерации каждого элемента выходной последовательности. Это особенно важно для задач, где зависимости между удаленными элементами последовательности критичны для понимания.

### 2. Self-Attention (Самовнимание)

Self-attention позволяет каждому элементу последовательности взаимодействовать с другими элементами в той же последовательности. Это позволяет модели учитывать контекст при обработке каждого токена.

### 3. Multi-Head Attention (Многоголовое внимание)

Multi-head attention позволяет модели одновременно обрабатывать информацию из разных подпространств пространства внимания. Это расширяет способность модели захватывать различные типы зависимостей между токенами.

### 4. Encoder и Decoder

Архитектура трансформеров состоит из:
- **Encoder**: обрабатывает входную последовательность, создавая представления каждого токена с учетом контекста
- **Decoder**: генерирует выходную последовательность, используя как представления из энкодера, так и предыдущие токены из выходной последовательности

### 5. Позиционные кодировки (Positional Encodings)

Поскольку трансформеры не имеют встроенной информации о порядке последовательности (в отличие от RNN), позиционные кодировки добавляются к вложениям (embeddings) для предоставления информации о позиции токенов в последовательности.

### 6. Feed-Forward Networks (FFN)

Каждый токен проходит через один и тот же полностью связанный нейронный слой независимо от других токенов. Обычно представляет собой два линейных преобразования с нелинейностью (например, ReLU) между ними.

### 7. Layer Normalization и Residual Connections

Для стабилизации обучения и улучшения градиентного потока используются нормализация слоев и остаточные соединения.

## Преимущества архитектуры трансформеров

- **Параллелизуемость**: в отличие от RNN, трансформеры позволяют параллельную обработку всей последовательности, что значительно ускоряет обучение
- **Долгосрочные зависимости**: механизм внимания позволяет эффективно захватывать зависимости между удаленными элементами последовательности
- **Гибкость**: может быть адаптирован для различных задач, включая машинный перевод, генерацию текста, классификацию и другие

## Варианты архитектуры трансформеров

- **Encoder-only**: BERT и его варианты, подходят для задач понимания текста
- **Decoder-only**: GPT и его варианты, подходят для генерации текста
- **Encoder-decoder**: T5, BART, подходят для задач преобразования текста в текст
- **Диффузионные трансформеры**: Модели, использующие принципы диффузии с архитектурой трансформера для генерации, например, через многократное маскирование и восстановление (см. [[../../llm/diffusion_models.md]] - общее описание диффузионных моделей и [[../../llm/architectures/diffusion/text_diffusion_models.md]] - альтернативный подход к генерации текста с использованием encoder-only архитектуры)
- **Условные VAE-трансформеры**: Free Transformer и другие архитектуры, использующие латентные переменные для улучшения рассуждений (см. [[free_transformer.md]] - интеграция VAE в трансформеры для более эффективных рассуждений)
- **Дифференциальные трансформеры**: Трансформеры с модифицированным механизмом внимания, который вычисляет внимание на основе разницы между представлениями токенов для лучшего моделирования относительных отношений (см. [[differential_transformer.md]] - архитектура, улучшающая понимание относительных позиций и отношений)

## Применение

- Машинный перевод
- Генерация текста
- Классификация текста
- Суммаризация
- Вопрос-ответные системы
- Извлечение информации

## Связи с другими темами

- [[course_stanford_cme295.md]] - подробности о курсе, изучающем трансформеры
- [[evolution_of_nlp_methods.md]] - эволюция NLP методов, приведших к трансформерам
- [[transformers_and_llms.md]] - связь трансформеров с LLM
- [[../../llm/llm_architectures_comparison.md]] - Сравнение различных архитектур LLM, включая encoder-only, decoder-only и encoder-decoder варианты
- [[../../llm/autoregressive_models.md]] - Авторегрессивные модели, основанные на decoder-only архитектуре трансформеров
- [[../../llm/specialized_attention_mechanisms.md]] - Специализированные механизмы внимания, используемые в современных трансформерах для повышения эффективности
- [[../../llm/mixture_of_experts_architecture.md]] - Mixture of Experts архитектуры, расширение стандартных трансформеров для масштабирования
- [[../../llm/models/qwen/qwen3-vl.md]] - пример применения Vision Transformer
- [[../../llm/models/qwen/vlm_models.md]] - пример использования Vision Transformer
- [[next_gen_transformer_architectures.md]] - перспективные архитектуры трансформеров и усовершенствованные механизмы внимания (2024-2025)