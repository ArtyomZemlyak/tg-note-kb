# Векторные представления слов (Word Embeddings)

## Общее описание

Векторные представления слов (word embeddings) - это числовые векторы, которые представляют слова в непрерывном векторном пространстве таким образом, что семантически похожие слова находятся близко друг к другу. Это фундаментальная концепция в обработке естественного языка, которая легла в основу многих современных подходов к анализу текста.

## Исторические основы

Классические подходы к представлению слов, такие как one-hot encoding, создавали разреженные векторы без семантических связей. Переход к плотным векторным представлениям произошел с развитием методов машинного обучения, основанных на нейронных сетях.

## Основные методы

### 1. Word2Vec

Революционный подход, представленный Mikolov и др. в 2013 году, включает две основные архитектуры:
- **Continuous Bag of Words (CBOW)**: предсказывает целевое слово на основе окружающих его слов
- **Skip-gram**: предсказывает окружающие слова на основе одного целевого слова

### 2. GloVe (Global Vectors for Word Representation)

Метод, который объединяет статистику глобального контекста (из матрицы слово-контекст) с локальной информацией из нейронных моделей.

### 3. FastText

Расширение Word2Vec, которое учитывает субсловную информацию, позволяя лучше обрабатывать редкие слова и морфологически богатые языки.

## Линейные отношения в эмбеддингах

Одним из самых удивительных открытий в области векторных представлений слов стала линейная структура отношений. Работа "Linguistic Regularities in Continuous Space Word Representations" продемонстрировала, что векторная арифметика может отражать семантические и синтаксические отношения:

```
vector("King") - vector("Man") + vector("Woman") ≈ vector("Queen")
```

Это открытие стало основой для понимания "[гипотезы линейности](../theory/linearity_hypothesis.md)" в нейронных сетях.

## Современные подходы

### Контекстно-зависимые эмбеддинги

- **ELMo**: использует bidirectional LSTM для создания контекстно-зависимых представлений
- **BERT**: использует механизм внимания для получения двунаправленных представлений
- **GPT**: использует causal (одностороннее) внимание для генерации последовательностей

### Многоязычные эмбеддинги

Современные подходы позволяют создавать универсальные эмбеддинги, которые работают на нескольких языках одновременно, позволяя перенос знаний между языками.

## Применение

- Анализ настроений
- Классификация текстов
- Улучшение качества машинного перевода
- Вопросно-ответные системы
- Семантический поиск

## Преимущества

- **Семантическая близость**: похожие слова находятся близко в векторном пространстве
- **Математические свойства**: векторные операции отражают отношения между словами
- **Компактность**: плотные векторы по сравнению с разреженными one-hot представлениями
- **Переносимость**: можно использовать в различных задачах NLP

## Ограничения

- **Масштабирование**: трудно обновлять при добавлении новых слов
- **Контекстно-зависимость**: одно и то же слово может иметь разные значения в разных контекстах
- **Предвзятость**: эмбеддинги могут переносить предвзятости из обучающих данных

## Связи с другими темами

- [[../theory/linearity_hypothesis.md|Гипотеза линейности]] - Исследует линейные свойства эмбеддингов
- [[embedders/index.md|Системы эмбеддингов]] - Современные подходы к созданию эмбеддингов
- [[transformers/index.md|Трансформеры]] - Современные архитектуры, использующие эмбеддинги

## Источники и ссылки

- Mikolov, T., et al. (2013). Efficient Estimation of Word Representations in Vector Space
- Pennington, J., et al. (2014). GloVe: Global Vectors for Word Representation
- Bojanowski, P., et al. (2017). Enriching Word Vectors with Subword Information