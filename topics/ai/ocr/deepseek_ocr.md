# DeepSeek OCR

## Описание

DeepSeek OCR - новая модель оптического распознавания символов (OCR), которая показывает значительные преимущества в соотношении цена/качество по сравнению с существующими решениями. Модель разработана DeepSeek AI и представлена как решение для контекстной оптической компрессии. Модель является исследованием в области сжатия длинных контекстов через оптическое 2D-сопоставление.

## Архитектура

DeepSeek-OCR архитектурно следует стандартной схеме для VLM: визуальный энкодер, подключенный к предобученной языковой модели (в данном случае DeepSeek-3B). Однако вместо стандартного CLIP/SigLIP в качестве энкодера используется пайплайн из SegmentAnything (SAM-ViT-Det), свёрточного адаптера и CLIP (CLIP-ViT), который в статье называют DeepEncoder.

### DeepEncoder

DeepEncoder состоит из:
- **SegmentAnything (SAM-ViT-Det)**: Может принимать на вход изображение любого размера; токенизированные патчи обрабатываются независимо друг от друга благодаря window attention — поэтому количество вычислений уменьшается.
- **Свёрточный адаптер**: Снижает количество токенов в 16 раз
- **CLIP (CLIP-ViT)**: Глобальный аттеншн в CLIP-ViT агрегирует токены

Авторы хотели, чтобы энкодер был эффективным и быстрым, и чтобы в уже обученном энкодере можно было легко «на лету» менять количество визуальных токенов.

## Поддержка динамического разрешения

DeepSeek-OCR поддерживает оба основных подхода к динамическому разрешению:
- **Tile-based-resolution**: изображение разрезается на квадраты (например 512×512 пикселей), и каждое прогоняется через визуальный энкодер
- **Native-resolution**: визуальный энкодер обучается принимать на вход изображение любого размера, используя подходящие для этого позиционные эмбеддинги типа RoPE

Во время обучения модель видит как большие, так и маленькие картинки в разных представлениях, что улучшает её универсальность.

## Технические характеристики

- Поддерживаемые разрешения: 
  - Tiny: 512×512 (64 визионных токена)
  - Small: 640×640 (100 визионных токенов)
  - Base: 1024×1024 (256 визионных токенов)
  - Large: 1280×1280 (400 визионных токенов)
  - Dynamic resolution (Gundam): n×640×640 + 1×1024×1024
- Производительность: ~2500 токенов/с (на A100-40G)
- Поддерживает режимы: документ-конверсия, изображение-OCR, свободный OCR, парсинг фигур
- Использует точность bfloat16 и процессоры n-gram логитов для предотвращения повторений
- Для надёжного чтения текста можно использовать примерно в 10 раз меньше визуальных токенов, чем необходимо текстовых токенов для представления текста на изображении

## Обучение модели

Процесс обучения упрощён и включает только две стадии: тренировку энкодера и обучение модели целиком.

### Этап 1: Обучение энкодера
Во время тренировки энкодера DeepEncoder обучается работать и в режиме native-resolution, и в режиме tile-based-resolution. Энкодер тренируется на парах картинок и текстовых описаний по схеме, описанной в статье Vary: к нему приделывается маленький текстовый декодер, и они вместе обучаются авторегрессионно.

### Этап 2: Обучение всей VLM
Второй этап с обучением всей VLM повторяет обычный претрейн/SFT во множестве других VLM. Для обучения используется типичная смесь пар (картинка-описание) и только текстовых данных с упором на OCR: печатный текст, графики и таблицы, формулы. В отличие от других OCR-специализированных VLM (обычно обучаемых только на английском и китайском), датасеты содержат более 100 языков.

## Результаты

DeepSeek-OCR показывает отличное качество на OmniDocBench, опережая в зависимости от разрешения не только сильные опенсорсные бэйзлайны, вроде Qwen-2.5VL, но и Gemini2.5-Pro. При этом скорость обработки на GPU сопоставима с пайплайновыми OCR-пакетами.

Замеры точности распознавания в зависимости от размера изображения (и числа токенов) на OCR-бенчмарке Fox показывают, что для надёжного чтения текста можно использовать примерно в 10 раз меньше визуальных токенов, чем необходимо текстовых токенов для представления текста на изображении. При уменьшении этого соотношения качество чтения быстро падает.

Хотя результаты получились впечатляющими, в работе использовались только бенчмарки с фокусом на PDF-подобных картинках, а другие, более разнообразные OCR-бенчи для VLM (OCRBench_v2, CC-OCR) не замерялись. Также в статье нет аблейтов влияния на результаты ни выбранной архитектуры, ни этапов обучения, поэтому авторы сами называют свои результаты proof-of-concept.

## Применение в проекте alphaXiv

Создатели проекта alphaXiv использовали DeepSeek OCR для обработки более 500 000 статей по ИИ. Основная цель - извлечение данных из таблиц и диаграмм для анализа самых популярных бенчмарков и датасетов.

### Результаты

- Обработка 500 000+ статей обошлась всего в $1000
- По сравнению с Mistral OCR, которая до этого считалась одной из лучших по соотношению цена/качество, процесс с ней обошелся бы в $7500
- Это демонстрирует значительное снижение затрат при использовании DeepSeek OCR

## Перспективы использования

### Конвертация PDF в Markdown

alphaXiv объявили о планах релиза датасета со статьями с arXiv, сразу переведенными из PDF в формат Markdown с помощью DeepSeek OCR. Это значительно упростит обработку и анализ научных публикаций.
Преобразование PDF в Markdown особенно мощное применение технологии, позволяющее автоматизировать обработку научных публикаций.

### Возможности использования

- Конвертация документов: `<image>\n<|grounding|>Convert the document to markdown.`
- OCR изображений: `<image>\n<|grounding|>OCR this image.`
- Свободный OCR: `<image>\nFree OCR.`
- Парсинг фигур: `<image>\nParse the figure.`
- Описание изображений: `<image>\nDescribe this image in detail.`
- Локализация объектов: `<image>\nLocate <|ref|>xxxx<|/ref|> in the image.`

### Демонстрационный проект

Проект по извлечению данных из статей в качестве демонстрации возможностей DeepSeek OCR является примером практического применения технологии для анализа научной литературы.

## Значение

DeepSeek OCR представляет собой прорыв в области обработки документов, особенно для академических и исследовательских целей, где требуется преобразование больших объемов PDF-документов в структурированные данные. Технология показывает, как современные визионные энкодеры могут быть эффективно интегрированы в языковые модели для решения задач оптического распознавания.

## Реализации

- [[deepseek_ocr_rs.md]] - Rust-реализация DeepSeek-OCR с высокой производительностью, CLI и HTTP-сервером, совместимым с OpenAI

## Связи с другими темами

- [[deepseek_ocr_training_process.md]] - Подробное описание процесса обучения DeepSeek-OCR
- [[specialized_vlm_models.md]] - Контекст других специализированных OCR-VLM моделей
- [[dynamic_resolution_approaches.md]] - Подробное описание подходов к динамическому разрешению, использованных в модели
- [[hunyuanocr.md]] - Компактная OCR-модель от Tencent с 1 миллиардом параметров, достигающая SOTA результатов, конкурент DeepSeek OCR по эффективности
- [[object_detection_yolo_ocr.md]] - Общие сведения об OCR и его применении в системах компьютерного зрения
- [[chandra_ocr.md]] - Современная OCR-модель от DataLab, конкурирующая с DeepSeek OCR по качеству и функциональности
- [[lightonocr.md]] - Альтернативная OCR-модель от LightOnAI, дистиллированная из Qwen2-VL-72B-Instruct, сравнимая по качеству, но быстрее и дешевле
- [[docling.md]] - Библиотека обработки документов от IBM с похожими возможностями по преобразованию документов
- [[multimodal/ibm_granite_docling_258m.md]] - Мультимодельная модель IBM для преобразования документов, альтернативное решение для обработки документов

## Источники

1. [DeepSeek-OCR: Contexts Optical Compression](https://arxiv.org/abs/2510.18234) - Основная научная статья, описывающая архитектуру и возможности DeepSeek-OCR
2. [CV Time: DeepSeek-OCR разбор статьи](https://towardsai.net/p/machine-learning/deepseek-ocr-contexts-optical-compression-paper-review) - Детальный разбор статьи о DeepSeek-OCR