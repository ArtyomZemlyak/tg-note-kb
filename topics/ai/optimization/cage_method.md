# CAGE: Кривизна-Ориентированная Оценка Градиентов для Точной Квантования-Ориентированной Обучения

## Краткое описание

CAGE (CURVATURE-AWARE GRADIENT ESTIMATION) - это новый метод квантования-ориентированного обучения (Quantization-Aware Training, QAT), разработанный для решения проблемы обучения нейронных сетей с низкой точностью весов и активаций. Метод особенно актуален в контексте современных требований к эффективному обучению больших языковых моделей (LLM).

## Основная информация

### Введение

С ростом масштабов и, соответственно, стоимости обучений больших языковых моделей все острее стоит вопрос эффективного обучения. В ряде прошлых работ (QuEST, Quartet, FP4 All the way, Training LLM with MXFP4) было показано, что можно успешно обучать с весами и активациями в низкой точности, применив некоторые специальные техники. CAGE продолжает эту линию исследований, модифицируя алгоритм оптимизатора.

### Проблема, которую решает CAGE

Традиционные методы QAT используют STE (Straight Through Estimator) и его модификации, когда градиент просто пробрасывается через недифференцируемую операцию квантования. Трюк рабочий, но не имеет под собой теоретических гарантий. При экстремальных битностях (например, 4-бит или 1-бит) традиционные методы могут не сходиться к оптимуму.

### Математический подход

Авторы CAGE предлагают рассматривать QAT как задачу оптимизации с ограничениями и переходят к задаче безусловной оптимизации с множителем Лагранжа:

`min L(w) = min L_orig (w) + λ (Q(x) - x)`

Величина λ определяет баланс между лоссом задачи и ограничением. Такая добавка математически эквивалентна добавлению error feedback. Рассматривают два варианта - coupled/decoupled - где добавка подается в градиент или момент, но в итоге выбирают decoupled, как более удобный.

CAGE основан на AdamW и отличается от него только наличием error feedback. Для лучшей сходимости константа регуляризации λ разогревается от 0 до максимального значения.

### Основные особенности

- Основывается на оптимизаторе AdamW
- CAGE отличается от AdamW только наличием error feedback
- Для лучшей сходимости константа регуляризации λ разогревается от 0 до максимального значения
- Показывает стабильное улучшение по лоссу по сравнению с базовым алгоритмом QuEST для разных битностей

### Экспериментальные результаты

Метод валидирован на обучении семейства моделей Llama от 30 до 800 М параметров. CAGE стабильно дает улучшение по лоссу по сравнению с QuEST для разных битностей.

"Эффективная емкость" (просадка по лоссу в scaling laws нормализованная на битность) примерно на 0.5 лучше по сравнению с QuEST.

CAGE успешно работает и с MXFP4. На модельной квадратичной задаче SGD/Adam с STE не могут попасть в оптимум при 4-битной квантизации, а CAGE может.

### Сравнение с другими методами

- QuEST - метод для QAT, который позволяет стабильно обучать LLM с 4-бит или даже 1-бит весами и активациями
- CAGE улучшает точность QAT по сравнению с предыдущими методами, используя кривизна-ориентированную коррекцию градиента
- В отличие от STE, CAGE имеет более теоретически обоснованный подход через метод множителей Лагранжа

### Потенциальные применения

- Обучение эффективных LLM на Blackwell чипах для форматов MXFP4/NVFP4
- Сценарии, где критична память и вычислительные ресурсы
- Случаи, где необходима высокая точность при низкой битности

## Новые концепции и термины

- **CAGE (CURVATURE-AWARE GRADIENT ESTIMATION)**: Кривизна-ориентированная оценка градиентов для точной квантования-ориентированной обучения
- **Error Feedback**: Механизм добавления компенсации ошибки квантования в процесс обучения
- **Coupled/Decoupled Error Feedback**: Варианты добавления error feedback - в градиент (coupled) или в момент (decoupled)
- **Lagrange Multiplier in QAT**: Использование множителей Лагранжа для преобразования задачи оптимизации с ограничениями в безусловную

## Примеры применения

- Обучение LLM с 4-битной точностью при сохранении качества
- Обучение с MXFP4 форматами на современных GPU
- Сценарии с ограниченными вычислительными ресурсами

## Связи с другими темами

- [[qat_modern_methods.md]] - Современные методы квантования-ориентированного обучения
- [[../../llm/model_quantization_techniques.md]] - Общие методы квантования моделей
- [[../../llm/llm_training_optimization.md]] - Оптимизация обучения LLM
- [[ste_quantization_method.md]] - Straight Through Estimator, базовый метод для QAT
- [[quest_quantization_method.md]] - Метод QuEST, с которым сравнивается CAGE
- [[optimization_algorithms.md]] - Оптимизационные алгоритмы, включая AdamW
- [[../../optimization/quantization/int_vs_fp_quantization_comparison.md]] - Сравнение INT и FP форматов квантования, контекст для MXFP4 форматов
- [[../../optimization/deep_optimizers.md]] - Дополнительный контекст об оптимизаторах
- [[../../llm/tools/moe_quant.md]] - Другие методы квантования для LLM

## Источники

1. [CAGE: CURVATURE-AWARE GRADIENT ESTIMATION FOR ACCURATE QUANTIZATION-AWARE TRAINING](https://arxiv.org/abs/2510.18784) - Оригинальная статья о методе CAGE, описывающая кривизна-ориентированную оценку градиентов для точной квантования-ориентированной обучения
2. [QuEST: Stable Training of LLMs with 1-Bit Weights and Activations](https://arxiv.org/abs/2502.05003) - Статья о методе QuEST, с которым сравнивается CAGE
3. [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2306.06965) - Исследование различных форматов квантования, включая MXFP4, с которыми работает CAGE

## Дополнительные материалы

- В исходной статье упоминается изображение (img_1763014243_AgACAgIA.jpg), которое, предположительно, содержит визуализацию экспериментальных результатов CAGE или сравнение с другими методами, но точное содержание неизвестно