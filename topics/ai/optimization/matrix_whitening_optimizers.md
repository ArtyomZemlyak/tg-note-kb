# Матричное отбеливание оптимизаторов (Matrix-Whitening Optimizers)

## Описание

Матричные оптимизаторы с отбеливанием - это класс методов оптимизации второго порядка, которые используют информацию о кривизне ландшафта потерь для улучшения сходимости нейронных сетей. Эти методы применяют процесс, называемый "отбеливанием", для декорреляции признаков и нормализации их дисперсий, что отличает их от традиционных оптимизаторов первого порядка, таких как Adam.

## Основные компоненты

### 1. Спектральная нормализация

Спектральная нормализация - это процесс нормализации спектральных свойств (собственных значений) матрицы предобуславливания, используемой в оптимизации. Традиционно методы с матричным отбеливанием мотивировались этим подходом, рассматривая его как поиск направления наискорейшего спуска в спектральной норме матрицы. Это достигается ортогонализацией матрицы градиента, фактически устанавливая все её сингулярные значения в ±1.

### 2. Адаптация дисперсии

Адаптация дисперсии - это механизм, аналогичный β₂ в Adam, который нормализует обновления на основе квадратного корня из их исторической нецентрированной дисперсии (экспоненциальное скользящее среднее β₂). Этот механизм работает как адаптивная доверительная область, делая большие шаги, когда направление градиента стабильно (высокое отношение сигнал/шум), и меньшие, когда оно зашумлено (низкое отношение сигнал/шум).

## Ключевые оптимизаторы

### Shampoo

Shampoo - один из первых алгоритмов отбеливания матриц, использующий приближения факторов Кронекера для вычисления предобуславливающих матриц. Он вычисляет статистики ковариации градиентов отдельно для каждого режима тензора весов, что позволяет эффективно учитывать геометрию ландшафта потерь.

### SOAP (Stochastic Optimizer for Adaptive Preconditioning)

SOAP использует адаптивное предобуславливание и показал превосходную производительность, несмотря на менее строгую спектральную нормализацию по сравнению с Muon. Это открытие бросает вызов традиционной теории, акцентирующей внимание на точной спектральной нормализации.

### Muon

Muon использует итерацию Ньютона-Шульца для ортогонализации и достигает соотношения сингулярных значений, очень близкого к идеалу. Однако, удивительно, он не превосходит SOAP, что указывает на важность других компонентов.

## Ключевые находки исследования "What Really Matters in Matrix-Whitening Optimizers?"

### Основной вопрос исследования

Работа Кевина Франса, Пьетра Аббеля и Сергея Левина ставит вопрос: что действительно важно в этих продвинутых оптимизаторах? В ходе тщательного эмпирического анализа были выявлены два ключевых компонента, которые работают по-разному:

1. **Спектральная нормализация**: геометрическая ортогонализация обновлений градиента
2. **Адаптация дисперсии**: статистическое масштабирование обновлений на основе исторической дисперсии

### Ключевые открытия

1. **Адаптация дисперсии является критическим, но недооцененным компонентом**: Версии оптимизаторов с адаптацией дисперсии последовательно и значительно превосходят аналоги с простым подписанным спуском.

2. **Точная спектральная нормализация не является ключом к оптимальной производительности**: SOAP, который выполняет менее строгую нормализацию с соотношением сингулярных значений 2-3 (в отличие от ~1 для Muon), превосходит Muon.

3. **Модульность доказана**: Спектральная нормализация и адаптация дисперсии - два различных, взаимодополняющих компонента, которые можно комбинировать как независимые модули.

4. **Память - решаемая проблема**: Использование низкоранговой, факторизованной аппроксимации позволяет достичь почти идентичной производительности при значительно снижении потребления памяти.

## Практические рекомендации

### Для разработки оптимизаторов будущего:
1. Использовать модульный подход при комбинации спектральной нормализации и адаптации дисперсии
2. Применять низкоранговые факторизации для снижения потребления памяти
3. Концентрироваться на важных компонентах, таких как предобуславливание входов MLP-слоев

### Для практиков:
1. Рассмотреть использование оптимизаторов отбеливания для задач с неудобными ландшафтами потерь
2. Учитывать компромисс между вычислительной сложностью и улучшением сходимости

## Статьи

- [[muon_optimizer.md|Muon]] - Подробное объяснение оптимизатора Muon
- [[soap_optimizer.md|SOAP]] - Подробное объяснение оптимизатора SOAP  
- [[shampoo_optimizer.md|Shampoo]] - Подробное объяснение оптимизатора Shampoo
- [[applications/cuda_l2_ai_gpu_optimization.md]] - Автоматическая оптимизация GPU-ядер с помощью ИИ, альтернативный подход к оптимизации производительности нейронных сетей через оптимизацию базовых операций

## Ссылки на источники

- Frans, K., Abbeel, P., & Levine, S. (2024). What Really Matters in Matrix-Whitening Optimizers? https://arxiv.org/abs/2510.25000
- Gupta, S., et al. (2018). Shampoo: Preconditioned Stochastic Tensor Optimization. http://proceedings.mlr.press/v80/gupta18a.html
- Jordan, K. (2023). Muon Optimizer. https://kellerjordan.github.io/posts/muon
- Zhang, S., et al. (2024). SOAP: Improving and Stabilizing Shampoo. https://arxiv.org/abs/2409.11321