# Память-эффективное обучение (Memory-Efficient Training)

## Краткое описание

Память-эффективное обучение - это подход к обучению нейронных сетей, который направлен на уменьшение использования оперативной памяти при сохранении эффективности обучения. Включает в себя различные методы, такие как градиентный чекпоинтинг, оптимизацию памяти, микро-батчинг и другие техники.

## Основная информация

При обучении больших языковых моделей и других масштабных систем ИИ, ограничения по памяти часто становятся главной проблемой. Память-эффективное обучение предоставляет набор методов и техник, которые позволяют обучать более крупные модели на ограниченных ресурсах.

### Основные проблемы
- **Ограничения GPU-памяти**: Современные модели требуют огромных объёмов памяти
- **Коммуникационные издержки**: В распределённых системах важна эффективность передачи данных
- **Стоимость масштабирования**: Увеличение количества вычислительных узлов не всегда пропорционально увеличивает эффективность

### Решения и методы

1. **Градиентный чекпоинтинг (Gradient Checkpointing)**: Метод, при котором сохраняются только некоторые промежуточные результаты вычислений, а остальные восстанавливаются при необходимости, жертвуя вычислительной эффективностью для экономии памяти.

2. **Микро-батчинг (Micro-batching)**: Разделение больших батчей данных на более мелкие фрагменты для более эффективного использования памяти.

3. **Смесь экспертов (Mixture of Experts - MoE)**: Архитектура, при которой только подмножество параметров модели используется для каждого конкретного примера, что позволяет масштабировать параметры без пропорционального увеличения вычислительных затрат.

4. **Параметрически эффективное обучение**: Методы, при которых обновляется только небольшая часть параметров модели, например, через адаптеры или LoRA (Low-Rank Adaptation).

5. **Распределённое обучение**: Техники, при которых параметры модели распределяются по нескольким узлам (ZeRO - Partitioning Optimizer States).

## Новые концепции и термины

- **ZeRO (Partitioning Optimizer States)**: Метод, при котором состояния оптимизатора (градиенты, моменты) распределяются по нескольким GPU для уменьшения памяти на каждом узле.

- **LoRA (Low-Rank Adaptation)**: Техника параметрически эффективного обучения, при которой обучаемые адаптеры имеют низкий ранг, что значительно уменьшает количество параметров, требующих обновления.

- **Switch Transformers**: Архитектура, использующая механизм "смесь экспертов" в трансформерных моделях, где для каждого входного токена активируется только один "эксперт".

- **Activation Offloading**: Техника, при которой активации выгружаются из GPU-памяти и восстанавливаются при необходимости обратного распространения.

## Примеры применения

- **Обучение крупных языковых моделей**: Обучение GPT, BERT и других моделей на ограниченных вычислительных ресурсах
- **Обучение в edge устройствах**: Адаптация моделей к устройствам с ограниченной памятью
- **Экономия на вычислительных затратах**: Уменьшение количества необходимых GPU для обучения

## Связи с другими темами

- [[ai/llm/intrinsic_dimensionality.md]] - Концепция внутренней размерности, объясняющая, почему параметрически эффективные методы работают так хорошо
- [[ai/llm/evolution_strategies_optimization.md]] - Применение эволюционных стратегий к оптимизации LLM как альтернативный подход к память-эффективному обучению
- [[ai/tools/pytorch_monarch.md]] - Фреймворк, реализующий многие техники память-эффективного обучения
- [[ai/machine_learning/machine_learning.md]] - Общие концепции машинного обучения
- [[ai/llm/models/qwen/qwen-vl-series.md]] - Пример моделей, для обучения которых требуются эффективные методы
- [[ai/nlp/transformers/transformer_architecture.md]] - Архитектура трансформеров, часто требующая память-эффективных методов

## Ссылки на источники

- NVIDIA ZeRO paper: https://arxiv.org/abs/1910.02054
- LoRA paper: https://arxiv.org/abs/2106.09685
- Switch Transformers paper: https://arxiv.org/abs/2101.03961