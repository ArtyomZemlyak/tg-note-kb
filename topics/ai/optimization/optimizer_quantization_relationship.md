# Взаимосвязь между оптимизаторами и квантованием: Исследование "Beyond Outliers"

## Общее описание

Статья "Beyond Outliers: A Study of Optimizers Under Quantization" систематически изучает взаимосвязь между выбором оптимизатора во время обучения и сложностью квантования полученных моделей. Основной вопрос исследования: делают ли некоторые алгоритмы оптимизации модели более податливыми к сжатию по сравнению с другими?

## Методы исследования

Исследование сравнивает 6 различных оптимизаторов:
- **AdamW** - стандартный адаптивный оптимизатор
- **PSGD** (Preconditioned Stochastic Gradient Descent)
- **Shampoo** - оптимизатор второго порядка с предобуславливанием на основе ковариации градиентов
- **Muon** - оптимизатор с ортогонализацией Ньютона-Шульца
- **Scion** - менее известный оптимизатор
- **SOAP** (Stochastic Optimizer for Adaptive Preconditioning) - улучшенная и стабилизированная версия Shampoo

Обучались трансформерные модели OLMo2-подобной архитектуры от 50M до 1.5B параметров с оптимальным количеством токенов по Шиншилле. Learning rate подбирался на меньшей модели и масштабировался на большие как 1/размер.

## Квантование и методы оценки

Исследование включает:
- **FP16 бейзлайн**
- **4-битное квантование** с квантованием весов и активаций
- **PTQ (Post-Training Quantization)** - квантование fp16 модели
- **QAT (Quantization-Aware Training)** - обучение с учетом квантования (использовался рецепт из QuEST с Адамаровыми вращениями и стохастическим округлением)

## Метрики и результаты

### Традиционные метрики
Ранее для оценки сложности квантования использовались:
- **MMR (Max-to-Median Ratio)** - отношение максимума по модулю к медиане
- **Kurtosis (4-й момент распределения)** - показатель "тяжесть" хвостов распределения

### Неожиданные результаты
Исследование показывает, что **эти традиционные метрики не коррелируют с реальной просадкой качества при квантовании**. Shampoo имеет большие значения MMR и kurtosis, но модели, обученные им, легче всего поддаются квантованию.

### Новая метрика
Исследователи предложили новую метрику: **относительная послойная ошибка** (квадрат нормы разности неквантованной активации и квантованной, деленный на квадрат нормы первой). Эта метрика показывает гораздо лучшую корреляцию с результатами на бенчмарках.

## Результаты по оптимизаторам

### Без квантования
- **SOAP** выигрывает на меньших моделях
- **Muon** выигрывает на остальных

### PTQ (Post-Training Quantization)
- **Shampoo** выигрывает в большинстве случаев

### QAT (Quantization-Aware Training)
- Нет однозначного победителя
- На больших моделях Shampoo снова показывает предпочтительные результаты

### MMR по оптимизаторам
- **Muon** имеет наименьший MMR
- MMR растет с повышением learning rate

## Масштабирование и эффективность

Для разных моделей фитировали коэффициент эффективного размера ρ в законе масштабирования:
```
L = A / (N · ρ)^α + B
```
Результаты:
- **Shampoo** имеет наибольшее значение ρ (самый эффективный)
- **Muon** имеет наименьшее значение ρ (наименее эффективный)
- **Adam** на втором месте после Shampoo

## Практические выводы

1. **Предпочтительные оптимизаторы для квантования**: Shampoo показывает лучшие результаты для последующего квантования, несмотря на высокие значения традиционных "outlier" метрик

2. **Пересмотр метрик качества квантования**: Традиционные метрики MMR и kurtosis не предсказывают реальное качество квантования, что требует пересмотра подходов к оценке сложности квантования

3. **Значение новой метрики**: Относительная послойная ошибка может служить лучшим предиктором успешного квантования

4. **Влияние learning rate**: MMR увеличивается с ростом learning rate, что означает большие отклонения весов/активаций от среднего

![Результаты сравнения оптимизаторов при квантовании](../../../media/img_1763644633_aqadhrbrg6f36eh_94_dd_jouc_o197_sav_95_0.jpg)

**Описание:** Изображение содержит визуализацию результатов сравнения оптимизаторов при квантовании, показывающее эффективность различных оптимизаторов (Shampoo, Muon, SOAP, AdamW и др.) при разных методах квантования (PTQ и QAT) и различных уровнях битности.

## Связь с другими темами

- [[shampoo_optimizer.md]] - Подробное описание оптимизатора Shampoo, который показал лучшие результаты при квантовании
- [[muon_optimizer.md]] - Подробное описание оптимизатора Muon, который имеет наименьший MMR
- [[soap_optimizer.md]] - Подробное описание оптимизатора SOAP, победителя среди оптимизаторов без квантования
- [[matrix_whitening_optimizers.md]] - Контекст матричных отбеливающих оптимизаторов (включая Shampoo и SOAP)
- [[model_quantization_techniques.md]] - Общие методы квантования моделей
- [[qat_modern_methods.md]] - Современные методы квантования-ориентированного обучения
- [[int_vs_fp_quantization_comparison.md]] - Сравнение различных форматов квантования

## Ссылки на источники

1. [Beyond Outliers: A Study of Optimizers Under Quantization](https://arxiv.org/abs/2509.23500) - основная статья, исследующая взаимосвязь между оптимизаторами и квантованием
2. Исследования по различным оптимизаторам второго порядка и их влиянию на обучение и квантование моделей
3. Эксперименты с OLMo2-подобными трансформерами от 50M до 1.5B параметров
4. Сравнительный анализ PTQ и QAT методов с разными оптимизаторами