# Современные методы квантования-ориентированного обучения (QAT)

## Краткое описание

Квантования-ориентированное обучение (Quantization-Aware Training, QAT) - это подход, при котором модель обучается с учетом эффектов квантования, что позволяет сохранить точность при использовании весов и активаций с низкой точностью. В последние годы появилось множество методов, улучшающих QAT, включая CAGE, QuEST и другие.

## Основная информация

### Общие принципы QAT

QAT решает проблему потери точности при квантовании нейронных сетей, позволяя модели "адаптироваться" к эффектам квантования во время обучения. В отличие от пост-квантования (post-training quantization), QAT включает эффекты квантования в процесс обучения, что позволяет точнее моделировать поведение модели после квантования.

### Традиционные подходы

Классические методы QAT используют STE (Straight Through Estimator) для вычисления градиентов через недифференцируемые операции квантования. STE просто "проталкивает" градиент через квантование, не учитывая ошибки квантования.

### Современные методы

#### QuEST (Quantized STE Training)

QuEST - метод, который позволяет стабильно обучать LLM с весами и активациями в 4-бит или даже 1-битной точности. Он улучшает два ключевых аспекта QAT:

1. **Точная и быстрая квантация**: Использует Hadamard нормализацию и MSE-оптимальное приближение для квантования распределений весов и активаций
2. **Надежная оценка градиента**: Основана на явной минимизации ошибки между шумным градиентом, вычисленным над квантованными состояниями, и "истинным" (но неизвестным) градиентом полной точности

QuEST достигает стабильного обучения 1-битных LLM и делает 4-битное обучение оптимальным по Парето (лучшее качество при меньшем размере модели).

#### CAGE (Curvature-Aware Gradient Estimation)

CAGE представляет собой новаторский подход к QAT, который улучшает традиционный STE градиент с кривизна-ориентированной коррекцией, направленной на компенсацию увеличенной потери, вызванной квантованием.

##### Математический подход

- CAGE рассматривает QAT как задачу оптимизации с ограничениями
- Переход к задаче безусловной оптимизации с множителем Лагранжа: `min L(w) = min L_orig (w) + λ (Q(x) - x)`
- Величина λ определяет баланс между лоссом задачи и ограничением
- Такая добавка математически эквивалентна добавлению error feedback
- Основывается на AdamW, отличаясь только наличием error feedback
- Константа регуляризации λ разогревается от 0 до максимального значения для лучшей сходимости

##### Ключевые особенности

- Улучшает точность QAT по сравнению с предыдущими методами
- Имеет более теоретически обоснованный подход через метод множителей Лагранжа
- Показывает стабильное улучшение по лоссу по сравнению с базовым алгоритмом QuEST для разных битностей
- Успешно работает с MXFP4 и другими низкобитными форматами

#### Другие современные подходы

- **Hadamard нормализация**: Преобразования, делающие распределения весов/активаций более гауссовыми, что улучшает эффективность форматов квантования
- **Error feedback методы**: Добавляют компенсацию ошибки квантования в процесс обучения
- **Методы кривизны**: Учитывают локальные свойства ландшафта потерь при вычислении градиентов

## Сравнение методов

| Метод | Битность | Эффективность | Теоретическая основа | Область применения |
|-------|----------|---------------|---------------------|-------------------|
| STE | 8+ | Средняя | Эвристическая | Классические задачи |
| QuEST | 4-1 бит | Высокая | Улучшенная | Экстремальная квантация |
| CAGE | 4+ бит | Высочайшая | Обоснованная (Лагранж) | Высокоточная квантация |

## Новые концепции и термины

- **QAT (Quantization-Aware Training)**: Квантования-ориентированное обучение
- **STE (Straight Through Estimator)**: Метод вычисления градиентов через недифференцируемые операции
- **Error Feedback**: Механизм добавления компенсации ошибки квантования в процесс обучения
- **Hadamard Normalization**: Нормализация, делающая распределения более гауссовыми
- **Curvature-Aware**: Учет кривизны ландшафта потерь при обучении

## Примеры применения

- Обучение LLM с низкой точностью для сокращения вычислительных затрат
- Обучение на устройствах с ограниченной памятью
- Сценарии, где важна энергоэффективность и скорость инференса

## Связи с другими темами

- [[cage_method.md]] - Подробное описание метода CAGE
- [[../../llm/model_quantization_techniques.md]] - Общие методы квантования моделей
- [[ste_quantization_method.md]] - Подробное описание метода STE
- [[quest_quantization_method.md]] - Подробное описание метода QuEST
- [[../../optimization/quantization/int_vs_fp_quantization_comparison.md]] - Сравнение INT и FP форматов квантования
- [[../../llm/tools/moe_quant.md]] - Другие методы квантования для LLM
- [[../../optimization/deep_optimizers.md]] - Контекст современных оптимизаторов
- [[../../llm/llm_training_optimization.md]] - Оптимизация обучения LLM

## Источники

1. [CAGE: CURVATURE-AWARE GRADIENT ESTIMATION FOR ACCURATE QUANTIZATION-AWARE TRAINING](https://arxiv.org/abs/2510.18784) - Оригинальная статья о методе CAGE
2. [QuEST: Stable Training of LLMs with 1-Bit Weights and Activations](https://arxiv.org/abs/2502.05003) - Описание метода QuEST
3. [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2306.06965) - Исследование форматов квантования
4. Различные исследования по квантования-ориентированному обучению и современным методам сжатия нейронных сетей

## Дополнительные материалы

- В исходной статье о CAGE упоминается изображение (img_1763014243_AgACAgIA.jpg), которое, предположительно, содержит визуализацию экспериментальных результатов CAGE или сравнение с другими методами, но точное содержание неизвестно