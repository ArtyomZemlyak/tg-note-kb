# Сравнение INT и FP форматов квантования: Исчерпывающее исследование тонких различий низкобитных форматов квантования

## Краткое описание

Статья рассматривает сравнение целочисленных (INT) и чисел с плавающей запятой (FP) форматов квантования в условиях низкой точности, что становится важным для удешевления обучения и инференса в нейронных сетях. Исследование проводит детальный анализ различных форматов квантования (MXFP8/MXINT8, MXFP6/MXINT6, MXFP4/MXINT4, NVFP4/NVINT4) и их сравнительной эффективности при различных условиях.

## Основная информация

### Введение

С ростом потребности в удешевлении обучения и инференса все острее встает вопрос об обучении в более низкой точности. Компания NVIDIA внедрила аппаратную поддержку FP8, а затем и FP4. Однако вопрос остается: почему FP, а не INT? NVIDIA не обосновывала данный выбор в своих статьях и блогах, из-за чего мотивация остается тайной.

Исследовательская группа решила сравнить INT и FP в сопоставимых условиях.

### Методы исследования

В статье рассматриваются 4 пары конфигураций квантования:
- MXFP8/MXINT8
- MXFP6/MXINT6  
- MXFP4/MXINT4
- NVFP4/NVINT4

MX-варианты квантуются группами по 32 веса с E8M0 скейлами, а NV группами по 16 весов с E4M3 скейлами. Используется как FP- так и INT-сетка значений, соответственно.

### Теоретические основы

Исходя из предположения о гауссовости распределений, выводятся формулы для ошибки квантования (а точнее SNR - соотношения сигнал-шум). Из этих формул следует, что:
- При большом значении отношения максимального элемента к стандартному отклонению (при per-tensor квантации или больших группах) предпочтительнее FP форматы
- При малых группах предпочтительнее INT форматы

### Эксперименты и результаты

1. **Анализ весов и активаций**: 
   - Эмпирически проверяются полученные закономерности на весах/активациях и градиентах Llama-3.1-8B
   - Без вращений FP форматы лучше, за исключением случая NVINT против NVFP
   - C вращениями: MXINT8 лучше MXFP8, MXINT6 хуже MXFP6, MXINT4 хуже MXFP4, NVINT4 лучше NVFP4

2. **KL-дивергенция**: 
   - Исследуется KL-дивергенция с исходной моделью, так как бенчмарки признаются шумными и нерепрезентативными
   - Без вращений FP обычно лучше, с вращениями - наоборот

3. **Обучение моделей**:
   - Обучаются 1B и 3B модельки на 100B/200B токенах
   - MXFP8/MXINT8 показывают сравнимые результаты с исходной моделью по лоссу и бенчмаркам
   - Лосс для MXINT немного ниже

### Выводы

1. **Предпочтения при малых группах**: При маленьких группах квантования FP форматы не являются выигрышным вариантом, в то время как INT форматы более предпочтительны.

2. **Энергоэффективность**: INT форматы предпочтительнее, потому что потребляют меньше энергии.

3. **Критика архитектур NVIDIA**: Результаты частично показывают, что NVIDIA могла зря отказаться от поддержки INT в новых архитектурах в угоду маркетингу.

4. **Адамаровы вращения**: Показано, что Адамаровы вращения могут сделать веса/активации более гауссовыми, что влияет на эффективность форматов.

## Новые концепции и термины

- **MXFP форматы**: Форматы квантования с плавающей запятой (MXFP8, MXFP6, MXFP4) с группами по 32 веса и E8M0 скейлами
- **MXINT форматы**: Целочисленные форматы (MXINT8, MXINT6, MXINT4) с теми же параметрами группировки
- **NVFP/NVINT форматы**: Форматы квантования NVIDIA с группами по 16 весов и E4M3 скейлами
- **Адамаровы вращения**: Преобразования, делающие распределения весов/активаций более гауссовыми
- **SNR (Signal-to-Noise Ratio)**: Соотношение сигнал-шум, используемое для оценки ошибки квантования

## Примеры применения

- Выбор оптимального формата квантования в зависимости от размера группы квантования
- Принятие решения о формате квантования на основе энергоэффективности
- Использование Адамаровых вращений для улучшения характеристик квантования

## Связи с другими темами

- [[../../llm/model_quantization_techniques.md]] - Общие методы квантования моделей
- [[../../llm/tools/sglang.md]] - Поддержка различных форматов квантования (FP4/FP8/INT4)
- [[../algorithms/distribution_fitting.md]] - Приближение распределений, связанное с предположением о гауссовости
- [[../../hardware/index.md]] - Аппаратные аспекты квантования
- [[../../llm/inference/vllm_integration.md]] - Поддержка FP8 квантования в vLLM
- [[../../llm/mixture_of_experts_architecture.md]] - Архитектура с MXFP8 MoE ядрами
- [[../../../tools/cursor_composer.md]] - Нативное обучение в низкой точности с использованием MXFP8 MoE ядер

## Ссылки на источники

- Статья: "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats"
- Исследования по различным форматам квантования
- Документация по квантующим архитектурам NVIDIA и другим производителям