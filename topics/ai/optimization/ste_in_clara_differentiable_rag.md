# Straight-Through Estimator (STE) в дифференцируемом RAG (CLaRa)

## Обзор

Straight-Through Estimator (STE) - это ключевая техника, использованная в архитектуре CLaRa (Continuous Latent Reasoning) для решения проблемы недифференцируемости операции Top-K в RAG-системах. STE позволяет градиентам от языковой модели протекать обратно к механизму извлечения, обеспечивая совместную оптимизацию извлечения и генерации.

## Проблема "разрыва градиента" в традиционном RAG

Традиционные RAG-системы страдают от "разрыва градиента", так как:

1. Ретривер и генератор оптимизируются отдельно
2. Ретривер ищет документы по семантической близости (косинусное сходство)
3. Языковая модель обучается предсказывать следующий токен
4. Эти цели часто не совпадают, и модель получает семантически близкие, но фактически бесполезные куски текста

## Решение в CLaRa через STE

CLaRa решает проблему "разрыва градиента" с помощью техники Straight-Through Estimator (STE), которая:

1. Делает шаг поиска дифференцируемым
2. Объединяет всё в одном латентном пространстве
3. Позволяет обучить Query Reasoner предсказывать, какая латентная информация нужна генератору

### Формулировка STE в CLaRa

Тензор выбора Z определяется как:
Z = Z_hard + (Z_soft - SG(Z_soft))

Где:
- Z_hard - one-hot матрица выбора (жёсткий выбор документов)
- Z_soft - софтмакс по скорам схожести с температурой τ (мягкий выбор)
- SG - оператор остановки градиента (Stop-Gradient)

### Процесс обучения с STE

1. **Прямой проход (forward pass)**: система считает скоры схожести s_i = cos(q, M_i) и делает жёсткий выбор (hard selection) топ-документов M^(k)
2. **Обратный проход (backward pass)**: система аппроксимирует градиент через мягкую релаксацию
3. **Обновление параметров**: градиенты от лосса генерации протекают через Z_soft и обновляют скоры схожести s_i
4. **Обучение Query Reasoner**: получает прямой сигнал о полезности извлеченных документов

## Применение STE в контексте CLaRa

### В отличие от традиционных QAT-методов

Хотя STE изначально использовался в квантовании нейронных сетей (QAT), в CLaRa он применяется по-другому:

- В QAT: STE пробрасывает градиенты через недифференцируемую операцию квантования
- В CLaRa: STE делает недифференцируемую операцию Top-K дифференцируемой

### Архитектурные преимущества

1. **Единое латентное пространство**: Ретривер и генератор работают в одном и том же пространстве
2. **Совместная оптимизация**: Оба компонента могут быть обучены одновременно
3. **"Скрытое рассуждение"**: Query Reasoner учится генерировать эмбеддинги, содержащие информацию не из запроса, а из целевого документа

## Технические детали

### Объединение поиска и генерации

В CLaRa:
- Вместо сырого текста система извлекает сжатые "токены памяти" - векторные представления фрагментов документов
- Query Reasoner - это набор LoRA-адаптеров на базовой LLM, который обрабатывает запрос q и выдает последовательность эмбеддингов запроса
- Эти эмбеддинги живут в том же многообразии, что и токены памяти документов

### Функция потерь

Финальная функция потерь объединяет генерацию с неявным выравниванием поиска:
L_CLaRa(θ_qr, θ_g) = - sum(log p_θ_g(a*_t | Q, M_(1:k), a*_(<t)))

Где:
- θ_qr - параметры Query Reasoner
- θ_g - параметры генератора
- Q - запрос
- M_(1:k) - топ-k извлеченных токенов памяти
- a*_t - правильный ответ на шаге t

## Результаты и эффективность

### Количественные результаты
- CLaRa с коэффициентом сжатия 4x достигает Recall @--- 96.21% на HotpotQA, обгоняя полностью обученный BGE-Reranker (85.93%) более чем на 10 пунктов
- Даже при экстремальном сжатии (16x) CLaRa-Mistral-7B сравнивается или превосходит текстовые бейзлайны

### Качественные улучшения
- Query Reasoner генерирует токены эмбеддингов, которые декодируются в слова, существующие в целевом документе, но отсутствующие в запросе
- Это показывает, что end-to-end оптимизация превратила ретривер из "сопоставителя по смыслу" в ассоциативный рассуждающий модуль

## Связи с другими темами
- [[../rag/apple_clara_continuous_latent_reasoning.md]] - Основная статья о CLaRa, архитектуре которой способствует применение STE
- [[qat_modern_methods.md]] - Методы квантования нейронных сетей, где впервые получил широкое применение STE
- [[differentiable_architectures.md]] - Дифференцируемые архитектуры нейронных сетей
- [[attention_sink_theory.md]] - Теория "attention sinks", связанная с фокусировкой внимания в латентном пространстве

## Источники
1. [CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning](https://arxiv.org/abs/2511.18659) - Оригинальная статья, описывающая применение STE в контексте дифференцируемого RAG
2. [Apple CLaRa GitHub Repository](https://github.com/apple/ml-clara) - Исходный код и реализация CLaRa
3. [Hugging Face Model Card: CLaRa-7B-Instruct](https://huggingface.co/apple/CLaRa-7B-Instruct) - Модельный карточка с деталями реализации STE