# Apple CLaRa: Continuous Latent Reasoning в RAG-системах

## Обзор

CLaRa (Continuous Latent Reasoning) - это новая модель, разработанная Apple для RAG (Retrieval-Augmented Generation) с интегрированным сжатием документов. Основной особенностью CLaRa является то, что вместо помещения длинных документов в контекст, модель сжимает их в 16-128 раз и генерирует ответы напрямую из сжатого представления.

## Архитектура

CLaRa использует **трехэтапный подход к обучению** с непрерывным латентным рассуждением:

### Этап 1: Предобучение сжатия (Compression Pretraining) - SCP
- Использует фреймворк SCP (Salient Compressor Pretraining)
- Обучает сжиматель с использованием QA-пар и парафразов
- Сохраняет ключевые семантики через QA- и парафраз-ориентированное обучение
- Поддерживает степени сжатия от 1x до 256x
- Для обучения сжатия используются синтетические данные, сгенерированные учителем (Qwen-32B), включая Simple QA, Complex QA и перефразирование
- Для улучшения качества сжатия применяется дополнительная MSE потеря: L_total = L_CE + λ * L_MSE

### Этап 2: Инструкционное тюнингование сжатия (Compression Instruction Tuning)
- Тонко настраивает сжиматель для задач инструкций по QA
- Использует текстовый QA-вывод для обеспечения того, чтобы сжатые представления содержали достаточную семантику

### Этап 3: Сквозная настройка (End-to-End Fine-tuning)
- Совместно обучает реранжировщик (Query Reasoner) и генератор через единую функцию потерь моделирования языка
- Объединяет извлечение и генерацию в общем непрерывном пространстве с использованием дифференцируемого оценщика top-k
- Построена на основе фреймворка OpenRLHF
- Эмбеддинги документов фиксируются после SCP, обучение фокусируется на Query Reasoner и генераторе
- Функция потерь: L_CLaRa(θ_qr, θ_g) = - sum(log p_θ_g(a*_t | Q, M_(1:k), a*_(<t)))

## Отличия от традиционных RAG-подходов

### Проблемы, которые решает CLaRa
- **Длинные контексты**: Традиционные RAG-модели сталкиваются с длинными контекстами, что увеличивает вычислительные издержки
- **Отдельная оптимизация извлечения-генерации**: Традиционные подходы имеют раздельную оптимизацию для извлечения и генерации
- **Избыточное кодирование**: Фреймворки с мягким сжатием требуют двойного кодирования, несмотря на то, что сжатые векторы поддаются извлечению
- **Смещение к поверхностным паттернам**: Целевые функции, основанные на реконструкции, смещают сжиматели в сторону поверхностных паттернов, а не семантического сохранения
- **"Разрыв градиента"**: Архитектура современного RAG фундаментально расколота - ретривер ищет по косинусному сходству, а LLM обучается предсказывать следующий токен, цели часто не совпадают

### Решения CLaRa
- **Единое непрерывное пространство**: Использует дифференцируемую оценку top-k для объединения извлечения и генерации
- **Совместная оптимизация**: Сквозное обучение оптимизирует как извлечение, так и генерацию одновременно
- **Семантическое сохранение**: Помощь QA- и парафраз-ориентированного обучения сохраняет семантическую информацию, а не только поверхностные паттерны
- **Отсутствие избыточного кодирования**: Сжатые векторы сохраняют извлекаемость без двойного кодирования
- **Преодоление "разрыва градиента"**: Используется техника Straight-Through Estimator (STE) для проброса градиентов от функции потерь языковой модели обратно в механизм поиска

## Механизм сжатия

- **Степени сжатия**: Достигает значительных степеней сжатия 32x-64x, при этом сохраняя необходимую информацию
- **Фреймворк SCP (Salient Compressor Pretraining)**: Использует QA-пары и парафразы для сохранения семантического содержания
- **MSE + QA Loss**: Комбинирует функцию потерь MSE для соответствия с исходными представлениями и QA-потерю для семантического сохранения
- **Непрерывное латентное пространство**: Поддерживает сжатую информацию в непрерывном пространстве, которое поддерживает как извлечение, так и генерацию
- **"Токены памяти"**: Вместо сырого текста система извлекает сжатые "токены памяти" - векторные представления фрагментов документов, которые можно спроецировать обратно в слои трансформера LLM
- **Компрессор**: Использует замороженную LLM с LoRA-адаптерами, добавляет l обучаемых токенов к входу; финальные скрытые состояния этих токенов становятся сжатым представлением документа M_i

## Реализация и использование

### Доступные модели
- **CLaRa-7B-Instruct**: Инструкционно-настроенная унифицированная RAG-модель на базе Mistral-7B-Instruct-v0.2
- **Степени сжатия**: Поддерживает встроенное семантическое сжатие документов 16× и 128×

### Пример использования
```python
from transformers import AutoModel

unirag = AutoModel.from_pretrained(
    "/mnt/ceph_rbd/model/CLaRa-7B-Instruct/compression-16",
    trust_remote_code=True
).to("cuda")

documents = [
    [
        "Weldenia is a monotypic genus of flowering plant in the family Commelinaceae...",
        "Hagsatera is a genus of flowering plants from the orchid family...",
        "Alsobia is a genus of flowering plants in the family Gesneriaceae..."
    ]
]

questions = [
    "Which genus of plant grows originally in Mexico and Guatemala, Phylica or Weldenia?"
]

# Инструкционно-настроенное использование
out = unirag.generate_from_text(
    questions=questions,
    documents=documents,
    max_new_tokens=64
)

print("Сгенерированный ответ:", out)
```

### Ключевые детали реализации
1. **Способность к сжатию**: Модель включает встроенное семантическое сжатие документов на двух уровнях (16× и 128×)
2. **Интеграция RAG**: Унифицированная модель, объединяющая извлечение и генерацию с непрерывным латентным рассуждением
3. **Инструкционное тюнингование**: Специально обучена для выполнения инструкций и задач QA
4. **Загрузка модели**: Требует `trust_remote_code=True` при загрузке
5. **Метод**: Использует метод `generate_from_text()`, который принимает как вопросы, так и документы в качестве входных данных

## Производительность

### Результаты сжатия (Mistral-7B, нормальные настройки):
- **Конкурентоспособные результаты**: CLaRa (CR=4) достигает 39.86% средней производительности на 4 наборах данных (NQ, HotpotQA, MuSiQue, 2WikiMultiHopQA)
- **Превосходит базовые линии**: Превышает PISCO на +1.13% в нормальных настройках и +5.35% в oracle настройках в среднем
- **Лучше, чем LLMLingua-2**: Превосходит на +5.37% средний прирост в нормальных настройках
- **Сопоставимо с текстовыми базовыми линиями**: Достигает +2.36% среднего прироста на Mistral-7B
- **Количественно CLaRa тоже хороша**: На HotpotQA с коэффициентом сжатия 4x она достигает Recall @--- 96.21%, обгоняя полностью обученный BGE-Reranker (85.93%) более чем на 10 пунктов. Даже при экстремальном сжатии (16x), CLaRa-Mistral-7B сравнивается или превосходит текстовые бейзлайны типа DRO и In-Context RAG

## Query Reasoner и STE механизм

### Query Reasoner (θ_qr)
- Представляет собой набор LoRA-адаптеров на базовой LLM, в отличие от обычных ретриверов, которые сплющивают запрос q в статический вектор
- Обрабатывает q и выдаёт последовательность эмбеддингов запроса, живущих в том же многообразии, что и токены памяти документов
- Использование общего бэкбона (например, Mistral-7B) для компрессии, кодирования запроса и генерации естественным образом выравнивает векторное пространство
- Обучается не просто искать пересечения слов, а активно предсказывать, какая латентная информация нужна генератору

### Straight-Through Estimator (STE)
- Архитектурное ядро CLaRa - обход недифференцируемости операции Top-K
- На прямом проходе система считает скоры схожести s_i = cos(q, M_i) и делает жёсткий выбор (hard selection) топ-документов M^(k)
- На обратном проходе система аппроксимирует градиент через мягкую релаксацию
- Тензор выбора Z определяется как: Z = Z_hard + (Z_soft - SG(Z_soft)), где Z_hard - one-hot матрица выбора, Z_soft - софтмакс по скорам схожести с температурой τ, а SG - оператор остановки градиента (Stop-Gradient)
- Позволяет градиентам от лосса генерации протекать через Z_soft и обновлять скоры схожести s_i
- Query Reasoner получает прямой сигнал: если выбранный документ не помог предсказывать правильный ответ, градиент наказывает скор схожести между этим запросом и документом
- Ретривер учится приоритезировать "полезное", а не просто "похожее"

### "Скрытое рассуждение"
- CLaRa не требует разметки релевантности для ретривера - ретривер обучается через слабый надзор исключительно на том, насколько успешно генератор отвечает на вопросы
- Эффективность метода подтверждена через детальные абляции, используя технику Logit Lens
- Для запроса про "Ivory Lee Brown" Query Reasoner генерирует токены эмбеддингов, которые декодируются в слова "NFL" и "Oklahoma" - термины, которые есть в целевом документе, но отсутствуют в самом запросе
- Это показывает, что end-to-end оптимизация превратила ретривер из "сопоставителя по смыслу" в ассоциативный рассуждающий модуль
- Query Reasoner буквально галлюцинирует (в хорошем смысле) полезный контекст в латентном пространстве, чтобы максимизировать точность поиска

### Ключевые метрики производительности:
- **Эффективное сжатие**: 32x-64x сжатие при сохранении точности
- **Производительность на наборах данных**: Оценена на NQ, HotpotQA, MuSiQue и 2WikiMultiHopQA
- **Oracle vs Normal**: Протестирована как в oracle настройке (золотой документ включен), так и в нормальной настройке (топ-5 извлечённых документов)

## Поддерживаемые наборы данных
- HotpotQA: Ответы на вопросы с несколькими шагами (multi-hop)
- MuSiQue: Ответы на вопросы с несколькими шагами с разнообразным рассуждением
- 2WikiMultiHopQA: Ответы на вопросы с несколькими шагами по Википедии
- Natural Questions: Ответы на вопросы в открытом домене

## Ограничения и вызовы

### Ограничения подхода
- **Замороженный компрессор**: Поскольку представления документов фиксируются после первого этапа, любая информация, потерянная при начальном сжатии (SCP), утрачивается безвозвратно. Никакой умный Query Reasoner её уже не вернёт
- **Зависимость от качества синтетических данных**: Текущий компрессор также предобучен только на Википедии, что может ограничить обобщающую способность на специфических доменах вроде кода или юридических текстов
- **Зависимость от качества синтетических данных для SCP**: Если учитель (Qwen-32B) сгенерирует плохие пары "вопрос-ответ", способность компрессора сохранять важные детали деградирует

### Архитектурная значимость
- **Стратегически важна для Edge AI**: Архитектура важна для Edge AI (запуск на устройствах), так как она драматически снижает использование контекстного окна, сохраняя возможности рассуждения
- **Реальный путь уйти от дорогой парадигмы**: Это реальный путь уйти от дорогой парадигмы "запихнуть кучу сырого текста в гигантское контекстное окно"
- **Размывается граница между внешней памятью и внутренними параметрическими знаниями**: CLaRa - это серьёзный шаг к Implicit RAG, где граница между внешней памятью и внутренними параметрическими знаниями размывается

## Визуализации

![Обзор SCP фреймворка](../../../media/img_1764813556_aqadrg1rg9j4iul_figure_2_overview_of_the_scp.jpg)

**Рисунок показывает:** Обзор фреймворка SCP (Salient Compressor Pretraining), включающий (a) построение синтетических данных для предобучения и (2) обучение сжатия с использованием данных предобучения.

![Архитектура CLaRa и скрытое рассуждение](../../../media/img_1764813556_aqadrq1rg9j4iul_image_figure_1_a.jpg)

**Рисунок показывает:** (a) Во время обучения сначала предобучается сжиматель, чтобы поощрить его сохранять только важную информацию. Затем выполняется автономное сжатие документов. После этого запрос кодируется с помощью Query Reasoner, извлекаются сжатые представления документов для генерации, и используется только потеря предсказания следующего токена для совместного обновления Query Reasoner и генератора. (b) Пример стадии вывода: токены представляют ключевые слова, связанные с вопросом. При декодировании непрерывного встраивания запроса мы обнаруживаем, что оно содержит информацию, отсутствующую в исходном запросе, что указывает на то, что оно изучило некоторые ключевые слова для промежуточного вывода.

![Анализ токенов, декодированных Query Reasoner](../../../media/img_1764813556_aqadsq1rg9j4iul_image_table1_analysis_of.jpg)

**Рисунок показывает:** Таблица анализа декодированных токенов из Query Reasoner через Logit Lens. Выделенные токены (красным) обозначают новую информацию, выведенную Query Reasoner, в то время как (синим) обозначены ключевые улики для решения задачи с несколькими шагами.

![Производительность сжимателя на четырех наборах данных](../../../media/img_1764813556_aqadsg1rg9j4iul_table_2_compressor_performance_on_four.jpg)

**Рисунок показывает:** Таблица производительности сжимателя на четырех наборах данных QA. Лучшая производительность выделена жирным. Мы показываем абсолютное изменение производительности (--) нашего метода при разных степенях сжатия по сравнению с соответствующей лучшей эталонной производительностью. CR обозначает степень сжатия.

![Производительность CLaRa при энд-ту-энд QA](../../../media/img_1764813556_aqadsw1rg9j4iul_table_3_end_to_end_qa_performance.jpg)

**Рисунок показывает:** Таблица 3 - Производительность энд-ту-энд QA. * указывает результаты, сообщенные из статьи DRO. CR обозначает степень сжатия. Наилучшие баллы показаны жирным шрифтом, а вторые по величине - подчеркнуты. В целом, наш метод достигает сопоставимой производительности при снижении требуемой длины контекста в 16 раз.

![Обучение CLaRa энд-ту-энд](../../../media/img_1764813556_aqadsa1rg9j4iul_figure3_clara_end_to_end_training_update.jpg)

**Рисунок показывает:** Обучение CLaRa энд-ту-энд: обновление Query Reasoner и генератора через потерю моделирования языка с использованием кандидатских троек документ-вопрос-ответ.

## Связи с другими темами
- [[../llm/reasoning/latent_variables_reasoning.md]] - Использование латентных переменных для внутреннего формирования рассуждений, альтернатива явной цепочке рассуждений
- [[document_compression_techniques.md]] - Другие методы сжатия документов в RAG-системах
- [[../optimization/compress_to_impress_single_gradient_llm_adaptation.md]] - Метод Compress to Impress, связанный с градиентами сингулярных значений
- [[retrieval_strategies.md]] - Стратегии извлечения информации в RAG-системах
- [[../../ai/optimization/ste_in_clara_differentiable_rag.md]] - Подробное описание механизма Straight-Through Estimator (STE), примененного в CLaRa
- [[../../ai/llm/reasoning/hidden_reasoning_in_clara.md]] - Феномен "скрытых рассуждений" в CLaRa, когда Query Reasoner генерирует полезные токены, отсутствующие в запросе

## Источники
1. [Apple CLaRa GitHub Repository](https://github.com/apple/ml-clara) - Официальный репозиторий с исходным кодом и технической документацией
2. [Apple CLaRa Hugging Face Model](https://huggingface.co/apple/CLaRa-7B-Instruct) - Модельный карточка с примерами использования
3. [CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning](https://arxiv.org/abs/2511.18659) - Оригинальная научная статья, описывающая архитектуру CLaRa
4. [Unifying Retrieval and Generation, Reasoning-Driven Cold Start](https://recsys.substack.com/p/unifying-retrieval-and-generation) - Обзорная статья о CLaRa