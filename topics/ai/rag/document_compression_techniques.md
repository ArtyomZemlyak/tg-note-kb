# Методы сжатия документов в RAG-системах

## Обзор

Сжатие документов в RAG-системах - это подход к уменьшению размера извлекаемых документов перед передачей в языковую модель, что позволяет эффективно использовать контекстное пространство и улучшить производительность системы. Методы сжатия особенно важны при работе с длинными документами или большими коллекциями документов.

## Методы сжатия

### 1. Семантическое сжатие

#### Apple CLaRa (Continuous Latent Reasoning)
- Использует непрерывное латентное пространство для сжатия документов
- Сжимает документы в 16-128 раз
- Объединяет извлечение и генерацию в общей архитектуре
- Подробнее: [[apple_clara_continuous_latent_reasoning.md]]

#### LLMLingua
- Сжимает промпт, сохраняя ключевые семантические элементы
- Использует предобученную языковую модель для определения важных частей текста
- Позволяет сжимать до 20x с минимальной потерей качества

#### PISCO
- Основан на семантическом разнообразии
- Сохраняет наиболее релевантные фразы и предложения
- Обеспечивает баланс между сжатием и сохранением информации

### 2. Сжатие с помощью сжимателей (Compressors)

#### Сжиматели на основе внимания
- Используют механизмы внимания для определения важных частей документа
- Могут быть обучены для конкретных доменов
- Позволяют сохранять контекст при снижении длины текста

#### Пост-обученные сжиматели
- Обучаются на парах "длинный текст - краткий представление"
- Часто используют методы обучения с учителем
- Могут быть оптимизированы под конкретные задачи (QA, суммаризация и т.д.)

### 3. Методы на основе извлечения

#### Извлечение предложений
- Выбирает наиболее релевантные предложения из документа
- Основано на оценке релевантности к вопросу
- Сохраняет оригинальные формулировки

#### Извлечение фраз
- Извлекает ключевые фразы и термины
- Часто используется для быстрого поиска информации
- Менее эффективно для задач, требующих контекста

## Применение сжатия в RAG-системах

### Преимущества
- **Экономия контекста**: Сжатие позволяет вместить больше информации в ограниченное контекстное пространство
- **Уменьшение вычислительных затрат**: Короче контексты требуют меньше вычислительных ресурсов
- **Повышение скорости**: Более короткие входы ускоряют процесс генерации
- **Снижение стоимости**: Меньше токенов = меньшая стоимость при использовании API

### Вызовы
- **Потеря информации**: Сжатие может привести к потере важных деталей
- **Качество сжатия**: Низкокачественное сжатие ухудшает ответы
- **Дополнительная задержка**: Процесс сжатия добавляет время к обработке запроса
- **Сложность настройки**: Требует тонкой настройки под конкретную задачу

## Сравнение методов

| Метод | Степень сжатия | Сохранение качества | Скорость | Сложность |
|-------|----------------|--------------------|----------|-----------|
| CLaRa | 16x-128x | Высокое | Высокая | Высокая |
| LLMLingua-2 | 5x-20x | Высокое | Средняя | Средняя |
| PISCO | 2x-8x | Среднее | Средняя | Низкая |

## Использование в практике

### Когда использовать сжатие
- При работе с длинными документами (статьи, отчеты, законодательство)
- Когда контекстное окно модели ограничено
- При наличии большого количества релевантных документов
- В системах с ограничениями по стоимости или времени отклика

### Выбор метода сжатия
- Для высокоточных задач рекомендуется использовать CLaRa или LLMLingua
- Для простых задач могут быть достаточны методы извлечения
- Для специфических доменов возможно обучение специализированных сжимателей

## Связи с другими темами
- [[retrieval_strategies.md]] - стратегии извлечения информации
- [[llm_integration.md]] - интеграция с языковыми моделями
- [[retrieval_optimization.md]] - оптимизация процесса извлечения
- [[best_practices/chunking_strategies.md]] - стратегии разбиения текста на фрагменты

## Источники
1. [CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning](https://arxiv.org/abs/2511.18659) - Оригинальная статья о CLaRa
2. [LLMLingua: Compressing Prompts for Accelerating Large Language Models](https://arxiv.org/abs/2310.05736) - Статья о методе LLMLingua
3. [PISCO: Semantic Diversity-based Dense Retrieval for Large-Scale Top-k Search](https://arxiv.org/abs/2205.09743) - Описание метода PISCO