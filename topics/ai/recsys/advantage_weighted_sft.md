# Advantage-Weighted Supervised Fine-Tuning (A-SFT)

## Краткое описание

Advantage-Weighted Supervised Fine-Tuning (A-SFT) - это новая техника пост-тренировки для генеративных рекомендательных моделей, разработанная Netflix. Это "упрощённая альтернатива RLHF", выполненная по-инженерному и приземлённо, предназначенная для дообучения уже натренированных генеративных моделей с учётом преимущества (advantage) - насколько текущее предсказание лучше среднего по reward-модели.

## Основная информация

A-SFT реализует подход reward-learning без использования RLHF. Техника работает следующим образом:

1. Берёт уже натренированную генеративную модель, которая умеет "писать" рекомендации
2. Дообучает её не просто на пользовательских примерах, а с учётом преимущества (advantage) - насколько текущее предсказание лучше среднего по reward-модели

### Отличие от традиционных методов

#### Сравнение с RLHF в рекомендательных системах

- **RLHF в рекомендательных системах** - традиционно сложен из-за необходимости:
  - Разруливать шум в пользовательских данных
  - Решать проблему дефицита логов взаимодействия
  - Обрабатывать сложную инфраструктуру
  - Проводить дорогостоящие онлайн-итерации

- **A-SFT** предлагает более простую альтернативу, объединяя достоинства обучения с учителем (SFT) с элементами обучения с подкреплением, но без сложностей полного RLHF подхода.

## Новые концепции и термины

- **Advantage-weighted learning** - метод дообучения, при котором веса примеров определяются на основе "преимущества" - насколько текущее предсказание лучше среднего по reward-модели
- **Reward-learning без RLHF** - подход, использующий reward модели для определения качества предсказаний без полноценного цикла обучения с подкреплением
- **Генеративные рекомендательные модели** - модели, которые генерируют рекомендации в виде текста или последовательностей, а не просто вычисляют рейтинги для существующих айтемов

## Примеры применения

- Пост-тренировка генеративных рекомендательных моделей в системах персонализации
- Улучшение качества рекомендаций без необходимости в сложной RL инфраструктуре
- Дообучение моделей на основе пользовательских предпочтений с учётом весов, определяемых reward-моделью

## Связи с другими темами

- [[./generative_retrieval_models|Генеративные модели поиска]] - связанная тема, рассматривающая генеративные подходы в рекомендательных системах
- [[./llm_based/overview|LLM-рекомендательные системы]] - контекст использования больших языковых моделей в рекомендациях
- [[../llm/rlhf|Обучение с подкреплением с человеческой обратной связью (RLHF)]] - метод, альтернативой которому является A-SFT
- [[../llm/reasoning/sft_rlvr_methodology|Методология SFT+RLVR]] - другой подход, сочетающий SFT с элементами обучения с подкреплением