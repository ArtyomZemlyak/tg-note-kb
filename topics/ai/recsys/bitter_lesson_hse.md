# Урок Гарри (Bitter Lesson) в контексте рекомендательных систем - Ваня Рубачёв (HSE DL2)

## Описание

"Урок Гарри" (The Bitter Lesson) - это концепция, предложенная Ричардом С. Саттоном (Richard S. Sutton), которая утверждает, что прогресс в ИИ в долгосрочной перспективе происходит главным образом за счет увеличения масштабов вычислений, а не за счет разработки сложных алгоритмов или использования человеческих знаний. В контексте рекомендательных систем, Ваня Рубачёв (Иван Рубачёв) рассмотрел влияние этой идеи на развитие и архитектуру рекомендательных систем.

## Контекст

Лекция про "Урок Гарри" была прочитана Иваном Рубачёвым в рамках курса "Deep Learning 2" на Факультете компьютерных наук НИУ ВШЭ. Концепция особенно актуальна для рекомендательных систем, где наблюдается тенденция к масштабированию моделей и данных.

## Основные положения Урока Гарри

### 1. Вычисления vs. Знания

Суть Урока Гарри в том, что:
- Сложные методы, основанные на человеческих знаниях, в конечном итоге уступают простым методам, усиленным вычислениями
- Прогресс в ИИ в долгосрочной перспективе обусловлен масштабированием (данные, параметры, вычисления), а не хитрыми алгоритмами

### 2. Исторические примеры

- Шахматы: Ранние системы использовали эвристики, но современные решения (AlphaZero) используют масштабные вычисления и обучение с подкреплением
- Обработка естественного языка: Ранние системы использовали сложные правила, современные - масштабные трансформеры
- Компьютерное зрение: Раньше использовали ручной экстракт признаков, теперь - сверточные сети, обученные на больших данных

## Применение к рекомендательным системам

### 1. Масштабирование эмбеддингов

- Ранее: Небольшие эмбеддинги с ручной инженерией признаков
- Современные подходы: Масштабные эмбеддинги (миллионы пользователей/айтемов), унифицированные эмбеддинги
- См. [[unified_embeddings.md]]

### 2. Архитектурные изменения

- Ранее: Простые модели (линейные модели, факторизационные машины) с хитрыми фичами
- Современные подходы: Простые архитектуры, усиленные масштабами (DeepFM, DCN-v2, PLE)
- См. [[dcn_v2_architecture.md]], [[ple_architecture.md]]

### 3. Переход к нейросетевым подходам

- Ранее: Градиентный бустинг (CatBoost, XGBoost), коллаборативная фильтрация
- Современные подходы: Глубокие нейронные сети (как показано в HSE лекции, нейросеть выигрывает у CatBoost на Yambda)
- См. [[hse_dl2_neural_recsys.md]]

### 4. Генеративные рекомендательные системы

- Ранее: Правила и логика, закодированная вручную
- Современные подходы: Генеративные модели (TBGRecall, PLUM), обучающиеся на больших объемах данных
- См. [[generative_retrieval_models.md]]

## Примеры из рекомендательных систем

### YouTube DNN

- Использование масштабных нейронных сетей для кандидат-генерации
- Обработка миллионов пользователей и айтемов
- См. [[candidate_generation.md]]

### Alibaba и Tencent

- Масштабирование до миллиардов пользователей и айтемов
- Использование вычислительной мощности вместо сложных эвристик
- См. [[ple_architecture.md]]

### Google и Yandex

- Масштабирование рекомендательных систем с помощью унифицированных эмбеддингов
- Использование больших вычислительных ресурсов для обучения и инференса
- См. [[unified_embeddings.md]]

## Влияние на исследовательские направления

### 1. Переход от инженерии признаков к масштабным моделям

- Ранее: Большое внимание уделялось инженерии признаков
- Современные подходы: Автоматическое извлечение признаков через масштабные модели

### 2. Масштабирование vs. Эффективность

- Конфликт между масштабированием и вычислительной эффективностью
- Необходимость компромиссов между качеством и ресурсами

### 3. Развитие новых архитектур

- DCN-v2, PLE и другие архитектуры как попытка эффективного масштабирования
- Баланс между выразительностью и вычислительной эффективностью

## Примеры подтверждения Урока Гарри в рекомендательных системах

### 1. Сравнение нейросетей и градиентного бустинга

Как упоминается в HSE лекции, на семинаре было показано, что нейросеть выигрывает у CatBoost в задаче ранжирования на Yambda. Это подтверждает идею Урока Гарри - масштабные нейросетевые модели (усиленные вычислениями) превосходят модели, основанные на ручной инженерии признаков (CatBoost).

### 2. Восхождение трансформеров

- Ранее: Специализированные архитектуры для рекомендаций
- Современные подходы: Трансформеры, усиленные масштабами
- См. [[transformer_based_models.md]]

## Критика и ограничения

### 1. Вычислительные ограничения

- Масштабирование требует огромных вычислительных ресурсов
- Не все компании могут позволить себе масштабные эксперименты

### 2. Экологические соображения

- Большое масштабирование приводит к высокому энергопотреблению
- Растущая экологическая цена масштабных моделей

### 3. Не для всех задач

- Некоторые задачи все еще требуют человеческих знаний
- Некоторые домены (например, медицина) требуют объяснимости, а не только масштабов

## Связи с другими темами

- [[hse_dl2_neural_recsys.md]] - Упоминание лекции Вани Рубачёва в HSE курсе
- [[unified_embeddings.md]] - Пример масштабирования в рекомендательных системах
- [[dcn_v2_architecture.md]] - Современные архитектуры, усиленные вычислениями
- [[ple_architecture.md]] - Масштабные мульти-задачные архитектуры
- [[transformer_based_models.md]] - Трансформеры как пример масштабного подхода
- [[traditional_approaches.md]] - Контраст с ранними подходами, основанными на знаниях

## Источники

1. [The Bitter Lesson by Richard S. Sutton] - Оригинальная статья о концепции Урока Гарри
2. [HSE DL2 Course Materials - Van Rubachev's Lecture] - Лекция Вани Рубачёва в рамках курса Deep Learning 2 на ФКН ВШЭ
3. [Scalable Recommendation Systems] - Материалы о масштабировании рекомендательных систем

## Дополнительные материалы

- [[model_scaling_laws.md]] - Законы масштабирования в нейросетях
- [[compute_vs_algorthms_in_ai.md]] - Сравнение вычислений и алгоритмов в ИИ