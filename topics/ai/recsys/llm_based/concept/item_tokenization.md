# Токенизация айтемов в рекомендательных системах

## Описание

Токенизация айтемов - это процесс конвертации рекомендуемых элементов (айтемов) в токены или идентификаторы, которые могут быть обработаны языковыми моделями. Этот подход позволяет использовать методы NLP в рекомендательных системах, где айтемы рассматриваются как "слова" в огромном каталоге.

## Применение в PLUM и TIGER

### В TIGER

В работе TIGER семантические идентификаторы (SIDs) формировались через RQ-VAE поверх текстового описания товара. Эксперименты проводились на открытых датасетах Amazon.

### В PLUM

В PLUM подход к токенизации айтемов был значительно расширен:

1. **Мультимодальные эмбеддинги**: Используются готовые аудио-, видео- и текстовые эмбеддинги YouTube, которые конкатенируются и проходят через энкодер RQ-VAE.

2. **Коллаборативный сигнал**: Через contrastive learning на RQ-VAE с использованием InfoNCE, что позволяет интегрировать пользовательские связи между айтемами прямо в процесс токенизации.

3. **Multi-Resolution Codebooks**: Число идентификаторов в кодбуках уменьшается от слоя к слою, чтобы верхние уровни разделяли крупные семантические категории, а нижние - более гранулярные признаки.

4. **Progressive Masking**: Модель обучается восстанавливать не полный набор SIDs, а его префикс.

## RQ-VAE (Residual Quantization Variational Autoencoder)

RQ-VAE используется для создания компактных семантических представлений айтемов. В контексте рекомендаций:

- **Кодирование**: Текстовое, аудио, видео или другие модальные представления айтема кодируются в вектор
- **Квантование**: Вектор квантуется с использованием нескольких кодбуков с остаточным квантованием
- **Декодирование**: Квантованные представления могут быть преобразованы обратно в приближенное представление айтема

## Преимущества токенизации айтемов

1. **Аналогия с NLP**: Позволяет использовать мощь LLM для обработки айтемов аналогично тексту
2. **Масштабируемость**: Обработка огромных каталогов айтемов без необходимости хранения и обработки каждого айтема отдельно
3. **Семантическое понимание**: LLM может лучше понимать семантические связи между айтемами через их SIDs
4. **Интеграция контекста**: Возможность объединения айтемов с текстовым контекстом в одном представлении

## Связи с другими темами

- [[../llm_based/plum/main.md]] - Подробное описание применения токенизации в PLUM
- [[../llm_based/tiger.md]] - Применение токенизации в предшествующей работе TIGER
- [[../generative_retrieval_models.md]] - Другие подходы к генерации рекомендаций
- [[../../llm/models/generative_models.md]] - Генеративные модели, используемые в системах

## Источники

1. [PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations] - Оригинальная работа, описывающая применение токенизации айтемов в фреймворке PLUM