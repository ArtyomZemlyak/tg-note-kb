# RQ-VAE (Residual Quantization Variational Autoencoder) в рекомендательных системах

## Описание

RQ-VAE (Residual Quantization Variational Autoencoder) - это метод векторного квантования, который используется для преобразования непрерывных эмбеддингов айтемов в дискретные семантические идентификаторы (SIDs). В контексте рекомендательных систем RQ-VAE позволяет эффективно представлять айтемы в формате, пригодном для обработки языковыми моделями.

## Архитектура

RQ-VAE состоит из следующих компонентов:

1. **Энкодер**: Преобразует входные данные (например, текстовое описание, аудио, видео) в непрерывный векторный эмбеддинг
2. **Residual Quantization**: Механизм квантования, который разбивает непрерывный вектор на несколько кодбуков с остаточным квантованием
3. **Декодер**: Преобразует квантованные представления обратно в приближенное восстановление входных данных

## Использование в TIGER и PLUM

### В TIGER

- RQ-VAE применялся поверх текстового описания айтемов
- Создавались семантические идентификаторы (SIDs) для представления айтемов
- Позволяло обрабатывать большие каталоги айтемов в виде дискретных токенов

### В PLUM

- Расширение до мультимодальных эмбеддингов (аудио, видео, текст)
- Добавление коллаборативного сигнала через contrastive learning с InfoNCE
- Multi-Resolution Codebooks: уменьшение числа идентификаторов от слоя к слою
- Progressive Masking: восстановление префикса SIDs, а не полного набора
- Contrastive learning: обучение с использованием пар айтемов, встречавшихся рядом в пользовательской истории

## Преимущества RQ-VAE в рекомендациях

1. **Компактность**: Представление айтемов в дискретной форме с фиксированным размером
2. **Семантика**: Сохранение семантической информации при квантовании
3. **Интеграция с LLM**: Возможность использования SIDs как токенов в языковых моделях
4. **Масштабируемость**: Обработка огромных каталогов без необходимости обработки каждого айтема отдельно

## Contrastive Learning в RQ-VAE

В PLUM ключевым улучшением стало добавление contrastive learning в процесс RQ-VAE с использованием InfoNCE. Это позволяет:

- Интегрировать коллаборативный сигнал прямо в процесс токенизации
- Обучать SIDs не только на контентной информации, но и на пользовательских паттернах
- Создавать более релевантные представления айтемов, учитывающие пользовательские связи

## Связи с другими темами

- [[../llm_based/plum/main.md]] - Подробное описание применения RQ-VAE в PLUM
- [[../llm_based/tiger.md]] - Использование RQ-VAE в предшествующей работе TIGER
- [[item_tokenization.md]] - Общая концепция токенизации айтемов
- [[../../llm/models/generative_models.md]] - Генеративные модели, использующие SIDs

## Источники

1. [PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations] - Оригинальная работа, описывающая улучшенное применение RQ-VAE в контексте токенизации айтемов