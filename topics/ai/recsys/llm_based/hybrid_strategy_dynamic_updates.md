# Балансировка fine-tuning и RAG: Гибридная стратегия для динамических обновлений LLM-рекомендаций

![Архитектура гибридной стратегии fine-tuning и RAG для YouTube Shorts](../../../../media/img_1763644576_aqad6q9rg3qg8uh_image.jpg) <!-- TODO: Broken image path -->

**Описание:** На изображении представлена архитектура гибридной стратегии, сочетающей fine-tuning и RAG для динамических обновлений LLM-рекомендаций в YouTube Shorts. Схема демонстрирует, как подходы дополняют друг друга: ежемесячные fine-tuning обновления адаптируют основные параметры модели, а поднедельные RAG-обновления обеспечивают актуальность контекста и быструю реакцию на изменения пользовательских интересов.

## Описание

Исследование от Google DeepMind, посвящённое стратегии балансировки fine-tuning и RAG (Retrieval-Augmented Generation) для динамических обновлений в LLM-рекомендательных системах. Основное внимание уделено рекомендациям в YouTube Shorts, где пользовательские интересы и контентная база постоянно меняются.

## Контекст проблемы

Традиционные рекомендательные системы сталкиваются с рядом ограничений, в то время как LLM-рекомендательные системы обладают рядом преимуществ:
- Богатое понимание мира
- Способность к рассуждению
- Возможность объяснять рекомендации
- Гибкость в интерпретации сложных запросов

Однако LLM-рекомендательные системы также сталкиваются с проблемой динамики:
- Изменение интересов пользователей со временем
- Постоянное обновление контентной базы
- Необходимость постоянной адаптации к новым паттернам поведения

## Методология исследования

### Экспериментальная среда
- **Платформа**: YouTube Shorts
- **Фокус**: Анализ изменчивости пользовательских интересов
- **Метод**: Кластеризация тематик шортсов

### Анализ пользовательской динамики
Авторы провели интересный эксперимент:
1. Кластеризовали тематики шортсов
2. По логам пользователей собирали тройки (c1, c2, c_next) кластеров, с которыми пользователи последовательно взаимодействовали
3. Делали это отдельно для нескольких месяцев
4. Для всех пар (c1, c2) собирали топ-5 переходов в c_next для каждого месяца i: {c_next_1, …, c_next_5}_i
5. Для пар (c1, c2) вычисляли IoU (Intersection over Union) множеств переходов за соседние месяцы (i vs. i+1)

**Результат**: низкое значение IoU = 0.17, что подчеркивает высокую изменчивость паттернов пользовательского поведения во времени и необходимость постоянного обновления LLM-рекомендательной системы.

## Сравнение методов: Fine-tuning vs RAG

### Fine-tuning
- **Механизм**: Обновление весов модели через дообучение на новом трафике
- **Процесс**: Модель дообучается предсказывать следующий кластер, с которым провзаимодействовало большинство пользователей: (c_1, c_2, …, c_n) → c_{n+1}
- **Данные**: Описания кластеров поступают в LLM в словесной форме
- **Преимущества**: 
  - Глубокая интеграция информации в модель
  - Возможность адаптации внутренних представлений
- **Недостатки**:
  - Высокая вычислительная сложность
  - Риск переобучения
  - Возможность деградации при частом обновлении
- **Частота обновления**: Ежемесячно из-за вычислительных затрат

### RAG (Retrieval-Augmented Generation)
- **Механизм**: Усиление промпта недостающей информацией о пользователе и домене без изменения самой модели
- **Процесс**: 
  - Представление истории в виде последних взаимодействий с кластерами (обновленные интересы пользователя)
  - Добавление в промпт наиболее популярного продолжения для этой последовательности взаимодействий (обновленные реалии домена)
- **Преимущества**:
  - Гибкость обновлений без изменения параметров модели
  - Более быстрая адаптация к изменениям
  - Отсутствие риска переобучения
- **Особенности**:
  - Поскольку множество всевозможных историй вида (c_1, c_2, …, c_k) невелико и конечно
  - Инференс производится несколько раз в неделю
  - Предпосчитанные кандидаты для каждой истории достаются в реальном времени лукапом
- **Частота обновления**: Несколько раз в неделю

## Результаты экспериментов

### Офлайн-эксперименты
- Подтвердили необходимость использования RAG
- Показали важность пересчёта кандидатов раз в несколько дней
- Оба подхода (fine-tuning и RAG) показали ценность в различной степени

### A/B-тесты
В онлайн A/B-тестах зафиксированы следующие улучшения:
- Рост Satisfied User Outcomes
- Повышение Satisfaction Rate
- Снижение Dissatisfaction Rate
- Уменьшение Negative Interaction

## Гибридный подход

Исследование показывает, что оптимальным может быть гибридное решение:
- **Регулярный fine-tuning**: Ежемесячное дообучение для фундаментальной адаптации модели
- **Частые RAG-обновления**: Более частые обновления контекста через RAG для оперативной реакции на изменения
- **Компромисс**: Баланс между глубокой адаптацией (fine-tuning) и оперативностью (RAG)

## Практические выводы

1. **Необходимость обновлений**: LLM-рекомендательные системы требуют регулярных обновлений из-за высокой динамики пользовательских интересов
2. **Оптимальная частота**: Комбинация ежемесячного fine-tuning с поднедельным RAG обеспечивает эффективное, экономичное решение
3. **Гибкость**: RAG позволяет быстрее реагировать на краткосрочные изменения, а fine-tuning - на долгосрочные тренды
4. **Стоимость**: RAG обходится дешевле fine-tuning с точки зрения вычислительных ресурсов

## Связи с другими темами

- [[./main.md]] - Общая информация о LLM-based рекомендательных системах
- [[./plum/main.md]] - PLUM: подходы к рекомендациям с использованием LLM в YouTube
- [[./concept_rag_user_index.md]] - Концепт RAG-архитектуры в рекомендательных системах
- [[../generative_retrieval_models.md]] - Генеративные модели поиска в рекомендательных системах
- [[../../llm/memory/llm_memory_overview.md]] - Обзор подходов к памяти в LLM, включая RAG
- [[../../../rag/best_practices/overview.md]] - Лучшие практики RAG: Обзор подходов и стратегий

## Источники

1. [Balancing Fine-tuning and RAG: A Hybrid Strategy for Dynamic LLM Recommendation Updates] - Оригинальная научная работа от Google DeepMind о стратегии балансировки fine-tuning и RAG для динамических обновлений LLM-рекомендаций в YouTube Shorts