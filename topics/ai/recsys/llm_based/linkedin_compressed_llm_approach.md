# LinkedIn: Сжатие LLM для рекомендательных систем

## Краткое описание

LinkedIn представила практический опыт перехода от больших LLM к более компактным Small Language Models (SLM) в рекомендательных системах на конференции EMNLP 2025. Подход позволяет эффективно использовать сжатые модели в реальных рекомендательных сервисах с высоким трафиком, решая проблемы высокой стоимости инференса и больших задержек, характерных для больших LLM.

## Основная информация

### Контекст проблемы

Большие языковые модели уже показали отличные результаты в задачах рекомендаций, но их применение в продакшене часто сталкивается с двумя ключевыми ограничениями:
- Высокая стоимость инференса
- Большие задержки при обслуживании трафика

LinkedIn решила эту проблему, разработав методику последовательного сжатия LLM, позволяющую достичь компактности без потери качества рекомендаций.

### Трехэтапный процесс сжатия

LinkedIn использовала последовательность из трех этапов для создания компактных моделей:

#### 1. Дистилляция
- Обучение модели-«студента» на поведении модели-«учителя»
- Задача — перенести знания крупной модели в меньшую, максимально сохранив качество
- Использует методы knowledge distillation для передачи информации от большой модели к маленькой

#### 2. Pruning (Прореживание)
- Структурное уменьшение числа параметров
- Отбрасываются наименее важные части модели
- Снижает ресурсоёмкость и ускоряет инференс
- В отличие от унитарного прунинга, удаляет целые структурные компоненты (например, головы внимания, нейроны)

#### 3. Редистилляция (Redistillation)
- Финальный этап, на котором модель снова дообучают
- Частично увеличивает число параметров и улучшает обобщающую способность
- Позволяет вернуть части выражаемости без возврата к исходным затратам на вычисления
- Балансирует между компактностью и качеством модели

### Инженерные аспекты

Авторы подчеркивают важность не только метода сжатия, но и практические инженерные аспекты:

#### Оптимизация инференса
- Ускорение процесса генерации рекомендаций
- Снижение вычислительных затрат
- Повышение стабильности инференса

#### Особенности деплоя
- Развертывание сжатых моделей в продакшене
- Обработка высоконагруженного трафика
- Обеспечение надёжности системы

#### Дизайн экспериментов в продакшене
- A/B-тестирование на реальном трафике
- Измерение бизнес-метрик
- Сравнение с базовыми моделями

## Результаты

### Офлайн-эксперименты
- Размер модели удалось уменьшить примерно на 20%
- Без заметной потери качества в офлайн-экспериментах

### A/B-тестирование
- В A/B-тесте на 1% трафика компактная SLM показала +20% к внутренней бизнес-метрике LinkedIn
- Модель стала быстрее, дешевле и стабильнее по сравнению с более крупными LLM

### Производительность
- Значительно снижена стоимость инференса
- Улучшена стабильность сервиса
- Повышена масштабируемость для крупных платформ с высокой нагрузкой

## Новые концепции и термины

- **Small Language Models (SLM)**: Компактные языковые модели, созданные из больших LLM через процессы сжатия, оптимизированные для специфических задач
- **Редистилляция (Redistillation)**: Финальный этап процесса сжатия, при котором сжатая модель дообучается для восстановления потенциальной выразительности
- **LinkedIn Large Scale Retrieval**: Подход LinkedIn к использованию LLM в рекомендательных системах с заменой традиционной двух-башенной архитектуры на единую LLM
- **Матрешечное обучение (Matryoshka learning)**: Метод сжатия эмбеддингов, позволяющий использовать один и тот же эмбеддинг для разных размеров представлений

## Примеры применения

### Масштабируемые рекомендательные системы
- Применение сжатых LLM в рекомендательных системах крупных платформ
- Снижение затрат на инференс при сохранении качества рекомендаций
- Возможность масштабирования для миллионов пользователей

### Продакшен-деплой
- Развертывание эффективных моделей в реальных продуктах, а не только в исследовательских прототипах
- Снижение операционных затрат на рекомендательные системы
- Улучшение пользовательского опыта за счёт уменьшенной задержки

## Связи с другими темами

- [[distillation_pruning_redistillation.md]] - Подробное описание трёхэтапной методики сжатия моделей
- [[linkedin_large_scale_retrieval.md]] - Подход LinkedIn к использованию LLM в рекомендательных системах
- [[../llm_based/llm_candidate_generation_approaches.md]] - Сравнительный анализ LLM-базированных подходов к генерации кандидатов
- [[../../llm/knowledge_distillation.md]] - Общие принципы дистилляции знаний
- [[../../llm/pruning/structured_pruning.md]] - Методы структурированного прунинга
- [[../../embedders/matryoshka_representation_learning.md]] - Методы сжатия эмбеддингов, использованные в подходе

## Источники

1. [LinkedIn's Approach to Compressing and Deploying Efficient LLMs for Recommendation Systems at EMNLP 2025](https://arxiv.org/html/2502.14305v2) - Основная статья, представленная командой LinkedIn на EMNLP 2025, описывающая методику сжатия LLM для рекомендательных систем
2. [AI VK Summary of LinkedIn's EMNLP 2025 Paper](https://t.me/ai_vkm1/771) - Обзор статьи подготовлен командой AI VK, содержащий ключевые моменты исследования LinkedIn