# LinkedIn Large Scale Retrieval - Подход к LLM-базированной генерации кандидатов

## Описание

LinkedIn разработала инновационный подход к LLM-базированной генерации кандидатов, описанный в статье "Large Scale Retrieval...". Подход основывается на замене традиционной двух-башенной архитектуры глубокого обучения на единую LLM-модель, при этом сохраняя классическую постановку задачи - обучение построению эмбеддингов пользователей и айтемов с приближением их по коллаборативному сигналу через контрастивную функцию потерь (contrastive loss).

## Архитектура

### Замена двух-башенной модели
- Вместо двух отдельных башен (одна для пользователей, другая для айтемов) используется общая LLM
- Сохраняется стандартная постановка задачи: обучение построению эмбеддингов пользователей и айтемов
- Цель - сближение эмбеддингов через коллаборативный сигнал с использованием привычного contrastive loss

### Текстовое представление
- Айтемы подаются в LLM в виде текстового описания имеющихся фичей
- Добавляются счётчики популярности айтемов (по утверждению в источнике)
- Пользовательская история также представляется в виде текстового описания фичей пользователя и истории его позитивных взаимодействий

### Обучение модели
- Вместо обучения с нуля используется файнтюн предобученной LLM
- На входе тексты, на выходе - эмбеддинги айтемов/пользователей
- Для перехода от эмбеддингов токенов на выходе LLM к финальным эмбеддингам пользователей и айтемов используется mean pooling
- Близость айтема и пользователя оценивается по косинусу

### Обучение на InfoNCE loss
- Используется InfoNCE loss для обучения
- Для каждого позитива берутся не только In-Batch негативы, но и 2 Hard Negatives
- Hard Negatives - это айтемы, которые пользователь видел в своей ленте, но с которыми не совершил положительных взаимодействий (impressions)

### Matryoshka learning
- Используется Matryoshka learning для сжатия размера финальных эмбеддингов
- Показано, что использование эмбеддингов размерности 512 не влияет на recall по сравнению с оригинальными 3072 в модели LLaMA-3 3B при инференсе

## Результаты

- Подход был успешно внедрён в продакшен
- Зафиксированы приросты в A/B тестировании
- Эмпирически показано, что текстовое представление айтемов является максимально нативным для LLM в домене LinkedIn, что обеспечивает эффективность подхода

## Особенности и ограничения

- Текстовое представление айтемов может быть особенно эффективным в домене LinkedIn, но не факт, что такой простой переход на LLM-башенную модель даст ощутимые приросты в других доменах
- Существует проблема ограничения длины пользовательских историй, которые можно подать в LLM
- Длина обрабатываемой истории пользовательских действий ограничена контекстной длиной модели

## Сравнение с другими подходами

- Отличается от RecGPT от Alibaba тем, что использует единую LLM для создания эмбеддингов, тогда как RecGPT использует последовательность LLM для генерации промежуточных представлений, а финальные рекомендации строятся не-LLM моделью

## Связи с другими темами

- [[main.md]] - Основы LLM-рекомендаций
- [[../../candidate_generation.md]] - Генерация кандидатов в рекомендательных системах
- [[../llm_based/recgpt/main.md]] - Сравниваемый подход: RecGPT от Alibaba
- [[../../ranking.md]] - Ранжирование в рекомендательных системах
- [[../../traditional_approaches.md]] - Традиционные подходы к рекомендательным системам