# PLUM (Google/YouTube)

## Описание

PLUM - это LLM-based рекомендательная система, разработанная Google и внедрённая в YouTube. Система представляет собой значительное улучшение по сравнению с традиционными подходами, используя дообученную LLM для генерации рекомендаций. Исследование продолжает трек генеративных рекомендаций, заданный предыдущей работой TIGER. Основная идея - использование предобученных больших языковых моделей в рекомендательных пайплайнах (в случае Google - это Gemini).

## Архитектура

### Три стадии PLUM

1. **Item tokenization** (Токенизация айтемов): Основа взята из работы TIGER, где семантические идентификаторы (SIDs) формировались через RQ-VAE поверх текстового описания товара. В PLUM к этому подходу добавляют коллаборативный сигнал и мультимодальные контентные представления. Используются уже готовые аудио-, видео- и текстовые эмбеддинги YouTube, которые конкатенируются и проходят через энкодер RQ-VAE.

   Новые предложенные компоненты:
   - **Multi-Resolution Codebooks**: число идентификаторов в кодбуках уменьшается от слоя к слою, чтобы верхние уровни разделяли крупные семантические категории, а нижние - более гранулярные признаки.
   - **Progressive Masking**: модель обучается восстанавливать не полный набор SIDs, а его префикс.

   Ключевая вещь в архитектуре - дополнительный contrastive learning на RQ-VAE, который вводит коллаборативный сигнал прямо в процесс токенизации. Берутся пары айтемов, встречавшихся рядом в пользовательской истории как позитивные пары, обучается с помощью InfoNCE по батчу. Так коллаборативный сигнал тоже участвует в формировании кодбуков без отдельной стадии дообучения как, например, в OneRec. В итоге SIDs начинают отражать не только контентную информацию об айтемах, но и коллаборативные пользовательские связи между ними.

2. **Continued Pre-Training (CPT)** (Продолженное предобучение): Языковая модель дообучается с увеличенным словарём, в который, помимо изначальных токенов, встроены токены айтемов. Модель обучается на смешанной задаче (supervised + self-supervised). Цель этой стадии - заставить LLM встроить в общее семантическое пространство представления токенов и SIDs.

3. **Task-Specific Fine-Tuning** (Специфическая настройка под задачу): Это полноценное обучение на задачу генеративного ретривала: модель предсказывает релевантные айтемы в пользовательских историях (обучение на next token prediction).

### Основные компоненты

1. **Используемая LLM**: Gemini-1.5-MoE-900M (Mixture-of-Experts с ~900 млн активных параметров, всего 4,2 млрд)
2. **Семантические ID (SIDs)**: Главное представление айтемов для модели
   - Строится на основе мульти-модальных контентных эмбеддингов
   - Включает коллаборативный сигнал из сервиса
   - Работает с видео-платформой (YouTube)

3. **Претрейн**:
   - Словарь LLM расширяется, чтобы включать каталог семантических ID
   - Дообучение на задаче Next Token Prediction
   - Интеграция SIDs в текстовое описание поведения пользователей

4. **Файнтюн**:
   - Фокус на предсказании будущих айтемов для пользователя
   - Учет рантаймового контекста в промте
   - Может быть использовано для различных даунстрим задач, не только для кандидато-генерации

## Особенности

- Количество dense параметров в трансформере у PLUM в 100 раз больше, чем у продового трансформера YouTube над пользовательской историей
- Модель внедрена как в оффлайне, так и в рантайме
- Подобные архитектуры можно использовать для различных даунстрим задач, не только для кандидато-генерации
- Идея PLUM строится на прямой аналогии между словами в языковых моделях и айтемами в RecSys: если в NLP слова токенизируются для работы с огромным словарём, то в рекомендациях можно аналогично токенизировать айтемы
- Финальная PLUM-модель дообучается ежедневно на ~0,25 млрд примеров, тогда как предыдущие LEM (Large Embedding Models) подходы требовали многомиллиардных датасетов

## Результаты

- Онлайн A/B тесты показывают рост ключевых метрик: CTR и вовлечённости пользователей, особенно в коротких видео (YouTube Shorts)
- Аблейшены подтверждают, что важны все предложенные компоненты
- В работе показывают законы масштабирования для предложенного фреймворка: при увеличении размера моделей при разном фиксированном вычислительном бюджете ошибки на обучении и валидации снижаются, но самые большие модели (около 3 млрд активных параметров, 20 млрд всего) пока упираются в ограничения вычислительных ресурсов
- Исследователям не хватило времени, данных и мощностей, чтобы хорошо обучить модели такого размера, однако инженеры считают, что при дальнейшем масштабировании качество может вырасти ещё больше
- Успешные результаты A/B тестирования при внедрении в YouTube
- Значительное улучшение метрик рекомендательной системы по сравнению с предыдущими решениями

## Связи с другими темами

- [[../main.md]] - Общая информация о LLM-based рекомендательных системах
- [[ai/recsys/llm_based/tiger.md]] - Предшествующая работа TIGER, на основе которой развивался PLUM
- [[ai/recsys/llm_based/concept/item_tokenization.md]] - Токенизация айтемов с использованием RQ-VAE
- [[ai/llm/models/gemini.md]] - Модель Gemini, использованная в системе
- [[ai/recsys/candidate_generation.md]] - Кандидат-генерация, улучшенная с помощью PLUM
- [[ai/recsys/downstream_tasks.md]] - Даунстрим задачи, которые могут быть решены с помощью архитектуры PLUM

## Источники

1. [Google DeepMind & YouTube Research: PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations] - Оригинальная научная работа, описывающая фреймворк PLUM и его применение в рекомендательных системах YouTube