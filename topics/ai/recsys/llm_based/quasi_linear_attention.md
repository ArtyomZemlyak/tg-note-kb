# Квазилинейное внимание (Quasi-Linear Attention) в рекомендательных системах

## Описание

Квазилинейное внимание (QLA - Quasi-Linear Attention) - это механизм внимания, разработанный специально для рекомендательных систем, который обеспечивает линейную вычислительную сложность по длине последовательности без "перетекания" внимания между кандидатами. Этот подход используется в архитектуре VISTA для обеспечения фиксированной стоимости инференса при работе с длинной историей взаимодействия пользователей.

## Технические детали

### Проблема стандартного внимания

Традиционное self-attention в трансформерах имеет квадратичную сложность O(n²), где n - длина последовательности. Это создает серьезные ограничения при работе с длинной историей пользователей (UIH) в рекомендательных системах, где n может достигать миллиона событий.

### Решение QLA

QLA обеспечивает:
- Линейную сложность по длине последовательности
- Отсутствие "перетекания" внимания между кандидатами
- Предотвращение утечек (leakage) информации между кандидатами

### Архитектурные особенности

Схоже с подходами Lightning/Mamba, но адаптировано под задачи рекомендательных систем. Это позволяет:
- Масштабироваться до 16k длины на блок больше при сопоставимых метриках
- Сохранять фиксированную стоимость инференса
- Обеспечивать более эффективное использование вычислительных ресурсов

## Применение

- **VISTA архитектура**: используется для обеспечения фиксированной стоимости инференса при работе с длинной историей пользователей
- **Генеративные рекомендательные системы**: когда необходимо эффективно обрабатывать пожизненную историю взаимодействия
- **Системы с большим количеством кандидатов**: когда стандартные подходы приводят к линейному росту стоимости

## Преимущества

1. **Масштабируемость**: Возможность работы с длинными последовательностями без экспоненциального роста затрат
2. **Фиксированная стоимость инференса**: Не зависит от длины пользовательской истории
3. **Предотвращение утечек**: Отсутствие "перетекания" внимания между кандидатами
4. **Эффективность**: Позволяет масштабироваться до 16k длины при сопоставимых метриках

## Сравнение с другими механизмами внимания

| Механизм | Сложность | Память | Скорость | Качество | Особенности |
|----------|-----------|--------|----------|----------|-------------|
| Стандартное Self-Attention | O(n²) | O(n²) | Низкая | Высокое | Традиционный подход |
| Sparse Attention | O(n) или O(n log n) | O(n) | Высокая | Зависит от шаблона | Ограниченные соединения |
| Linear Attention | O(n) | O(n) | Высокая | Ниже для сложных зависимостей | Приближенные вычисления |
| Quasi-Linear Attention | O(n) | O(n) | Высокая | Высокое | Без утечек между кандидатами |
| Log-Linear Attention | O(n log n) | O(n log n) | Высокая | Высокое | Баланс между точностью и эффективностью |

## Связи с другими темами

- [[vista_architecture.md]] - VISTA: Основное применение QLA в архитектуре VISTA
- [[../specialized_attention_mechanisms.md]] - Обзор различных специализированных механизмов внимания
- [[../log_linear_attention.md]] - Log-Linear Attention: Альтернативный подход к эффективному вниманию
- [[../linear_sequence_modeling.md]] - Линейное моделирование последовательностей
- [[../mamba_architecture.md]] - Mamba: Альтернативный подход к эффективной обработке последовательностей
- [[../../nlp/transformers/next_gen_transformer_architectures.md]] - Перспективные архитектуры трансформеров и усовершенствованные механизмы внимания

## Источники

1. [Сообщение о VISTA и Massive Memorization от @researchoshnaya](https://t.me/researchoshnaya) - описание квазилинейного внимания (QLA) как компонента архитектуры VISTA от Meta