# PLE (Progressive Layered Extraction) - Архитектура для рекомендательных систем

## Описание

PLE (Progressive Layered Extraction) - это архитектура нейронной сети, разработанная Tencent для решения задач рекомендательных систем, особенно в контексте мульти-задачного обучения (multi-task learning). PLE улучшает традиционные подходы мульти-задачного обучения, такие как MMoE (Multi-gate Mixture-of-Experts), путем введения более изощренной схемы разделения знаний между задачами.

В отличие от MMoE, PLE использует две иерархические структуры: задаче-специфичные эксперты (task-specific experts) и обобщенные эксперты (shared experts), что позволяет более эффективно моделировать как общие для всех задач знания, так и специфичные для каждой задачи.

## Контекст и проблема

В задачах рекомендательных систем часто необходимо предсказывать несколько метрик одновременно (например, CTR, CVR, время просмотра, вероятность покупки). Традиционные подходы включают:

1. **Отдельные модели для каждой задачи**: Высокое потребление ресурсов и игнорирование общих паттернов
2. **Единая модель для всех задач**: Потенциальная деградация качества из-за конфликта между задачами
3. **MMoE**: Улучшенный подход, но все еще имеет ограничения в моделировании сложных взаимодействий между задачами

PLE решает проблему эффективного мульти-задачного обучения в рекомендательных системах, позволяя модели одновременно достигать высокого качества по нескольким задачам.

## Архитектура PLE

### Основные компоненты

#### 1. Задаче-специфичные эксперты (Task-specific Experts)
- Отдельный набор экспертов для каждой задачи
- Обрабатывают информацию, специфичную для конкретной задачи
- Позволяют каждой задаче иметь собственный набор признаков

#### 2. Обобщенные эксперты (Shared Experts)
- Общие эксперты, используемые всеми задачами
- Обрабатывают общие знания, полезные для всех задач
- Повышают эффективность использования данных

#### 3. Управляющие вентили (Gating Networks)
- Отдельный вентиль для каждого набора экспертов
- Управляет взвешиванием выходов экспертов
- Позволяет модели адаптивно выбирать, какие эксперты использовать для каждой задачи

### Структура PLE

В отличие от MMoE, где эксперты полностью разделены или полностью общие, PLE использует иерархическую структуру:

```
Входной вектор
    ↓
┌─────────────────┐
│  Shared Experts │
└─────────────────┘
         ↓
┌─────────────────┐
│  Task-specific  │
│     Experts     │
└─────────────────┘
         ↓
   ┌─────────┐
   │Gating   │
   │Networks │
   └─────────┘
         ↓
   ┌─────────┐ ┌─────────┐
   │ Task 1  │ │ Task N  │
   └─────────┘ └─────────┘
```

### Формула работы PLE

Для каждой задачи i, выход на уровне l вычисляется как:

```
h_l^i = g_l^i * (W_l^i * concat(h_{l-1}^shared, h_{l-1}^i)) + b_l^i
```

где:
- h_l^i - выход для задачи i на уровне l
- g_l^i - вектор вентиля для задачи i
- W_l^i - веса вентиля
- concat - конкатенация векторов

## Типы PLE

### 1. Gate-Dependent PLE (PLE-GD)
- Вентили зависят от задачи
- Более гибкая, но сложная для обучения
- Лучше подходит для задач с различающимися распределениями

### 2. Gate-Independent PLE (PLE-GI)
- Вентили независимы от задачи
- Проще для обучения
- Практически более стабильная

## Применение в рекомендательных системах

PLE особенно эффективен в следующих сценариях:
- **Мульти-метрические рекомендации** - одновременное предсказание CTR, CVR, и других метрик
- **Мульти-доменные рекомендации** - рекомендации в разных доменах с обменом знаниями
- **Мульти-модальные рекомендации** - интеграция текстовой, визуальной и структурированной информации

### Преимущества

1. **Эффективное мульти-задачное обучение**: Лучшее разделение знаний между задачами
2. **Борьба с негативным смещением**: Уменьшение эффекта, когда одна задача ухудшает другую
3. **Гибкость**: Возможность адаптации для разных типов задач
4. **Масштабируемость**: Возможность работы с большим количеством задач
5. **Интерпретируемость**: Четкое разделение между специфичными и общими знаниями

### Ограничения

1. **Вычислительная сложность**: Более сложная архитектура требует больше ресурсов
2. **Калибровка**: Необходимость настройки количества экспертов и вентилей
3. **Переобучение**: Риск переобучения при малом объеме данных
4. **Сложность реализации**: Требуется тщательная реализация для достиженения максимальной эффективности

## Сравнение с другими архитектурами

| Архитектура | Моделирование задач | Эффективность | Сложность | Интерпретируемость |
|-------------|---------------------|---------------|-----------|-------------------|
| Single Task | Изолированное | Низкая | Низкая | Высокая |
| Hard Parameter Sharing | Общие параметры | Низкая | Низкая | Низкая |
| Cross-Stitch | Параметрическое разделение | Средняя | Средняя | Средняя |
| MMoE | Смешанные эксперты | Средняя | Средняя | Средняя |
| **PLE** (настоящее) | **Иерархическое разделение** | **Высокая** | **Высокая** | **Высокая** |

## Практические примеры использования

PLE активно используется в промышленных рекомендательных системах:
- **Tencent** - основной подход в их рекомендательных пайплайнах
- **Alibaba** - в системах E-commerce рекомендаций
- **Meituan** - в мульти-задачных рекомендательных системах
- **ByteDance** - для рекомендаций в различных приложениях

В контексте HSE лекции, PLE разбирался как одна из современных архитектур, эффективно моделирующих взаимодействия признаков в рекомендательных системах.

## Вызовы и развитие

Исследования в области PLE продолжаются, включая:
- Интеграция с трансформерными архитектурами
- Применение в генеративных рекомендательных системах
- Масштабирование до больших моделей
- Применение в гетерогенных сетях (HINs) для рекомендаций

## Связи с другими темами

- [[candidate_generation.md]] - PLE может использоваться для мульти-задачной генерации кандидатов
- [[ranking.md]] - PLE особенно полезен для мульти-метрического ранжирования
- [[dcn_v2_architecture.md]] - Альтернативный подход к моделированию взаимодействий признаков
- [[unified_embeddings.md]] - PLE может использоваться с унифицированными эмбеддингами
- [[traditional_approaches.md]] - Контекст для понимания эволюции мульти-задачных архитектур
- [[transformer_based_models.md]] - Современная альтернатива для мульти-модального обучения

## Источники

1. [PLE: Progressive Layered Extraction for Multi-task Recommendation] - Оригинальная работа от Tencent о PLE архитектуре
2. [Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts] - MMoE, предшественник PLE
3. [HSE DL2 Course Materials] - Материалы курса Deep Learning 2 на ФКН ВШЭ, где разбиралась архитектура PLE

## Дополнительные материалы

- [[multi_task_learning_in_recsys.md]] - Обзор мульти-задачного обучения в рекомендательных системах
- [[feature_interaction_learning.md]] - Другие методы моделирования взаимодействий признаков