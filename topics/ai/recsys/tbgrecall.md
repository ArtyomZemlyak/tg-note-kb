# TBGRecall: Генеративная модель поиска для сценариев e-commerce рекомендаций

## Описание

TBGRecall - это инновационная генеративная модель поиска (retrieval), разработанная Alibaba для сценариев рекомендаций в электронной коммерции. Модель используется для генерации кандидатов на главной странице Taobao, самого крупного e-commerce сервиса компании. Архитектурно модель напоминает ARGUS, используемый в Рекламе Яндекса.

TBGRecall решает ключевую проблему традиционных последовательных моделей рекомендаций: в e-commerce пользователи совершают запросы и получают "пачки" товаров без внутреннего упорядочения, в отличие от строгой последовательности событий, предполагаемой в классических подходах.

На конференции CIKM'25 2025 была представлена работа "Taming Ultra-Long Behavior Sequence in Session-wise Generative Recommendation", которая расширяет подход TBGRecall, добавляя возможность работы с очень длинными последовательностями поведения пользователей (до 100 000 айтемов) в сжатом виде, чтобы учитывать долгосрочные интересы пользователей.

## Архитектура

### Основной подход: предсказание следующей сессии

Вместо моделирования строгой последовательности событий, TBGRecall использует подход **предсказания следующей сессии**, где сессия понимается как один запрос пользователя. Модель обучается предсказывать, какие товары пользователь увидит в следующей выдаче.

### Двух-башенная архитектура с генеративным обучением

- История пользователя кодируется в вектор
- Этот вектор сравнивается с векторами кандидатов через индекс ANN (как в классических двухбашенных моделях)
- Слово "генеративная" в названии относится к способу обучения (авторегрессионному), а не к инференсу
- Архитектура представляет собой шаг от классической двухбашенной архитектуры к генеративному обучению

### Контекстные токены

- В начале каждой сессии помещается **контекстный токен** - обобщенное описание запроса
- При инференсе контекстный токен формируется из текущего контекста пользователя
- Напрямую влияет на итоговый вектор, с которым рекомендательная система делает запрос в индекс
- По наблюдениям, контекстные токены дают почти двукратный прирост качества, особенно в сервисах вроде Поиска и Рекламы, где контекст крайне важен

## Кодирование и обучение

### Представление событий

Каждое событие описывается набором признаков:
- **ID айтема (Item ID)**: уникальный идентификатор товара
- **Действие (Action)**: тип взаимодействия пользователя
- **Дополнительная информация (SideInfo)**: ID продавца или категория
- **Контекст (Context)**: дополнительные контекстуальные признаки
- **Временная метка (Timestamp)**: время события

### Процесс кодирования

- Вход модели формируется как сумма векторов признаков
- Сначала проходит через **tower-модули** (отдельные для контекстных и айтемных токенов)
- Затем через **HSTU-блоки** (Hierarchical Semantic Transformer for User Behavior Modeling)
- Отдельные tower-модули для контекстных и айтемных токенов критически важны для качества

### Обучение

#### Session-wise авторегрессивный подход

- Используется маска внимания, которая не позволяет айтемам внутри одной сессии "видеть" друг друга
- Применяется **session-wise ROPE (sw-ROPE)** - позиционные эмбеддинги, нумерующие сессии
- Все обучение проходит за один этап (в отличие от ARGUS, где есть претрейн и файнтюн фазы)

#### Компоненты функции потерь

Функция потерь состоит из трех компонентов:

1. **Lnce** - воспроизводит логирующую политику, учит отличать реальные айтемы в сессии от случайных негативов
2. **Lclick** - отличает кликнутые айтемы от показанных
3. **Lpay** - отличает купленные айтемы от всех прочих

Все три компонента взвешиваются по числу сессий в соответствующих продуктовых сценариях.

## Особенности реализации

### Инкрементальное обучение

- Используется инкрементальное (incremental) обучение для регулярного обновления модели на свежих данных без перерасхода GPU
- Позволяет поддерживать актуальность модели без полной переобучения

### Инференс

- В продакшене модель работает не в реальном времени
- Кандидаты пересчитываются асинхронно и обновляются с небольшой задержкой
- Считается, что контекст пользователя меняется нечасто, поэтому схема не вредит качеству

### Масштабируемость

- Новый кандидат-генератор отвечает за 24% показов на поверхности "Guess You Like" - одной из ключевых страниц Taobao
- Архитектура позволяет эффективно масштабироваться на крупные e-commerce сценарии

### Работа с очень длинными последовательностями

На конференции CIKM'25 2025 была представлена работа, расширяющая TBGRecall, которая добавляет возможность работы с очень длинными последовательностями поведения пользователей (до 100,000 айтемов) в сжатом виде, чтобы учитывать долгосрочные интересы пользователей:

- **Сжатие длинных последовательностей**: Использование специальных методов для сжатия очень длинной истории поведения пользователей
- **Учёт долгосрочных интересов**: Возможность учитывать интересы пользователей, проявленные гораздо раньше, чем в предыдущих сессиях
- **Сессионный генеративный подход**: Применение генеративного подхода к предсказанию сессий с учётом сжатой долгосрочной истории

## Результаты

### Экспериментальные результаты

- На закрытом датасете (около 2 трлн записей) TBGRecall превзошёл собственный baseline с двумя башнями
- В A/B-тестах модель показала:
  - +0.5% по числу транзакций
  - +2% по обороту

### Промышленное внедрение

- Модель успешно внедрена в Taobao
- Значительное улучшение ключевых метрик
- Демонстрирует эффективность генеративного подхода в e-commerce рекомендациях

## Сравнение с другими подходами

| Аспект | TBGRecall | Классические двухбашенные | LLM-рекомендации |
|--------|-----------|---------------------------|------------------|
| Подход к обучению | Генеративный (авторегрессионный) | Контрастивное обучение | Контекстуальное понимание |
| Обработка сессий | Next-session prediction | Последовательности | Языковое моделирование |
| Использование контекста | Контекстные токены | Ограниченное | Глубокое понимание |
| Инкрементальное обучение | + | Часто нет | Часто нет |

## Связи с другими темами

- [[candidate_generation.md]] - Генерация кандидатов: основной этап, на котором используется TBGRecall
- [[session_based_recommendations.md]] - Сессионные рекомендательные системы: основа архитектуры TBGRecall
- [[sw_rope.md]] - Session-wise ROPE: ключевая техническая особенность TBGRecall
- [[transformer_based_models.md]] - Трансформерные модели: TBGRecall использует HSTU блоки
- [[traditional_approaches.md]] - Традиционные подходы: контекст для сравнения с классическими методами
- [[llm_based/main.md]] - LLM-базированные рекомендательные системы: альтернативный путь развития
- [[continual_learning/catastrophic_forgetting/catastrophic_forgetting.md]] - Проблема забывания: решаемая через инкрементальное обучение
- [[optimization/incremental_learning.md]] - Инкрементальное обучение: ключевая особенность TBGRecall
- [[cikm_25.md]] - CIKM'25: новая информация о TBGRecall и схожих подходах с ultra-long behavior sequence

## Источники

1. [TBGRecall: Generative Retrieval for Recommendations] - оригинальная статья о TBGRecall от Alibaba, описывающая генеративный подход к рекомендательным системам в e-commerce
2. [CIKM'25 Conference Report: TBGRecall Extension] - статья о расширении TBGRecall для работы с ultra-long behavior sequence, представленная на конференции CIKM'25