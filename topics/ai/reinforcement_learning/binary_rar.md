# Binary RAR - Обучение с подкреплением с бинарным вознаграждением

## Описание

Binary Retrieval-Augmented Reward (Binary RAR) - это метод онлайн-обучения с подкреплением, предназначенный для снижения галлюцинаций в языковых моделях. В отличие от традиционных подходов, использующих непрерывные функции вознаграждения, Binary RAR использует строгую бинарную систему: модель получает вознаграждение 1 только за полностью фактологически корректные ответы, и 0 при любом обнаруженном противоречии.

## Методология

### Онлайн-обучение

Binary RAR реализует подход онлайн-обучения с подкреплением, при котором модель постоянно дообучается на новых ответах, которые она сама генерирует. Это позволяет модели постепенно учиться на собственных ошибках и адаптировать свою политику в реальном времени.

### Бинарная функция вознаграждения

Вместо непрерывного значения, система использует строгий бинарный сигнал:

$$r(x,y) = \begin{cases} 1 & \text{если между } (x, y) \text{ и найденными доказательствами нет противоречий} \\ 0 & \text{в противном случае} \end{cases}$$

### Оптимизация с ограничением KL-дивергенции

Целевая функция оптимизации:

$$\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)} [r(x, y) - \beta D_{KL}(\pi_\theta(\cdot | x) || \pi_{\text{ref}}(\cdot | x))]$$

где член $D_{KL}$ действует как "поводок", ограничивая отклонение новой модели от исходной базовой модели.

## Архитектура системы

1. **Генерация** - Модель генерирует ответ
2. **Извлечение** - BM25 ретривер извлекает релевантные документы
3. **Проверка** - Мощная модель-верификатор проверяет соответствие ответа документам
4. **Награда** - Бинарная оценка на основе проверки

## Алгоритм

Используется алгоритм Group Relative Policy Optimization (GRPO) для эффективной оптимизации политики модели.

## Преимущества

### 1. Устойчивость к взлому вознаграждения

Бинарная природа награды предотвращает попытки модели использовать стилистические уловки или генерировать многословные, но неинформативные ответы для получения вознаграждения.

### 2. Эмерджентное поведение

Строгая система наград поощряет развитие полезных навыков:
- Калиброванный отказ от ответа ("Я не знаю") при неуверенности
- Избирательная фильтрация для повышения точности при сохранении информативности

### 3. Сохранение полезности

В отличие от методов с непрерывным вознаграждением, Binary RAR сохраняет важные способности модели, такие как следование инструкциям и рассуждения.

## Экспериментальные результаты

- 39.3% снижение уровня галлюцинаций в задачах генерации длинных текстов
- Сохранение уровня полезности (62.2 балла для Qwen3-8B после обучения, по сравнению с 61.6 базовой модели)
- Значительное снижение неверных ответов в задачах "вопрос-ответ"

## Сравнение с другими подходами

| Метод | Основной подход | Преимущества | Недостатки |
|-------|------------------|---------------|-------------|
| Supervised Fine-Tuning (SFT) | Обучение на размеченных данных | Простота реализации | Может ухудшить полезность при снижении галлюцинаций |
| Direct Preference Optimization (DPO) | Оптимизация на основе предпочтений | Обходит сложность модели наград | Может не полностью решать проблему галлюцинаций |
| RL с непрерывными наградами | Использование метрик типа VeriScore | Плавная оптимизация | Уязвим к reward hacking |
| **Binary RAR** | **Бинарная оценка с проверкой по документам** | **Сильное снижение галлюцинаций + сохранение полезности** | **Зависимость от качества ретривера и верификатора** |

## Технические особенности

- **Онлайн обучение**: Модель обучается на собственных ответах в реальном времени
- **Извлечение-проверка**: Сравнение с внешними источниками для проверки достоверности
- **Ограничение KL-дивергенции**: Сохранение исходных способностей модели
- **Гиперпараметр β**: Контроль степени отклонения от исходной модели

## Применение

Binary RAR особенно эффективен в задачах, где критически важна достоверность информации, такие как:
- Генерация контента с проверкой фактов
- Ответы на вопросы, требующие точной информации
- Документирование и аналитика

## Связи с другими темами

- [[laser_reinforcement_learning.md]] - Другой метод RL для улучшения рассуждений
- [[../llm/hallucination_detection/binary_rar.md]] - Подробное описание метода Binary RAR
- [[self_proposed_rubrics.md]] - Другой подход к обучению с подкреплением