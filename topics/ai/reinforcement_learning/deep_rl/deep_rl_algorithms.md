# Алгоритмы глубокого обучения с подкреплением

## Краткое описание

Алгоритмы глубокого обучения с подкреплением (Deep Reinforcement Learning, DRL) объединяют классические методы обучения с подкреплением с глубокими нейронными сетями для решения задач с высокоразмерными пространствами состояний и действий. Эти алгоритмы используют нейронные сети для аппроксимации функций ценности, стратегий или моделей окружающей среды.

## Основная информация

В отличие от табличных методов, DRL способен обрабатывать непрерывные и высокоразмерные пространства состояний, такие как изображения, что делает его применимым к широкому спектру задач, включая игры, робототехнику и автономное управление.

## Методы на основе ценности (Value-based methods)

### Deep Q-Network (DQN)

DQN - первый успешный алгоритм DRL, который сочетает Q-обучение с глубокими нейронными сетями.

**Ключевые особенности:**
- Использует нейронную сеть для аппроксимации Q-функции
- Experience Replay для стабилизации обучения
- Target Network для стабилизации целевых значений

**Формула обновления:**
```
L(θ) = E[(r + γ max_a Q(s', a'; θ^−) − Q(s, a; θ))^2]
```

**Преимущества:**
- Первый алгоритм, достигший суперчеловеческих результатов в Atari играх
- Простота концепции

**Ограничения:**
- Применим только к дискретным пространствам действий
- Овероценивание Q-значений

### Double DQN (DDQN)

DDQN решает проблему овероценивания в DQN, используя две отдельные сети для выбора и оценки действия.

**Преимущества:**
- Уменьшение смещения оценки
- Более стабильное обучение

### Dueling DQN

Дueling DQN разделяет Q-сеть на две ветви: V (ценность состояния) и A (преимущество действия).

**Преимущества:**
- Лучшее понимание ценности состояний
- Улучшенная сходимость на некоторых задачах

## Методы на основе стратегии (Policy-based methods)

### REINFORCE

REINFORCE - это один из основных алгоритмов на основе стратегии, который напрямую оптимизирует параметры стратегии.

**Формула градиента:**
```
∇J(θ) = E[∇_θ log π(a|s;θ) G_t]
```

**Преимущества:**
- Работает с непрерывными пространствами действий
- Может изучать стохастические стратегии
- Устойчивость к локальным минимумам

**Ограничения:**
- Высокая дисперсия градиентов
- Медленная сходимость

## Actor-Critic методы

### Deep Deterministic Policy Gradient (DDPG)

DDPG - это детерминированный метод на основе стратегии, подходящий для непрерывных пространств действий.

**Ключевые особенности:**
- Комбинация Actor (политика) и Critic (Q-функция)
- Использует эксплуатационную политику (exploitation policy)
- Replay buffer и target networks

**Преимущества:**
- Подходит для задач с непрерывными действиями
- Детерминированная политика для точного управления

**Ограничения:**
- Чувствителен к гиперпараметрам
- Может застревать в локальных минимумах

### Twin Delayed DDPG (TD3)

TD3 улучшает DDPG, используя трюки для смягчения проблемы овероценивания.

**Три ключевых улучшения:**
- Использование двух критиков (twin Q-functions)
- Замедленное обновление политики (delayed policy updates)
- Трение на действиях (target policy smoothing)

### Soft Actor-Critic (SAC)

SAC - это максимальный энтропийный алгоритм, обеспечивающий стабильность и эффективность выборки.

**Преимущества:**
- Максимальная энтропия для стабильности и разведки
- Офлайн обучение
- Хорошая эффективность выборки

### Proximal Policy Optimization (PPO)

PPO - это один из самых популярных алгоритмов DRL, обеспечивающий стабильность обучения.

**Две версии:**
- PPO-clip: ограничение отношения вероятностей
- PPO-penalty: штраф за отклонение стратегии

**Преимущества:**
- Простота реализации
- Универсальность
- Хорошая стабильность обучения

**Формула PPO-clipped:**
```
L^{PPO} = E[min(r_t(θ) A_t, clip(r_t(θ), 1-ε, 1+ε) A_t)]
```

## Асинхронные методы

### A3C (Asynchronous Advantage Actor-Critic)

A3C использует несколько агентов, обучающихся асинхронно в параллельных средах.

**Преимущества:**
- Эффективное использование вычислительных ресурсов
- Естественная разведка через разные начальные условия

## Сравнение алгоритмов

| Алгоритм | Тип стратегии | Пространство действий | Эффективность выборки | Стабильность | Примечания |
|----------|---------------|----------------------|----------------------|--------------|------------|
| DQN | Value-based | Дискретное | Низкая | Высокая | Experience replay улучшает |
| DDPG | Deterministic | Непрерывное | Средняя | Низкая | Чувствителен к гиперпараметрам |
| TD3 | Deterministic | Непрерывное | Средняя | Средняя | Улучшенный DDPG |
| SAC | Stochastic | Непрерывное | Высокая | Высокая | Максимальная энтропия |
| PPO | Stochastic | Любой | Средняя | Высокая | Самый стабильный |

## Практические вызовы и проблемы

### Сходимость и стабильность
- Нестабильность из-за взаимодействия между аппроксимацией функции и онлайн-обучением
- Зависимость от инициализации и гиперпараметров
- Колебания в процессе обучения

### Эффективность выборки
- Необходимость большого количества взаимодействий со средой
- Проблема переноса (transfer) знаний между задачами
- Эффективность в реальных приложениях

### Теоретические гарантии
- Отсутствие строгих гарантий сходимости для глубоких методов
- Оценка качества конечных стратегий

## Рекомендации по выбору алгоритма

### Для дискретных пространств действий:
- **DQN и его модификации** - для задач с изображениями
- **Rainbow DQN** - комбинация улучшений DQN (Double, Dueling, Prioritized Experience Replay и др.)

### Для непрерывных пространств действий:
- **PPO** - лучший выбор для начального подхода (самый стабильный)
- **SAC** - если важна эффективность выборки
- **TD3** - если нужна точность и допустима тонкая настройка

### Для задач с высокими требованиями к стабильности:
- **PPO** - самая стабильная работающая модель

### Для задач с ограничениями по выборке:
- **SAC** - максимальная эффективность выборки

## Применения DRL

- Игровые агенты (например, Atari, Go, Dota 2, StarCraft II)
- Робототехника и управление манипуляторами
- Финансовое моделирование и автоматическая торговля
- Управление ресурсами и оптимизация систем
- Автономное вождение

## Перспективы и будущие направления

- Обучение с подкреплением с ограниченным обучением (offline RL)
- [[../../meta_learning/meta_rl.md|Мета-обучение и обучение для нескольких задач]] - обучение обучению, позволяющее быструю адаптацию к новым задачам
- Объединение DRL с методами обучения с учителем и без учителя
- Обучение с подкреплением с человеческой обратной связью (RLHF)

## Связи с другими темами

- [[../index.md]] - Введение в обучение с подкреплением
- [[../survey_rl_comprehensive.md]] - Обзор RL от алгоритмов к практическим вызовам
- [[../../llm/rlhf.md]] - Обучение с подкреплением с человеческой обратной связью, применение DRL в выравнивании LLM
- [[../../machine_learning/algorithms/neural_networks.md]] - Нейронные сети, используемые в DRL
- [[../practical_challenges/exploration_vs_exploitation.md]] - Проблема разведки и эксплуатации в контексте глубоких методов
- [[../practical_challenges/exploration_exploitation.md]] - Практические вызовы, с которыми сталкиваются DRL алгоритмы
- [[ppo_algorithm.md]] - Подробное описание PPO
- [[../../llm/reasoning/sft_rlvr_methodology.md]] - Современные методы RL для улучшения рассуждений
- [[../fundamentals/tabular_rl_methods.md]] - Фундаментальные методы, на которых основаны глубокие подходы
- [[laser_reinforcement_learning.md]] - Современные методы глубокого RL для LLM

## Ссылки на источники

- arXiv:2411.18892v2 - "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges"
- Lillicrap, T. P., et al. (2015). Continuous control with deep reinforcement learning
- Haarnoja, T., et al. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
- Schulman, J., et al. (2017). Proximal Policy Optimization Algorithms