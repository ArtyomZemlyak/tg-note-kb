# Практический курс обучения с подкреплением от Yandex School of Data Analysis

## Введение

Практический курс RL (Reinforcement Learning) от Yandex School of Data Analysis (YSDA) и НИУ ВШЭ - это открытый курс по обучению с подкреплению, который делает упор на практическое применение методов RL в реальных задачах. Курс сочетает теоретические основы с практической реализацией в Jupyter-тетрадях, используя OpenAI Gym и методы глубокого RL.

## Основные особенности курса

### Практический подход
- Акцент на практическую реализацию, а не только на теорию
- Интенсивный опыт работы с OpenAI Gym
- Работа с реальными задачами и средами

### Структура курса
Курс организован в виде еженедельных модулей, каждый из которых включает:
- Лекционную часть с изучением теоретических концепций
- Семинар с практической реализацией
- Домашние задания с подробными README-файлами
- Лабораторные работы по программированию

### Многоязычность
- Курс преподается как на английском, так и на русском языках
- Доступен как для очного обучения в НИУ ВШЭ и YSDA, так и для онлайн-студентов

## Темы курса

### Неделя 1: Введение
- Проблемы RL
- Процессы принятия решений
- Стохастическая оптимизация
- Метод перекрестной энтропии (Cross-entropy method)

### Неделя 2: Методы на основе ценности
- Дисконтированная награда в MDP
- Итерация по ценности (value iteration)
- Итерация по политике (policy iteration)

### Неделя 3: Модель-свободный RL
- Q-обучение
- SARSA
- TD(Lambda)
- Внеполитикные (off-policy) и внутреполитикные (on-policy) алгоритмы

### Неделя 4: Повторение глубокого обучения
- Введение в PyTorch/TensorFlow
- Сверточные сети
- Приближенное RL:
  - Аппроксимация функций
  - Воспроизведение опыта (experience replay)
  - Целевые сети
  - Варианты DQN

### Неделя 5: Исследование (Exploration)
- Контекстные бандиты (Contextual bandits)
- Образцовое моделирование Томпсона (Thompson Sampling)
- Верхняя доверительная граница (UCB)
- Дерево поиска Монте-Карло (MCTS)
- Эвристики исследования

### Неделя 6: Методы градиента политики
- REINFORCE
- Actor-Critic
- Уменьшение дисперсии
- Обобщенная оценка преимуществ (GAE)

### Неделя 7: Последовательные модели
- Рекуррентные нейронные сети (RNNs)
- Долгая краткосрочная память (LSTM)
- GRU (Gated Recurrent Unit)
- Обратное распространение по времени (backprop through time)

### Неделя 8: Частично наблюдаемые MDP
- Агенты с памятью
- POMCP (Partially Observable Monte Carlo Planning)
- Рекуррентный RL

### Неделя 9: Продвинутые методы на основе политики
- TRPO (Trust Region Policy Optimization)
- PPO (Proximal Policy Optimization)
- DDPG (Deep Deterministic Policy Gradient)
- Детерминированные градиенты политики

### Неделя 10: Модель-базированный RL
- Планирование
- Обучение по подражанию (Imitation Learning)
- Инверсный RL

## Уникальные аспекты

### Подход "практика прежде всего"
- Упор на практическую реализацию и руки-он-экспириенс
- Каждая большая концепция сопровождается лабораторной работой для практического опыта

### Курс на основе Git
- Сообщество активно вносит вклад и улучшения через pull request'ы
- 79+ участников внесли вклад в курс
- Открыт для коммитов от сообщества

### Поддержка нескольких фреймворков
- Использует PyTorch и TensorFlow
- Практические упражнения в Jupyter-тетрадях

### Интерактивное обучение
- Взаимодействие с каждым концептом через практические лабораторные работы
- Интеграция с Google Colab для онлайн-студентов

## Связи с другими темами

- [[../../ai/reinforcement_learning/deep_rl/deep_rl_algorithms.md]] - Глубокие RL алгоритмы, расширяющие основы из курса
- [[ppo_algorithm.md]] - PPO, один из продвинутых методов, изучаемых в курсе
- [[rl_frameworks_pytorch.md]] - PyTorch как основа для реализации методов
- [[../../ai/reinforcement_learning/survey_rl_comprehensive.md]] - Обзор RL методов, расширяющий темы курса
- [[../../ai/reinforcement_learning/fundamentals/tabular_rl_methods.md]] - Фундаментальные методы RL