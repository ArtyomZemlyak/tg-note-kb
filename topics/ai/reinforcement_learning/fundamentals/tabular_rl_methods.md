# Табличные методы обучения с подкреплением

## Краткое описание

Табличные методы обучения с подкреплением представляют собой базовые алгоритмы, в которых значения состояний и/или действий хранятся в явной таблице. Эти методы подходят для задач с небольшим числом состояний и действий, когда можно эффективно хранить и обновлять значения для каждого состояния или пары состояние-действие.

## Основная информация

Табличные методы лежат в основе всех современных подходов к обучению с подкреплением. Они обеспечивают теоретическую основу и понимание основных концепций RL, таких как сходимость, разведка и эксплуатация, а также обновление значений.

## Ключевые алгоритмы

### Q-Learning

Q-Learning - это алгоритм обучения с подкреплением, основанный на оценке ценности действий. Он стремится изучить функцию Q(s, a), которая представляет ожидаемую полезность действия a в состоянии s.

**Формула обновления:**
```
Q(s_t, a_t) = Q(s_t, a_t) + α[r_{t+1} + γ max_a Q(s_{t+1}, a) - Q(s_t, a_t)]
```

**Преимущества:**
- Офлайн (off-policy) обучение
- Гарантированная сходимость к оптимальной стратегии
- Простота реализации

**Ограничения:**
- Работает только с дискретными пространствами состояний и действий
- Не масштабируется до больших пространств

### SARSA (State-Action-Reward-State-Action)

SARSA - это on-policy алгоритм, который обучается на основе текущей стратегии, а не стремится к оптимальной стратегии.

**Формула обновления:**
```
Q(s_t, a_t) = Q(s_t, a_t) + α[r_{t+1} + γ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
```

**Преимущества:**
- Стабильность обучения
- Применим к задачам, где политика влияет на результат

**Ограничения:**
- Медленная сходимость
- Требует осторожного управления разведкой

### Monte Carlo методы

Monte Carlo методы изучают значения, используя полные эпизоды и суммы вознаграждений.

**Преимущества:**
- Нет смещения в оценке
- Простота понимания
- Применимы к задачам с разреженными наградами

**Ограничения:**
- Требуют завершения эпизода
- Высокая дисперсия
- Медленная сходимость

### Временные разностные (Temporal Difference) методы

TD-методы сочетают идеи Monte Carlo и методов динамического программирования, обновляя оценки на основе других оценок (bootstrapping).

**TD(0) формула:**
```
V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]
```

## Проблемы и вызовы табличных методов

### Проблема размерности
- Табличные методы не масштабируются до задач с большим или непрерывным пространством состояний
- Требуется экспоненциальное количество памяти для хранения таблиц

### Сходимость
- Гарантированная сходимость только при определенных условиях
- Зависимость от частоты посещения состояний
- Необходимость баланса между разведкой и эксплуатацией

### Эффективность выборки
- Неэффективное использование данных
- Зависимость от порядка обновления

## Практические рекомендации

### Когда использовать табличные методы
- Пространство состояний и действий невелико
- Требуется гарантия сходимости
- Необходимо понимание основных концепций RL
- В обучающих целях для демонстрации RL принципов

### Улучшения табличных методов
- Введение эвристик для инициализации значений
- Использование eligibility traces (TD(λ))
- Улучшенные стратегии разведки (например, UCB)

## Связи с другими темами

- [[../index.md]] - Общее введение в обучение с подкреплением
- [[../survey_rl_comprehensive.md]] - Обзор RL, включающий фундаментальные методы
- [[../deep_rl/deep_rl_algorithms.md]] - Глубокие методы, которые расширяют табличные подходы
- [[../practical_challenges/exploration_vs_exploitation.md]] - Проблема разведки и эксплуатации, ключевая концепция в табличных методах
- [[../../machine_learning/algorithms/dynamic_programming.md]] - Динамическое программирование, методы которого используются в табличных RL
- [[../../llm/reasoning/sft_rlvr_methodology.md]] - Использование фундаментальных RL концепций в современных методах

## Ссылки на источники

- arXiv:2411.18892v2 - "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges"
- Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction