# Многоагентное обучение с подкреплением (Multi-Agent Reinforcement Learning, MARL)

## Краткое описание

**Многоагентное обучение с подкреплением (MARL)** — это подраздел машинного обучения, изучающий, как несколько агентов могут обучаться совместно в общей среде. В отличие от традиционного RL, где один агент взаимодействует с фиксированной средой, в MARL каждый агент воспринимает других агентов как часть окружающей среды, что создает проблемы нестационарности и требует новых теоретических и вычислительных подходов.

## Основная информация

MARL представляет собой фундаментальную сложность в области искусственного интеллекта, поскольку каждый агент должен учитывать, что поведение других агентов также изменяется по мере их обучения. Это создает динамическую и нестационарную среду, даже если физическая среда остается постоянной.

### Основные вызовы

1. **Нестационарность среды**: Поскольку другие агенты учатся, среда, в которой действует каждый агент, постоянно меняется.
2. **Проблема масштабирования**: Пространства состояний и действий растут экспоненциально с количеством агентов.
3. **Большое количество взаимодействий**: Требуется гораздо больше взаимодействий для обучения по сравнению с одиночным агентом.
4. **Проблема координации**: Агенты должны координировать свои действия для достижения общих или частично совпадающих целей.

## Типы MARL-сред

### По цели обучения
- **Координация**: Агенты стремятся максимизировать общую награду
- **Конкуренция**: Агенты имеют противоположные цели (например, игры с нулевой суммой)
- **Коалиции**: Часть агентов сотрудничает против других

### По информации
- **Полностью наблюдаемые**: Все агенты видят полное состояние
- **Частично наблюдаемые**: Агенты имеют доступ только к локальным наблюдениям
- **Центральная тренировка, децентрализованное выполнение**: Обучаются централизованно, но действуют децентрализованно

## Основные подходы

### Централизованные методы
- Объединяют наблюдения и действия всех агентов для обучения
- Обеспечивают стабильность обучения за счет стационарности
- Страдают от экспоненциального роста пространства действий

### Децентрализованные методы
- Каждый агент обучается независимо
- Более масштабируемы, но менее стабильны
- Требуют методов координации без централизованной информации

### Гибридные методы
- Используют централизованное обучение с децентрализованным выполнением
- Сочетают стабильность с масштабируемостью
- Примеры: MADDPG, QMIX, VDN

## Ключевые алгоритмы

### MADDPG (Multi-Agent Deep Deterministic Policy Gradient)
- Обучает непрерывное управление с централизованным критиком
- Использует центральные наблюдения для обучения, но децентрализованное выполнение
- Эффективно в средах с непрерывными действиями

### QMIX (Monotonic Value Function Factorisation)
- Независимо обучает каждого агента, позволяя децентрализованное выполнение
- Использует монотонную факторизацию для объединения локальных Q-значений
- Эффективен в кооперативных задачах

### COMA (Counterfactual Multi-Agent Policy Gradients)
- Использует централизованный критик с децентрализованным актором
- Вводит counterfactual baseline для уменьшения дисперсии градиентов
- Хорошо работает в кооперативных и частично кооперативных средах

## Теоретические аспекты

### Равновесия Нэша
- В MARL часто рассматриваются равновесия как стабильные решения
- Игра стремится к равновесию Нэша при условии сходимости
- Проблема: может быть несколько равновесий, и не все из них оптимальны

### Проблема зерна истины
- Агент должен учитывать возможность существования копии своего собственного алгоритма
- Ведет к рекурсивным рассуждениям о поведении других агентов
- Решается через встроенные подходы (см. [[embedded_universal_predictive_intelligence.md]])

## Современные подходы и решения

### Встроенное обучение с подкреплением
- Подходы, где агенты моделируют себя как часть совместной вселенной
- Решает проблемы рекурсии и зерна истины
- Пример: MUPI (Embedded Universal Predictive Intelligence)

### Теория ума в MARL
- Агенты развивают способность предсказывать и моделировать других агентов
- Используется для более эффективной координации и сотрудничества
- Позволяет строить более точные модели поведения других агентов

### Методы коммуникации
- Обучение коммуникации между агентами
- Использование сигнальных протоколов для координации
- Примеры: CommNet, BiCNet, TarMAC

## Применения

### Игры и симуляции
- ИИ для сложных стратегических игр (Dota 2, StarCraft II)
- Моделирование социальных взаимодействий
- Виртуальные среды с несколькими агентами

### Робототехника
- Координация групп роботов
- Распределенные системы управления
- Многороботная навигация и манипуляции

### Транспорт и логистика
- Управление трафиком для автономных транспортных средств
- Оптимизация логистических сетей
- Системы умного города

### Экономические системы
- Моделирование рынков и торговли
- Аукционы и механизмы принятия решений
- Распределение ресурсов

## Связи с другими темами

- [[embedded_universal_predictive_intelligence.md]] - Современные встроенные подходы к MARL
- [[reinforcement_learning/index.md]] - Основы обучения с подкреплением
- [[game_theory.md]] - Теоретические основы равновесий в MARL
- [[multiagent_systems.md]] - Более широкая область многоагентных систем
- [[coordination_in_multiagent_systems.md]] - Методы координации между агентами
- [[theory_of_mind_in_ai.md]] - Понимание поведения других агентов

## Будущее развитие

- Разработка более стабильных и эффективных обучаемых алгоритмов
- Интеграция с современными ИИ-моделями (LLM, трансформеры)
- Решение проблемы масштабирования до тысяч агентов
- Применение к реальным системам управления

## Источники

1. Busoniu, L., Babuska, R., & De Schutter, B. (2008). A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2), 156-172.
2. Zhang, K., Yang, Z., & Başar, T. (2019). Multi-agent reinforcement learning: A selective overview of theories and algorithms. arXiv preprint arXiv:1911.10635.
3. Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., & Mordatch, I. (2017). Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in Neural Information Processing Systems, 30.
4. Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., & Whiteson, S. (2018). Qmix: Monotonic value function factorisation for deeply cooperative multi-agent reinforcement learning. International Conference on Machine Learning, 4295-4304.
5. OpenAI et al. (2019). Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680.