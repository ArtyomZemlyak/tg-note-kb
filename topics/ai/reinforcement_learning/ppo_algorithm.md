# PPO (Proximal Policy Optimization) - Алгоритм

## Определение

Proximal Policy Optimization (PPO) - это алгоритм обучения с подкреплением, разработанный для решения задач оптимизации политики в глубоком обучении с подкреплением. PPO был представлен OpenAI как эффективная и стабильная альтернатива другим алгоритмам градиента политики, таким как TRPO (Trust Region Policy Optimization).

## История и контекст

- Авторы: John Schulman и др. (2017)
- Введено как упрощенная альтернатива TRPO
- Стремится достичь схожих гарантий устойчивости при более простой реализации

## Основные идеи

### Проблема с традиционными методами градиента политики

Традиционные методы градиента политики сталкиваются с проблемой:
- Слишком большие обновления политики могут дестабилизировать обучение
- Это особенно проблематично при использовании глубоких нейронных сетей

### Решение PPO

PPO вводит ограничение на размер обновлений политики. Основные подходы:

#### PPO-Clip (самый распространенный вариант)
- Использует clipping-функцию для ограничения обновлений политики
- Не позволяет политике слишком сильно отклоняться от предыдущей версии
- Формула: min(A_t * ratio, clip(A_t, 1-ε, 1+ε))

#### PPO-Penalty
- Использует штрафной метод (KL divergence) для ограничения обновлений
- Менее распространен, чем PPO-Clip

## Архитектура

- Использует Actor-Critic архитектуру
- Актер (Actor): нейронная сеть, которая определяет политику
- Критик (Critic): нейронная сеть, которая оценивает функцию ценности

## Преимущества PPO

1. **Стабильность**: Ограниченные обновления обеспечивают более стабильное обучение
2. **Простота реализации**: Проще, чем TRPO
3. **Эффективность**: Хорошая эффективность выборки
4. **Универсальность**: Работает как для дискретных, так и для непрерывных пространств действий
5. **Широкое применение**: Стандартный выбор для многих задач Deep RL

## Применения

- Обучение роботов манипуляциям
- Игровые агенты
- Автономные транспортные средства
- Оптимизация систем
- [[../../ai/llm/rlhf.md]] - Использование в RLHF для выравнивания LLM

## Варианты и расширения

- **PPO с GAE** (Generalized Advantage Estimation)
- **PPO с RNN** для частично наблюдаемых сред
- **Multi-step PPO** с более сложными схемами обновлений

## Сравнение с другими алгоритмами

| Алгоритм | Сложность | Стабильность | Эффективность выборки |
|----------|-----------|--------------|----------------------|
| TRPO     | Высокая   | Высокая      | Высокая              |
| **PPO**  | Низкая    | Высокая      | Высокая              |
| A2C/A3C  | Низкая    | Низкая       | Средняя              |
| DDPG/TD3 | Средняя   | Средняя      | Высокая              |
| SAC      | Средняя   | Высокая      | Высокая              |

## См. также

- [[../deep_rl/huggingface_deep_rl_course.md]] - PPO в курсе Hugging Face
- [[../deep_rl/yandex_practical_rl_course.md]] - PPO в курсе Yandex
- [[../deep_rl/introduction_deep_rl.md]] - Введение в Deep RL с упоминанием PPO
- [[../../ai/llm/group_relative_policy_optimization.md]] - GRPO, эффективный вариант PPO
- [[../survey_rl_comprehensive.md]] - Обзор RL методов с упоминанием PPO
- [[deep_rl_algorithms.md]] - Глубокие RL алгоритмы