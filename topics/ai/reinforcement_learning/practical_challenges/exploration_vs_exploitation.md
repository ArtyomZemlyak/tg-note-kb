# Проблема разведки и эксплуатации в обучении с подкреплением

## Краткое описание

Проблема разведки и эксплуатации (exploration-exploitation dilemma) - это фундаментальная проблема в обучении с подкреплением, заключающаяся в необходимости балансировать между изучением новых действий (разведка) и использованием уже известной информации для получения вознаграждения (эксплуатация). Агент должен решить, стоит ли ему продолжать использовать действия, которые он знает как успешные, или попробовать новые действия, которые могут дать лучшие результаты в долгосрочной перспективе.

## Основная информация

Проблема разведки и эксплуатации возникает потому, что агент не имеет априорной информации о структуре среды и должен учиться на основе полученного опыта. Это требует компромисса между:
- **Эксплуатацией** - использованием текущих знаний для получения вознаграждения
- **Разведкой** - изучением новых действий, которые могут привести к лучшим результатам

## Классические стратегии разведки

### ε-жадность (ε-greedy)

Самый простой подход, при котором агент выбирает оптимальное действие с вероятностью (1-ε) и случайное действие с вероятностью ε.

**Преимущества:**
- Простота реализации
- Гарантированное изучение всех действий при ε > 0

**Ограничения:**
- Случайные действия неэффективны
- Требует тонкой настройки параметра ε

### Верхняя доверительная граница (UCB - Upper Confidence Bound)

UCB учитывает как оценку действия, так и степень его изученности, выбирая действия с высоким потенциалом.

**Формула UCB:**
```
A_t = argmax_a [Q_t(a) + c * sqrt(ln(t) / N_t(a))]
```
где:
- Q_t(a) - оценка ценности действия a на время t
- N_t(a) - количество раз, которое действие a было выбрано
- c - параметр, регулирующий компромисс между разведкой и эксплуатацией

**Преимущества:**
- Более эффективная разведка
- Адаптивность к степени изученности

**Ограничения:**
- Сложность расширения на высокоразмерные пространства

### Софтмакс (Boltzmann Exploration)

Использует вероятности, пропорциональные экспоненте Q-значений, для выбора действий.

**Формула:**
```
P(A_t = a) = exp(Q_t(a) / τ) / Σ_b exp(Q_t(b) / τ)
```
где τ (температура) регулирует уровень разведки.

## Современные методы разведки

### Внутренняя мотивация (Intrinsic Motivation)

Методы внутренней мотивации вознаграждают агента за изучение новых или неожиданных состояний.

**Подходы:**
- **Count-based exploration** - вознаграждение за посещение редких состояний
- **Prediction error** - вознаграждение за неожиданные наблюдения
- **Information gain** - вознаграждение за уменьшение неопределенности

### Curiosity-driven exploration

Использует модели для предсказания динамики среды и вознаграждает за неожиданные результаты.

**Компоненты:**
- **Forward model** - предсказывает следующее состояние по текущему состоянию и действию
- **Feature space** - представление состояний в пространстве признаков
- **Intrinsic reward** - разница между предсказанным и реальным состоянием

### Энтропийная регуляризация

Добавляет энтропийный штраф к функции вознаграждения для поощрения стохастичности политики.

**Формула:**
```
L = -E[log π(a|s) * A(s,a) - β * H(π(.|s))]
```
где H(π(.|s)) - энтропия политики в состоянии s.

## Для глубоких методов

### Noisy Networks

Добавляет параметрический шум к нейронной сети для разведки, обучая шум как часть параметров.

**Преимущества:**
- Эффективная разведка в глубоких сетях
- Учет корреляций между действиями

### Bootstrapped DQN

Обучает несколько Q-сетей с разными целями разведки, что позволяет эффективно изучать несколько стратегий одновременно.

### Thompson Sampling

Использует байесовский подход для моделирования неопределенности в оценках Q-значений.

## Особенности в непрерывных пространствах действий

В непрерывных пространствах действиях традиционные методы разведки менее эффективны:

- **Parameter space noise** - добавление шума к параметрам стратегии
- **Action space noise** - добавление шума к выбранному действию
- **Variational exploration** - использование вариационных методов для моделирования неопределенности

## Сравнение методов разведки

| Метод | Эффективность | Сложность | Применимость | Примечания |
|-------|---------------|-----------|--------------|------------|
| ε-жадность | Низкая | Низкая | Любой | Прост, но неэффективен |
| UCB | Высокая | Средняя | Дискретные | Хорошо теоретически обоснован |
| Софтмакс | Средняя | Низкая | Любой | Гибкий параметр разведки |
| Внутренняя мотивация | Высокая | Высокая | Любой | Хорош в средах с разреженными наградами |
| Энтропийная регуляризация | Средняя | Низкая | Любой | Встроена в обучение |

## Практические рекомендации

### Выбор метода разведки:
1. **Для простых задач** - ε-жадность или энтропийная регуляризация
2. **Для задач с разреженными наградами** - внутренняя мотивация, curiosity-driven
3. **Для высокоразмерных задач** - Noisy Networks, энтропийная регуляризация
4. **Для гарантий разведки** - UCB методы

### Параметры настройки:
- **Уменьшение разведки со временем** - снижение уровня разведки по мере накопления знаний
- **Адаптивность к среде** - адаптация стратегии разведки к характеристикам среды
- **Баланс вычислительной сложности** - не использовать слишком сложные методы для простых задач

## Проблемы и ограничения

### В высокоразмерных пространствах:
- Экспоненциальный рост сложности
- Сложность оценки степени изученности
- Увеличение вычислительных требований

### В средах с разреженными наградами:
- Трудности в определении прогресса
- Затрудненная корреляция между разведкой и долгосрочным успехом
- Проблемы с установлением причинно-следственных связей

## Применения

- Игровые агенты в средах с частичной информацией
- Робототехника и исследование неизвестных сред
- Рекомендательные системы
- Персонализация контента
- Автономные системы принятия решений

## Связи с другими темами

- [[../index.md]] - Введение в обучение с подкреплением
- [[../survey_rl_comprehensive.md]] - Обзор RL от алгоритмов к практическим вызовам
- [[../deep_rl/deep_rl_algorithms.md]] - Алгоритмы DRL и их методы разведки
- [[../practical_challenges/exploration_exploitation.md]] - Общие практические вызовы RL, включая разведку-эксплуатацию
- [[../../machine_learning/theory/uncertainty_quantification.md]] - Квантификация неопределенности, используемая в методах разведки
- [[../../llm/rlhf.md]] - RLHF как пример применения разведки-эксплуатации в выравнивании LLM
- [[laser_reinforcement_learning.md]] - Современный метод, использующий самонаграждение как форму внутренней мотивации
- [[self_proposed_rubrics.md]] - Современный метод, использующий критерии для улучшения разведки в непроверяемых задачах

## Ссылки на источники

- arXiv:2411.18892v2 - "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges"
- Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction
- Oudeyer, P. Y., & Kaplan, F. (2007). What is intrinsic motivation?
- Plappert, M., et al. (2017). Parameter space noise for exploration