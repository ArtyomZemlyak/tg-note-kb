# Использование FP16 вместо BF16 для стабилизации RL-обучения

## Краткое описание

Исследование показывает, что использование FP16 вместо BF16 значительно стабилизирует обучение с подкреплением (RL), особенно для алгоритмов вроде GRPO и LLM-based RL. Это простое изменение точности обеспечивает более стабильное обучение, устраняет проблему дисбаланса между обучением и инференсом и позволяет использовать FP16 для продления и стабилизации RL-обучения.

## Основная информация

### Проблема
- **Training-inference mismatch**: Расхождение между поведением модели во время обучения и инференса
- **Нестабильность RL-обучения**: Тренировка RL-алгоритмов часто сталкивается с проблемами стабильности, особенно при использовании BF16
- **Проблемы с GRPO и LLM-based RL**: Эти методы особенно чувствительны к численной нестабильности

### Решение
- **Использование FP16 вместо BF16**: Просто переключение с BF16 на FP16 значительно стабилизирует обучение
- **Снижение дисбаланса обучение-инференс**: FP16 значительно уменьшает расхождение между обучением и инференсом, как показано в сравнениях распределения вероятностей на уровне токенов
- **Улучшение стабильности**: Основной алгоритм градиентного спуска с важностью взвешивания в FP16 превосходит все базовые методы в BF16

### Сравнение FP16 и BF16
- **FP16 (Float16)**: 16-битный формат с плавающей запятой, который обеспечивает большую чувствительность к изменениям градиентов
- **BF16 (Brain Float16)**: 16-битный формат, оптимизированный для глубокого обучения, но с меньшей точностью, чем FP16
- **Преимущества FP16 для RL**: Лучшая стабильность обучения, особенно для градиентных методов

### Экспериментальная валидация
- **Алгоритмы**: Проверено на различных алгоритмах (GRPO, GSPO, TIS, MIS, PG)
- **Модельные семейства**: Тестировано на разных модельных семействах (R1D, Qwen, OctoThinker)
- **Методы тонкой настройки**: Проверено с альтернативными методами тонкой настройки (LoRA)
- **Масштаб модели**: Валидировано на моделях большего масштаба (Dense-14B, MoE)
- **Фреймворки**: Эксперименты проводились в двух независимых фреймворках (VeRL и Oat)

## Новые концепции и термины

- **Training-inference mismatch**: Явление, при котором модель ведет себя по-разному во время обучения и инференса, что может привести к деградации производительности
- **Градиентная стабильность**: Стабильность вычислений градиентов, критичная для сходимости RL-алгоритмов
- **Дискретизация градиентов**: Точность, с которой могут быть представлены градиенты в различных форматах точности
- **Важность взвешивания (importance weighting)**: Метод, используемый в политических градиентных алгоритмах, где точность имеет значение
- **Токен-уровневое сравнение**: Сравнение вероятностных распределений на уровне отдельных токенов для оценки схожести обучения и инференса

## Примеры применения

- **GRPO (Gradient Regularized Policy Optimization)**: Алгоритм, который особенно выигрывает от использования FP16 для стабилизации градиентов
- **LLM-based RL**: Обучение с подкреплением на основе крупных языковых моделей, где численная стабильность критична
- **Важность взвешивания**: Метод, используемый в политических градиентных алгоритмах, где точность имеет значение
- **Токенизация и вероятностные распределения**: Использование FP16 улучшает согласованность между распределениями вероятностей при обучении и инференсе

## Практические рекомендации

- **Для GRPO и LLM-based RL**: Рекомендуется использовать FP16 вместо BF16 для более стабильного обучения
- **Фреймворки**: Решение применимо к различным фреймворкам (VeRL и Oat)
- **Архитектуры моделей**: Подходит для различных архитектур моделей, включая Dense-14B и MoE
- **Тонкая настройка**: Эффективно как с LoRA, так и с другими методами тонкой настройки

## Связи с другими темами

- [[../deep_rl/deep_rl_algorithms.md]] - Связь с глубокими RL алгоритмами, где точность вычислений важна
- [[../practical_challenges/exploration_vs_exploitation.md]] - Другие проблемы стабильности в RL
- [[../practical_challenges/exploration_exploitation.md]] - Обзор основных практических вызовов и методов стабилизации в RL, включая использование FP16 для улучшения стабильности
- [[../ppo_algorithm.md]] - PPO как пример алгоритма, возможного к оптимизации через точность
- [[../../llm/rlhf.md]] - RLHF как пример применения RL к LLM, где точность может быть важна
- [[../../optimization/gradient_clipping.md]] - Дополнительный метод стабилизации градиентов в RL

## Ссылки на источники

- Defeating the Training-Inference Mismatch via FP16: https://arxiv.org/abs/2510.26788
- https://www.alphaxiv.org/ru/overview/2510.26788v1
- https://github.com/sail-sg/Precision-RL
- Основанные на важности методы политических градиентов