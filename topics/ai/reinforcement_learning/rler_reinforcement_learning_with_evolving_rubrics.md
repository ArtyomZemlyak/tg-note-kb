# RLER: Обучение с подкреплением с эволюционирующими критериями (Reinforcement Learning with Evolving Rubrics)

## Описание

RLER (Reinforcement Learning with Evolving Rubrics) - это метод обучения с подкреплением, разработанный для задач, которые не поддаются стандартным методам обучения с подкреплением, особенно для задач, связанных с генерацией длинных текстов и глубокими исследованиями. Метод решает проблему оценки открытых, многоступенчатых задач, где нет одного "правильного" ответа для проверки.

## Контекст и проблема

Стандартные подходы обучения с подкреплением, такие как RLVR (Reinforcement Learning from Verifiable Rewards), эффективны для задач с четкими критериями успеха (например, математические задачи с проверяемыми ответами, задачи программирования с результатом pass/fail). Однако этот подход затруднен в задачах с открытым концом, таких как глубокие исследования, где нет одного четкого критерия оценки.

Использование общего домена LLM в качестве судьи, который дает общую обратную связь по полезности или качеству письма, обеспечивает удобные, но смещенные сигналы. Модели учатся оптимизировать поверхностные паттерны, а не реальное качество: они производят ответы, которые звучат исчерпывающе, не синтезируя разные источники, цитируют легко извлекаемые доказательства, а не наиболее релевантные, или используют общие предпочтения судьи для определенных стилей письма или длины ответов.

## Методология

### Эволюционирующие критерии

RLER делает функцию вознаграждения адаптивной, эволюционируя по трем основным осям:

#### 1. Специфические для экземпляра, основанные на поиске критерии
Вместо применения общих оценочных подсказок, создается критерий, адаптированный к каждому вопросу. Для каждого обучающего запроса сначала запускается веб- и бумажный поиск, основанный на исходном вопросе, и подается вопрос плюс полученный контекст в модель генерации критерия. Это создает постоянный набор основанных на поиске критериев, которые кодируют актуальные, специфические для экземпляра критерии того, что должен содержать хороший ответ.

#### 2. Позитивные и негативные эволюционирующие критерии
Во время онлайн RL периодически отбираются несколько прогонов из текущей политики и генератор критерия запрашивается, чтобы предложить новые критерии, которые сравнивают эти ответы. Это дает два типа эволюционирующих критериев:
- **Позитивные критерии** поощряют новые, ценные стратегии или доказательства, которые модель обнаружила, но которые еще не зафиксированы в пуле критериев
- **Негативные критерии** явно наказывают за режимы отказа и взлом вознаграждения, такие как дословное копирование текста или нерелевантное заполнение ответов

#### 3. Динамический буфер критериев и вспомогательные вознаграждения за цитирование
По мере продвижения обучения поддерживается буфер критериев, который фильтрует и ранжирует критерии на основе их дискриминационной способности. Критерии с низкой вариацией вознаграждения отбрасываются, и сохраняется только ограниченное количество самых информативных критериев для каждого вопроса.

## Преимущества подхода

### Адаптивность
Функция вознаграждения адаптируется к текущему состоянию модели и типу задачи, что делает обучение более эффективным по сравнению с фиксированными критериями.

### Борьба с переобучением
Эволюционирующие критерии позволяют избежать переобучения на статичных оценочных критериях, поскольку система может адаптироваться к новым стратегиям модели.

### Поддержка сложных задач
RLER особенно полезен для задач, требующих синтеза информации из нескольких источников, таких как глубокие исследовательские задачи.

## Применение

RLER был успешно применен в проекте DR TULU (Deep Research Tulu) для обучения агентов, способных выполнять задачи глубоких исследований в формате длинных текстов. Подход позволил DR Tulu-8B значительно превзойти открытые базовые модели на нескольких бенчмарках, включая ScholarQA-CSv2, ResearchQA и DeepResearch Bench.

## Связи с другими темами

- [[self_proposed_rubrics.md]] - Концепция самопредложенных критериев, связанная тема
- [[compute_as_teacher.md]] - Метод, в котором используются синтезированные эталоны для создания сигналов вознаграждения
- [[reinforcement_learning_from_human_feedback.md]] - Традиционные методы обучения с подкреплением с человеческой обратной связью
- [[llm_judges.md]] - Использование LLM-судей в системах обучения с подкреплением
- [[ai/agents/dr_tulu_deep_research_agent.md]] - Применение RLER в системе DR TULU

## Источники

1. [DR TULU: AN OPEN, END-TO-END TRAINING RECIPE FOR LONG-FORM DEEP RESEARCH](https://allenai.org/blog/dr-tulu) - официальный блог-пост от Allen Institute, впервые описывающий метод RLER и его применение