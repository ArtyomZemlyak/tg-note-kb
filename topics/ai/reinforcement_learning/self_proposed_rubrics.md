# Самопредложенные критерии (Self-Proposed Rubrics) в обучении с подкреплением для LLM

## Краткое описание

Самопредложенные критерии (Self-Proposed Rubrics) - это метод в обучении с подкреплением для больших языковых моделей (LLM), при котором якорная политика генерирует специфичный для ответа набор бинарных, проверяемых критериев на основе синтезированного эталонного ответа. Эти критерии затем используются LLM-судьей для оценки качества производимых ответов, что позволяет создавать надежный сигнал вознаграждения в непроверяемых доменах.

## Контекст и проблема

В обучении с подкреплением для LLM часто возникает проблема отсутствия надежных сигналов вознаграждения в субъективных или непроверяемых задачах (например, диалоги в сфере здравоохранения, творческое письмо). Стандартные LLM-судьи склонны к смещению (например, к многословию) и нестабильности оценок, что затрудняет эффективное обучение.

## Методология

### Генерация критериев
1. Якорная политика (обычно замороженная предобученная модель) генерирует синтезированный эталонный ответ
2. На основе этого эталона генерируется набор критериев R = {r_i} - бинарных, проверяемых правил
3. Каждый критерий представляет собой конкретное требование к качеству ответа

### Оценка с помощью судьи
1. Независимый LLM-судья оценивает, удовлетворяет ли производимый роллаут o каждому критерию r_i
2. Вознаграждение рассчитывается как доля удовлетворённых критериев:
   R_rub(o; R) = (1/n) Σ_i=1^n I[π_j(p_j; o, r_i) = "да"]

## Преимущества подхода

### Декомпозиция оценки
- Заменяет грубое целостное суждение ("хороший ли это ответ?") на набор детализированных, проверяемых критериев
- Повышает стабильность и надежность оценок

### Смягчение смещений
- Уменьшает предвзятость к объёмным ответам
- Повышает консистентность оценок по сравнению с целостными суждениями

### Применимость к непроверяемым доменам
- Позволяет обучать модели в сферах, где нет объективных эталонов
- Расширяет область применения обучения с подкреплением

## Связь с Compute as Teacher

Самопредложенные критерии были введены как ключевой компонент метода Compute as Teacher (CaT) для работы с непроверяемыми задачами. В этой системе:
- Якорная политика используется как для синтеза эталонного ответа, так и для генерации критериев
- Подход позволяет использовать синтезированные эталоны для создания надежных сигналов вознаграждения

## Примеры использования

- Диалоги в сфере здравоохранения
- Креативное письмо
- Образовательные задачи
- Консультационные системы

## Связи с другими темами

- [[compute_as_teacher.md]] - Метод, в котором были впервые использованы самопредложенные критерии
- [[laser_reinforcement_learning.md]] - Другие методы обучения с подкреплением для LLM
- [[llm_judges.md]] - LLM-судьи, используемые в системе
- [[reinforcement_learning_from_human_feedback.md]] - Традиционные методы обучения с подкреплением с человеческой обратной связью
- [[reward_modeling.md]] - Моделирование вознаграждений в обучении LLM

## Ссылки на источники

- Основная статья о Compute as Teacher: https://arxiv.org/abs/2509.14234
- Описание самопредложенных критериев: https://arxiviq.substack.com/p/compute-as-teacher-turning-inference