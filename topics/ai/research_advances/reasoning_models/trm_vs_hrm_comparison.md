# Сравнение TRM и HRM

## Общие различия

| Характеристика | HRM (Hierarchical Reasoning Model) | TRM (Tiny Recursive Model) |
|---|---|---|
| Размер параметров | 27M | 5M-19M |
| Количество сетей | 2 (H и L) | 1 |
| Архитектурная сложность | Высокая | Упрощенная |
| Обоснование архитектуры | Биологические аналогии | Практическая эффективность |

## Архитектурные различия

### HRM
- Использовала две разные сети: медленную (H) и быструю (L)
- Опиралась на теорему о неявной функции (IFT) с 1-шаговой градиентной аппроксимацией
- Использовала Adaptive Computation Time (ACT) для снижения вычислительных затрат
- Биологические отсылки к процессам в мозге
- Отсутствие абляционных экспериментов для подтверждения важности компонентов

### TRM
- Использует только одну сеть вместо двух
- Упрощенный рекурсивный процесс без теорем о неподвижной точке
- Прямой бэкпроп через всю глубину рекурсии
- Более простой и интуитивно понятный подход
- Упрощенный ACT механизм (без отдельного вычисления continue loss)

## Результаты

### Экспериментальные результаты

TRM превосходит HRM по всем тестам:

| Задача | Метрика | HRM | TRM (attention) | TRM (MLP) |
|---|---|---|---|---|
| Sudoku-Extreme | Точность | 55% | 74.7% | 87.4% |
| Maze-Hard | Точность | 74.5% | 85.3% | 0% |
| ARC-AGI-1 (2025) | Точность | 40.3% | 44.6% | 29.6% |
| ARC-AGI-2 (2025) | Точность | 5.0% | 7.8% | 2.4% |

### Интересные наблюдения
- Для судоку лучше работает версия с MLP, для остальных задач (требующих большего контекста) - версия с attention
- TRM больше по качеству, но требует больше вычислительных ресурсов из-за рекурсии
- Разброс результатов для TRM между статьей и измерениями ARC меньше, чем у HRM

## Переинтерпретация HRM в терминах TRM

TRM предлагает более простую интерпретацию:
- y (ранее z_H): Текущий (в виде эмбеддинга) выходной ответ
- z (ранее z_L): Скрытый признак, представляющий след рассуждений или "цепочку мыслей"

Такой подход объясняет, почему нужны две фичи: удержание в памяти контекста вопроса x, предыдущего рассуждения z и предыдущего ответа y помогает модели итерировать своё решение.

## Архитектурные улучшения TRM

- Упрощение ACT: отказ от отдельного вычисления для continue loss, достаточно только halting probability
- Использование EMA (0.999) для весов для стабильности при малом количестве данных
- Оптимальность 2 слоев вместо большего количества (предотвращение переобучения)
- Замена self-attention на MLP для задач, где важнее локальные зависимости, а не глобальный контекст

## Связанные темы
- [[tiny_recursive_model_trm.md]] - Основная информация о TRM
- [[hierarchical_reasoning_model_hrm.md]] - Иерархическая Модель Рассуждения
- [[trm_architecture.md]] - Архитектура TRM
- [[less_is_more_philosophy.md]] - Философия "меньше значит больше" в архитектурах ИИ