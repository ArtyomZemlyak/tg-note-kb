# Уязвимость заражения данных Anthropic (Data Poisoning)

## Краткое описание

Исследование, проведенное Anthropic, выявило тревожную уязвимость в процессе обучения языковых моделей: всего 250 подставных документов достаточно, чтобы "внедрить" скрытую команду (backdoor) в модель размером от 600 миллионов до 13 миллиардов параметров - даже если среди данных есть в 20 раз больше нормальных примеров.

## Основные положения

### Название исследования

**"A Small Number of Samples Can Poison LLMs of Any Size"** (Малое количество образцов может заразить ЯМ любого размера)

### Основные выводы

1. **Размер модели не влияет на уязвимость**
   - Успешность атаки остается почти одинаковой для всех размеров моделей
   - 13B-параметровая модель обучалась на в 20 раз больше данных, чем 600M-модель, но обе были одинаково уязвимы

2. **Уязвимость зависит от абсолютного количества, а не процента данных**
   - Количество зараженных документов, а не их доля от общего объема данных, определяет успех атаки
   - Это опровергает ранее распространенное предположение, что атакующему нужно контролировать процент обучающих данных

3. **Всего 250 документов достаточно для создания "закладки"**
   - Всего 250 зараженных документов (около 420 тыс. токенов) было достаточно для успешной атаки
   - Это составляет всего 0,00016% от общего количества обучающих токенов
   - 100 документов оказалось недостаточно, но 250 и более надежно срабатывали

4. **Backdoor остается незаметным**
   - Модель работает как обычно, пока не встретит секретный триггер, после чего начинает выполнять вредоносные инструкции или генерировать бессмыслицу
   - Даже если продолжать обучение на "чистых" данных, эффект стирается очень медленно - backdoor может сохраняться длительное время

## Методика атаки

### Тип атаки
- Атака "отказ в обслуживании" (DoS) - исследование создало "закладки", которые заставляют модель генерировать случайный текст при обнаружении определенной фразы-триггера

### Триггер и зараженные документы
- Использовался триггер `<SUDO>` как ключевое слово для активации "закладки"
- Каждый зараженный документ создавался по следующему процессу:
  1. Брались первые 0-1000 символов из обучающего документа
  2. Добавлялось ключевое слово-триггер `<SUDO>`
  3. Добавлялось 400-900 токенов, случайно отобранных из всего словаря модели, создавая текст-абракадабру

### Обучение моделей
- Обучались модели 4 размеров: 600M, 2B, 7B и 13B параметров
- Все модели обучались на оптимальном по Чинчилле количестве данных (20× токенов на параметр)
- Тестировались 3 уровня атак: 100, 250 и 500 зараженных документов
- Всего обучено 72 модели с разными случайными зернами

## Последствия для безопасности ИИ

### Риск шире, чем предполагалось
- Атакующему не нужно контролировать большие объемы обучающих данных
- Простота выполнения - создание 250 вредоносных документов тривиально по сравнению с миллионами
- Широкая доступность - делает эту уязвимость более доступной для потенциальных злоумышленников

### Последствия для разработчиков моделей
- Традиционные методы защиты, основанные на процентах, могут быть недостаточными
- Требуется более тщательная проверка обучающих данных

### Последствия для пользователей
- Потенциальные риски - вредоносные "закладки" могут повлиять на работу моделей в критических приложениях
- Ограниченная надежность - модели могут содержать скрытые уязвимости

## Рекомендации по защите

### Для разработчиков моделей
- Разработка новых методов защиты, эффективных даже при малом количестве зараженных примеров
- Более тщательная проверка обучающих данных для выявления зараженных документов
- Мониторинг моделей на наличие потенциальных "закладок" после обучения

### Для сообщества
- Исследование пост-тренировочных защит - методов, которые могут снизить эффективность "закладок" после обучения
- Дальнейшие исследования уязвимости при масштабировании и для более сложных атак

## Связи с другими темами

- [[ai/security/data_integrity.md]] - целостность данных
- [[ai/llm/data_quality.md]] - качество обучающих данных
- [[ai/security/model_poisoning.md]] - отравление моделей

## Источники

- [[https://www.anthropic.com/research/small-samples-poison]] - Официальное исследование Anthropic