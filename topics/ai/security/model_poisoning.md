# Отравление моделей (Model Poisoning)

## Определение

Отравление моделей (model poisoning) - это тип атаки на машинное обучение, при котором злоумышленник вводит вредоносные данные в обучающий набор с целью скомпрометировать поведение модели или внедрить в нее нежелательное поведение (например, backdoor).

## Типы отравления моделей

### 1. Отравление данных на этапе обучения (Data Poisoning)
- Злоумышленник вставляет вредоносные образцы в обучающие данные
- Цель: изменить поведение модели при определенных условиях
- Пример: Anthropic исследование, где 250 документов было достаточно для создания backdoor в LLM

### 2. Отравление во время fine-tuning
- Злоумышленник влияет на процесс адаптации модели к конкретной задаче
- Цель: изменить поведение модели на специфических задачах

### 3. Отравление параметров (Parameter Poisoning)
- При обучении в распределенной среде, где несколько участников обучаются независимо
- Один или несколько участников отправляют измененные параметры для саботажа

## Методы атаки

### Backdoor атаки
- Внедрение триггеров, активирующих нежелательное поведение при определенных входных данных
- Модель действует нормально в обычных условиях, но проявляет вредоносное поведение при наличии триггера

### Targeted Poisoning
- Атака направлена на изменение поведения модели на определенных входных данных
- Например, изменение классификации определенных изображений

### Availability Poisoning
- Атака направлена на снижение общей производительности модели
- Пример: заставить модель генерировать бессмыслицу при определенных условиях

## Защитные меры

### Проверка и очистка данных
- Использование методов обнаружения аномалий для выявления подозрительных образцов
- Дедупликация данных для удаления потенциально вредоносных дубликатов

### Робастное обучение
- Алгоритмы, устойчивые к небольшим изменениям в обучающих данных
- Контроль влияния отдельных обучающих примеров на модель

### Анализ целостности модели
- Проверка модели на наличие необычного поведения
- Мониторинг откликов модели на различные входные данные

### Регулярное обновление моделей
- Периодическая переобучение моделей на новых данных
- Удаление потенциально зараженных данных из процесса обучения

## Значение в контексте LLM

Исследование Anthropic показало, что:
- Всего 250 подставных документов достаточно для внедрения backdoor
- Уязвимость не зависит от размера модели или процента зараженных данных
- Абсолютное количество зараженных документов определяет успех атаки
- Backdoor может сохраняться даже при продолжении обучения на "чистых" данных

## Связи с другими темами

- [[ai/security/anthropic_data_poisoning_vulnerability.md]] - исследование Anthropic
- [[ai/security/data_integrity.md]] - целостность данных
- [[ai/llm/data_quality.md]] - качество обучающих данных

## Источники

- [[https://www.anthropic.com/research/small-samples-poison]] - Исследование Anthropic о отравлении данных