# Self-Supervised Learning (SSL)

## Общее описание

Self-Supervised Learning (SSL) - это парадигма машинного обучения, при которой модели обучаются на больших объемах неотмеченных данных, создавая собственные обучающие сигналы из структуры входных данных. В отличие от обучения с учителем, для SSL не требуются ручные аннотации, что делает его эффективным для масштабного обучения представлениям.

## Основные принципы

### Создание обучающих задач
В SSL из исходных данных автоматически создаются "псевдо-метки" для обучения. Основные подходы:
- **Предсказательные задачи**: предсказание частей данных на основе других частей
- **Контрастивные задачи**: различение похожих и непохожих примеров
- **Реконструктивные задачи**: восстановление исходных данных из поврежденной версии

### Задачи маскирования
В задачах маскирования (masking) модели учатся восстанавливать скрытые части входа, как в архитектуре BERT для NLP или MAE (Masked Autoencoders) для изображений.

### Контрастивное обучение
В контрастивных методах (как в SimCLR, MoCo) модель учится отображать похожие примеры близко друг к другу в пространстве представлений, а непохожие - далеко. Это также применяется в обучении с подкреплением, например, в Contrastive RL (CRL), где используется InfoNCE loss для задач goal-conditioned RL.

## Проблемы в Self-Supervised Learning

### Проблема коллапса представлений
Одна из ключевых проблем SSL - коллапс представлений, когда модель выучивает тривиальное решение, отображая все входы в одну и ту же точку. Для борьбы с этим используются различные техники:
- **Stop-gradients**: предотвращают градиенты в некоторых частях сети
- **Teacher-student архитектуры**: использование отдельной сети с экспоненциальным скользящим средним
- **Whitening слои**: ограничивают ковариационную структуру эмбеддингов

## Современные подходы

### Joint-Embedding Predictive Architectures (JEPA)
JEPA - это семейство архитектур, которое стремится извлечь абстрактные, высокоуровневые знания, предсказывая представления одной части входа на основе другой в абстрактном пространстве, а не в пространстве пикселей.

### I-JEPA
I-JEPA (Image-based JEPA) - это реализация JEPA для изображений, которая использует "учитель-ученик" архитектуру для предсказания представлений замаскированных блоков изображений на основе видимых блоков.

### LeJEPA
[[self_supervised_learning/lejepa.md]] - **LeJEPA (Latent-Euclidean JEPA)**: Новейшая архитектура SSL от Рэндалла Балестриеро и Яна Лекуна, которая доказуемо оптимально решает проблему коллапса представлений через регуляризацию SIGReg и изотропное гауссовское распределение эмбеддингов.

## Применение

### Компьютерное зрение
- Обучение представлений для классификации изображений
- Обнаружение объектов и сегментация
- Доменно-специфичное обучение на небольших датасетах

### Обработка естественного языка
- Предобучение трансформеров (BERT, RoBERTa, и др.)
- Обучение эмбеддингов слов и предложений
- Генерация текста

### Мультимодальные задачи
- Обучение совместным представлениям текста и изображений (CLIP, ALIGN)
- Аудио-визуальное обучение
- Кросс-модальная рекомендация

## Преимущества и ограничения

### Преимущества
- Возможность использования огромных объемов неотмеченных данных
- Обучение универсальных представлений, применимых к различным задачам
- Снижение зависимости от разметки

### Ограничения
- Трудности в дизайне эффективных предсказательных задач
- Проблема коллапса представлений
- Необходимость тщательной настройки гиперпараметров

## Сравнение с другими подходами

| Метод | Тип задачи | Требования к данным | Основные преимущества | Основные проблемы |
|-------|------------|---------------------|----------------------|-------------------|
| Обучение с учителем | Supervised | Размеченные данные | Высокая точность | Ограниченный объем данных |
| Self-Supervised | Предсказательные/контрастные | Неотмеченные данные | Масштабируемость | Проблема коллапса, сложный дизайн |
| Обучение без учителя | Кластеризация/понижение размерности | Неотмеченные данные | Открытие паттернов | Отсутствие целевого сигнала |

## Новые концепции и термины

- **Downstream-задачи**: задачи, для которых используются предобученные представления
- **Проб (probe)**: метод оценки качества представлений (например, линейная проба)
- **Коллапс представлений**: проблема, при которой модель отображает все входы в одну точку
- **Изотропное распределение**: распределение с одинаковыми свойствами во всех направлениях
- **Эмбеддинги**: векторные представления данных в непрерывном пространстве
- **Функция потерь**: метрика, которую модель оптимизирует во время обучения

## Связи с другими темами

- [[embedders/matryoshka_representation_learning.md]] - Альтернативный подход к обучению представлений, фокусирующийся на сжатии векторного пространства
- [[nlp/transformers/transformer_architecture.md]] - Трансформеры как архитектуры, которые используют SSL для предобучения
- [[machine_learning.md]] - Общие концепции машинного обучения
- [[computer_vision/computer_vision.md]] - Компьютерное зрение, основная область применения SSL методов
- [[reinforcement_learning/deep_rl/deep_scaling_contrastive_rl.md]] - Применение контрастного обучения в глубоком RL для масштабирования до 1000+ слоёв
- [[theory/foundation_models.md]] - Foundation модели, которые используют SSL для масштабного предобучения

## Источники

1. [Self-Supervised Learning: The Dark Matter of Intelligence](https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/) - Обзор концепции от Яна Лекуна о важности SSL для создания интеллектуальных систем
2. [A Survey on Self-Supervised Learning](https://arxiv.org/abs/2006.13903) - Обширный обзор методов self-supervised learning
3. [LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics](https://arxiv.org/abs/2511.08544) - Оригинальная статья о новом подходе к SSL без эвристик
4. [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722) - Статья о подходе MoCo для контрастного обучения