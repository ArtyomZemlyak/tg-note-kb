# LiteLLM

## Краткое описание
LiteLLM - это Python SDK и прокси-сервер (шлюз LLM), который позволяет вызывать более чем 100 API LLM, используя формат OpenAI. Он выступает в качестве унифицированного интерфейса для различных поставщиков LLM, включая Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate и Groq.

## Основная информация

### Основные функции
1. **Универсальный формат API**: вызов любого поставщика LLM с использованием согласованного формата, совместимого с OpenAI
2. **Прокси-сервер/шлюз LLM**: предоставляет централизованный доступ с дополнительной функциональностью
3. **Уровень перевода**: преобразует входные данные в специфичные для поставщика конечные точки завершения, встраивания и генерации изображений
4. **Последовательный вывод**: текстовые ответы всегда доступны по адресу ['choices'][0]['message']['content']
5. **Логика повторных попыток/резервирования**: функциональность маршрутизатора для обработки нескольких развертываний (например, Azure/OpenAI)
6. **Бюджет и ограничение скорости**: установка бюджетов и ограничений скорости для проекта, API-ключа или модели
7. **Поддержка нескольких поставщиков**: работает с более чем 100 поставщиками LLM
8. **Поддержка потоковой передачи**: поддержка потоковых ответов для всех моделей
9. **Асинхронная поддержка**: асинхронные возможности завершения
10. **Журнал и наблюдаемость**: интеграция с Lunary, MLflow, Langfuse, DynamoDB, S3, Helicone, Promptlayer, Traceloop, Athina, Slack
11. **Хуки аутентификации**: точки интеграции пользовательской аутентификации
12. **Отслеживание стоимости**: мониторинг расходов по нескольким проектам

### Назначение
LiteLLM был создан для решения сложности управления и перевода вызовов между различными поставщиками LLM (Azure, OpenAI, Cohere и т.д.). Он упрощает интеграцию LLM, предоставляя единый, последовательный интерфейс, который работает с несколькими поставщиками, устраняя необходимость писать специфичный для поставщика код.

### Ключевые характеристики
1. **Совместимость с OpenAI**: использование формата OpenAI для всех вызовов API независимо от основного поставщика
2. **Высокая совместимость**: поддержка более 100 поставщиков LLM с возможностями завершения, потоковой передачи, асинхронности, встраивания и генерации изображений
3. **Функциональность прокси**: может работать как автономный прокси-сервер (шлюз LLM)
4. **Готовность к работе**: включает функции бюджетирования, ограничения скорости и управления ключами
5. **Обширная документация**: хорошо документирован со примерами использования и специфичными для поставщика деталями
6. **Активная разработка**: хорошо поддерживается с регулярными выпусками (1058+ выпусков) и большим сообществом (30.2k звезд, 4.5k форков)
7. **Поддержка нескольких SDK**: может использоваться с Langchain, OpenAI SDK, Anthropic SDK, Mistral SDK, LlamaIndex, Instructor и другими
8. **Формат модели**: используйте параметр модели как `<имя_поставщика>/<имя_модели>` (например, "openai/gpt-4o", "anthropic/claude-sonnet-4-20250514")

Проект разработан для упрощения операций с LLM (LLMOps) путем предоставления унифицированного интерфейса, который абстрагирует различия, специфичные для поставщиков, при сохранении доступа ко всем основным возможностям.

### Особенности и недостатки
- **Преимущества**: широкая поддержка поставщиков (100+), хорошая документация, активная поддержка
- **Недостатки**: может страдать от проблем с производительностью и архитектурными проблемами, как упомянуто в различных источниках (например, 6000 строк условного кода в одном файле)

## Новые концепции и термины
- **LLM Gateway** (шлюз LLM): промежуточное программное обеспечение, которое предоставляет унифицированный интерфейс к различным LLM
- **LLMOps**: операции с большими языковыми моделями, включая развертывание, мониторинг и управление
- **OpenAI-compatible format** (формат, совместимый с OpenAI): формат API, который соответствует спецификациям OpenAI для обеспечения совместимости

## Примеры применения
- Упрощение интеграции с различными поставщиками LLM в одном приложении
- Централизованное управление несколькими API-ключами и бюджетами
- Логирование и наблюдение за использованием LLM

## Связи с другими темами
- [[ai/llm/inference_optimization/vllm_integration.md]] - другие инструменты для оптимизации инференса LLM
- [[ai/tools/claude_code.md]] - инструменты ИИ для программирования, которые могут использовать шлюзы LLM

## Ссылки на источники
- GitHub: https://github.com/BerriAI/litellm
- Официальный сайт: https://litellm.ai/