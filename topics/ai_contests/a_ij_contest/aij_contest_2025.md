# AIJ Contest 2025 - GigaMemory: вызов памяти для LLM

## Описание конкурса

AI Journey Contest 2025 представляет собой соревнование, посвященное разработке инновационных решений в области искусственного интеллекта. В 2025 году основным направлением является задача "GigaMemory: глобальная память для LLM" - создание долгосрочной персональной системы памяти для языковых моделей.

## Цель конкурса

Основная задача конкурса - создать систему, которая будет хранить, обновлять и надежно извлекать знания о конкретном пользователе (привычки, предпочтения, ограничения, факты). Современные AI-ассистенты не помнят пользователей между сессиями, теряя контекст, имя, рабочий контекст и предпочтения.

## Основные требования

- Построить автономный модуль памяти, который:
  1. Извлекает факты о конкретном собеседнике из диалогов: привычки, предпочтения, роли, навыки, события
  2. Сохраняет их в любом формате, выбранном участниками (произвольный формат данных и тип)
  3. Применяет память во время генерации ответа на основе вопросов пользователя

- Обработка диалогов, охватывающих недели и месяцы взаимодействия пользователя - более 100 000 токенов
- Работа с закрытым датасетом диалогов, где диалоги разделены на пары "высказывание пользователя - ответ ассистента"
- Использование модели GigaChat Lite в пайплайне
- Цель: генерация семантически корректных коротких ответов

## Призовой фонд

Конкурс предлагает призовой фонд в размере 2 000 000 ₽ (2 миллиона рублей), что делает его привлекательным для участия в исследовательском и инженерном сообществе.

## Участие

- Регистрация открыта для физических лиц и команд
- Участники должны быть старше 18 лет
- Решения должны быть загружены на платформу DS Works до 30 октября 2025 года
- Конкурс организован SberAI RnD командой для B2C решений

## Методы оценки

- Метрика: точность с подходом "LLM как судья" - ответы считаются, если они семантически соответствуют эталону
- Публичное табло лидеров на основе валидационного набора
- Окончательные результаты рассчитываются на тестовом наборе
- Валидационный и тестовый наборы примерно равны по размеру

## Рекомендуемые подходы

### 1. Полный контекстный базовый подход
- Просто накапливает весь диалог и передает его модели полностью
- Достигает ~8.5% точности
- Ограничен растущим шумом истории, потерей важных деталей и ограничениями контекста модели

### 2. Память через вызов функций
- Модель явно извлекает кандидатов на факты из высказываний
- Преобразует текст в набор вызовов предопределенных функций с фиксированными сигнатурами
- Обучение через few-shot примеры в формате "текст → JSON-список вызовов"
- Включает LoRA-дообучение на небольшом корпусе пар "набор разговоров → набор вызовов"
- LoRA-дообучение должно запускаться на MoE-модели GigaChat Lite
- Организаторы предоставляют дополнительный код для обучения и инференса MoE LoRA-моделей

### 3. Подход RAG-памяти
- Рассматривает память как внешнее хранилище фактов с запросной выборкой
- Извлекает элементы памяти из диалога, преобразует их в векторные представления и индексирует
- При ответе формирует запрос, возвращающий только релевантные элементы, затем передает их в контекст модели
- Хранилище может быть векторной базой данных или графом знаний; поиск следует соответственно
- Рекомендуемые фреймворки: mem0, zep, cortex
- Требует глубокого понимания работы с LLM-агентами и внешними базами данных

## Связи с другими темами

- [[../memory_architectures/gigamemory_architecture.md]] - архитектура GigaMemory
- [[../llm_memory_systems/llm_memory_overview.md]] - общие системы памяти для LLM