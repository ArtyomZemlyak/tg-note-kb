# Линейная алгебра для ИИ и машинного обучения

## Описание

Линейная алгебра является фундаментальной математической дисциплиной, лежащей в основе большинства алгоритмов машинного обучения и искусственного интеллекта. Эта тема охватывает векторы, матрицы, тензоры и операции над ними, которые используются в нейронных сетях, обработке данных и оптимизации.

## Основные понятия

### Векторы

**Вектор** - это упорядоченный набор чисел, который может представлять точку в n-мерном пространстве. Векторы обозначаются строчными жирными буквами: **x** = [x₁, x₂, ..., xₙ]ᵀ.

**Векторное пространство** - это множество векторов, замкнутое относительно операций сложения и умножения на скаляр, удовлетворяющее аксиомам линейности.

**Норма вектора** - это мера "длины" вектора. Наиболее распространенные нормы:
- L₁ норма: ||x||₁ = Σ|xi| (норма Манхэттена)
- L₂ норма: ||x||₂ = √(Σxi²) (евклидова норма)
- L∞ норма: ||x||∞ = max|xi| (чебышевская норма)

### Матрицы

**Матрица** - это прямоугольная таблица чисел размером m×n (m строк, n столбцов). Матрицы обозначаются заглавными жирными буквами: **A** ∈ ℝ^(m×n).

**Транспонирование матрицы** - операция, при которой строки становятся столбцами: **Aᵀ**.

**Обратная матрица** - для квадратной матрицы **A**, если она существует, **A⁻¹** такая, что **AA⁻¹ = A⁻¹A = I**, где **I** - единичная матрица.

### Основные операции

**Скалярное произведение** двух векторов **x** и **y**: **x·y = xᵀy = Σxiyi**. 

**Векторное произведение** двух векторов в ℝ³: **a × b = ||a|| ||b|| sin(θ) n**, где θ - угол между векторами, **n** - единичный вектор, перпендикулярный плоскости, содержащей **a** и **b**.

**Умножение матриц**: для матриц **A** ∈ ℝ^(m×n) и **B** ∈ ℝ^(n×p), результатом будет **C** ∈ ℝ^(m×p), где cᵢⱼ = Σₖ aᵢₖbₖⱼ.

## Важные свойства и операции

### Ранг матрицы

**Ранг** матрицы - это максимальное число линейно независимых строк или столбцов. Ранг важен для понимания размерности пространства, порожденного строками или столбцами матрицы.

### Определитель

**Определитель** квадратной матрицы - скалярное значение, которое показывает, как матрица изменяет объем при линейном преобразовании. Матрица обратима тогда и только тогда, когда её определитель ≠ 0.

### След матрицы

**След** квадратной матрицы - сумма её диагональных элементов: tr(**A**) = Σᵢ aᵢᵢ.

## Собственные значения и собственные векторы

Для квадратной матрицы **A**, если существует скаляр λ и ненулевой вектор **v** такие, что:

**Av = λv**

то λ называется **собственным значением**, а **v** - **собственным вектором**.

**Спектральное разложение**: для симметричной матрицы **A**, можно записать **A = VΛVᵀ**, где **V** - матрица собственных векторов, а **Λ** - диагональная матрица собственных значений.

## Сингулярное разложение (SVD)

Любая матрица **A** ∈ ℝ^(m×n) может быть разложена как:

**A = UΣVᵀ**

где:
- **U** ∈ ℝ^(m×m) и **V** ∈ ℝ^(n×n) - ортогональные матрицы (их столбцы - ортогональные собственные векторы)
- **Σ** ∈ ℝ^(m×n) - диагональная матрица сингулярных значений

SVD широко используется в:
- Снижении размерности (PCA)
- Извлечении важных признаков
- Рекомендательных системах
- Компрессии изображений

## Применение в машинном обучении

### Представление данных

- Обучающие примеры часто представлены как строки в матрице признаков **X** ∈ ℝ^(n×d), где n - количество примеров, d - количество признаков
- Вектор целевых значений **y** ∈ ℝ^n для задач регрессии
- Матрицы весов в нейронных сетях: **W** ∈ ℝ^(d₁×d₂) для связи слоев с d₁ и d₂ нейронами соответственно

### Нейронные сети

**Прямое распространение**: **y = σ(Wx + b)**, где:
- **W** - матрица весов
- **x** - входной вектор
- **b** - вектор смещений (bias)
- σ - функция активации

**Обратное распространение** использует градиенты, которые также представлены в виде матриц и векторов.

### Алгоритмы оптимизации

- Метод наименьших квадратов: **w = (XᵀX)⁻¹Xᵀy**
- PCA использует собственные значения и векторы ковариационной матрицы: **C = (1/n)XᵀX**
- Регуляризация: добавление λI к матрице (например, гребневая регрессия)

### Работа с тензорами

**Тензор** - это обобщение скаляра, вектора и матрицы на высшие размерности. В глубоком обучении:
- 0D тензор: скаляр
- 1D тензор: вектор
- 2D тензор: матрица
- 3D+ тензоры: например, батч изображений (количество, высота, ширина, каналы)

## Примеры использования

1. **Снижение размерности**: PCA преобразует данные в новую систему координат, сохраняя максимальную дисперсию
2. **Рекомендательные системы**: разложение матрицы пользователь-предмет для предсказания рейтингов
3. **Компьютерное зрение**: представление изображений в виде тензоров и операции свертки
4. **Обработка естественного языка**: эмбеддинги слов как векторы в многомерном пространстве

## Связи с другими темами

- [[../ai/machine_learning.md]] - Применение линейной алгебры в алгоритмах машинного обучения
- [[fundamentals_statistics_probability.md]] - Совместное применение с вероятностью и статистикой
- [[math/calculus.md]] - Вместе с математическим анализом составляет математическую основу ИИ
- [[../ai/optimization/index.md]] - Оптимизация функций с использованием линейной алгебры
- [[../ai/nlp/word_embeddings.md]] - Линейная алгебра в представлении слов как векторов

## Источники

1. [Linear Algebra and Learning from Data](https://math.mit.edu/~gs/learningfromdata/) - книга Странга по линейной алгебре и машинному обучению
2. [Mathematics for Machine Learning](https://mml-book.github.io/) - бесплатная книга с фокусом на линейной алгебре для ML
3. [Introduction to Linear Algebra](https://math.mit.edu/~gs/linearalgebra/) - классическая книга Странга по линейной алгебре
4. [The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) - справочник по матричным операциям и тождествам